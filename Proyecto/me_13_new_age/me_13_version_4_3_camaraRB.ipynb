{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comunicacion y camara\n",
    "import socket\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDPCamera:\n",
    "    def __init__(self):\n",
    "        self.host = '0.0.0.0'\n",
    "        self.port = 5000\n",
    "        self.buffer_size = 65536\n",
    "        self.mtu = 1400  # Tamaño máximo de fragmento\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.settimeout(2)\n",
    "        self.frame = None\n",
    "        self.fragments = []  # Almacena fragmentos del frame\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.lock = threading.Lock()  # Para acceso thread-safe\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.sock.bind((self.host, self.port))\n",
    "            \n",
    "            self.thread = threading.Thread(target=self._receive_frames, daemon=True)\n",
    "            self.thread.start()\n",
    "\n",
    "    def _receive_frames(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                # Recibir fragmento\n",
    "                fragment, _ = self.sock.recvfrom(self.buffer_size)\n",
    "                \n",
    "                with self.lock:\n",
    "                    self.fragments.append(fragment)\n",
    "\n",
    "                    # Detectar último fragmento (tamaño < MTU)\n",
    "                    if len(fragment) < self.mtu:\n",
    "                        # Reconstruir frame completo\n",
    "                        frame_bytes = b''.join(self.fragments)\n",
    "                        self.fragments = []  # Resetear fragmentos\n",
    "\n",
    "                        # Decodificar y almacenar frame\n",
    "                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)\n",
    "                        self.frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "            except socket.timeout:\n",
    "                print(\"[WARN] Timeout esperando fragmentos UDP\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Recepción UDP: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def read(self):\n",
    "        with self.lock:\n",
    "            if self.frame is not None:\n",
    "                return True, self.frame.copy()  # Copia para thread-safe\n",
    "            return False, None\n",
    "\n",
    "    def isOpened(self):\n",
    "        return self.running\n",
    "\n",
    "    def release(self):\n",
    "        self.running = False\n",
    "        with self.lock:\n",
    "            self.fragments = []\n",
    "            self.frame = None\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        self.sock.close()\n",
    "        print(\"Cámara UDP liberada\")\n",
    "\n",
    "    def __del__(self):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de TensorFlow para rendimiento\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "    # Permitir crecimiento de memoria según sea necesario\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        print(\"GPU disponible para aceleración\")\n",
    "    except:\n",
    "        print(\"Error al configurar GPU, usando CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "data_dir = \"hand_gestures_data_4_3\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Modelo y datos de entrenamiento\n",
    "model = None\n",
    "# Inicializar scaler y label encoder\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "model_file = \"hand_gesture_nn_model_4_3.h5\"\n",
    "scaler_file = \"hand_gesture_scaler_4_3.pkl\"\n",
    "encoder_file = \"hand_gesture_encoder_4_3.pkl\"\n",
    "gesture_data = \"gesture_data_4_3.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para estado\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Estado del sistema\n",
    "is_trained = False\n",
    "is_collecting = False\n",
    "current_gesture = \"\"\n",
    "samples_collected = 0\n",
    "max_samples = 100\n",
    "\n",
    "# Control de tiempo para la recolección continua\n",
    "last_sample_time = 0\n",
    "sample_delay = 0.05  # 50ms entre muestras\n",
    "\n",
    "# Temporizador para mostrar mensajes\n",
    "message = \"\"\n",
    "message_until = 0\n",
    "\n",
    "# Para evaluación del modelo\n",
    "metrics = {\n",
    "    'accuracy': 0,\n",
    "    'val_accuracy': 0,\n",
    "    'training_time': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extrae los landmarks de las manos desde un frame de video.\n",
    "\n",
    "    Args:\n",
    "        frame: Imagen capturada por la cámara (en formato BGR).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Lista de landmarks normalizados (126 elementos) y booleano indicando si se detectaron manos.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    landmarks_data = []\n",
    "    hands_detected = False\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hands_detected = True\n",
    "        # Extraer landmarks de hasta dos manos\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Dibujar landmarks en el frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = []\n",
    "\n",
    "            # Extraer coordenadas (x,y,z) de los 21 landmarks\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            landmarks_data.extend(landmarks)\n",
    "    \n",
    "    # Normalizar para 2 manos (si solo hay una o ninguna, rellenar con ceros)\n",
    "    while len(landmarks_data) < 21 * 3 * 2:  # 21 puntos * 3 coordenadas * 2 manos\n",
    "        landmarks_data.append(0.0)\n",
    "    \n",
    "    # Limitar a exactamente 126 características (21 puntos * 3 coordenadas * 2 manos)\n",
    "    landmarks_data = landmarks_data[:21 * 3 * 2]\n",
    "    \n",
    "    return landmarks_data, hands_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_message(message_text, duration=2):\n",
    "    \"\"\"\n",
    "    Establece un mensaje para mostrar en pantalla por una duración determinada.\n",
    "\n",
    "    Args:\n",
    "        message_text: Mensaje a mostrar (str).\n",
    "        duration: Duración en segundos (int o float, por defecto 2).\n",
    "    \"\"\"\n",
    "    global message, message_until\n",
    "    message = message_text\n",
    "    message_until = time.time() + duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_collection(gesture_name):\n",
    "    \"\"\"\n",
    "    Inicia la recolección de datos para una seña específica.\n",
    "\n",
    "    Args:\n",
    "        gesture_name: Nombre de la seña a recolectar (str).\n",
    "    \"\"\"\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = True\n",
    "    current_gesture = gesture_name\n",
    "    samples_collected = 0\n",
    "    set_message(f\"Mantenga la seña frente a la cámara. Recolectando '{gesture_name}'...\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_collection():\n",
    "    \"\"\"\n",
    "    Detiene la recolección de datos.\n",
    "    \"\"\"\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = False\n",
    "    current_gesture = \"\"\n",
    "    samples_collected = 0\n",
    "    set_message(\"Recolección finalizada\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "    \"\"\"\n",
    "    Guarda los datos recolectados en disco.\n",
    "    \"\"\"\n",
    "    global data, labels, gesture_data\n",
    "    data_dict = {\n",
    "        \"features\": data, \n",
    "        \"labels\": labels\n",
    "        }\n",
    "    with open(f\"{data_dir}/{gesture_data}\", \"wb\") as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "    set_message(f\"Datos guardados: {len(data)} muestras\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sample(landmarks):\n",
    "    \"\"\"\n",
    "    Recolecta una muestra de landmarks para la seña actual.\n",
    "\n",
    "    Args:\n",
    "        landmarks: Lista de landmarks extraídos (126 elementos).\n",
    "\n",
    "    Returns:\n",
    "        bool: True si se completó la recolección (max_samples alcanzado), False en caso contrario.\n",
    "    \"\"\"\n",
    "    global data, labels, samples_collected, last_sample_time, is_collecting\n",
    "\n",
    "    if not is_collecting:\n",
    "        return False\n",
    "    \n",
    "    current_time = time.time()\n",
    "    # Verificar si ha pasado suficiente tiempo desde la última muestra\n",
    "    if current_time - last_sample_time >= sample_delay:\n",
    "        data.append(landmarks)\n",
    "        labels.append(current_gesture)\n",
    "        samples_collected += 1\n",
    "        last_sample_time = current_time\n",
    "\n",
    "        # Guardar datos periódicamente\n",
    "        if samples_collected % 10 == 0:\n",
    "            save_data()\n",
    "            \n",
    "        # Verificar si se ha completado la recolección\n",
    "        if samples_collected >= max_samples:\n",
    "            stop_collection()\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Carga los datos recolectados desde disco.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si los datos se cargaron correctamente, False en caso contrario.\n",
    "    \"\"\"\n",
    "    global data, labels, gesture_data\n",
    "    try:\n",
    "        with open(f\"{data_dir}/{gesture_data}\", \"rb\") as f:\n",
    "            data_dict = pickle.load(f)\n",
    "            data = data_dict[\"features\"]\n",
    "            labels = data_dict[\"labels\"]\n",
    "        set_message(f\"Datos cargados: {len(data)} muestras\", 2)\n",
    "        return True\n",
    "    except:\n",
    "        set_message(\"No se encontraron datos previos\", 2)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Crea una red neuronal liviana para reconocimiento de gestos.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Dimensión de las características de entrada (int, ej. 126).\n",
    "        num_classes: Número de clases a predecir (int).\n",
    "\n",
    "    Returns:\n",
    "        Modelo de red neuronal compilado (tensorflow.keras.Model).\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Capa de entrada con regularización para prevenir sobreajuste\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,), \n",
    "              kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3), # Dropout para mejorar generalización\n",
    "\n",
    "        # Capa oculta \n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Capa de salida\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compilar con optimizador Adam y tasa de aprendizaje reducida para estabilidad\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    Entrena el modelo de red neuronal con los datos recolectados.\n",
    "\n",
    "    Returns:\n",
    "        float: Precisión del modelo en datos de prueba.\n",
    "    \"\"\"\n",
    "    global data, labels, scaler, label_encoder, is_trained, metrics\n",
    "    if len(data) < 10:\n",
    "        set_message(\"Se necesitan más datos para entrenar\", 2)\n",
    "        return 0\n",
    "    \n",
    "    set_message(\"Preparando datos para entrenamiento...\", 1)\n",
    "\n",
    "    # Convertir a arrays NumPy\n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Codificar etiquetas\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Dividir datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Normalizar datos\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "\n",
    "    # Crear modelo\n",
    "    num_classes = len(set(y_encoded))\n",
    "    set_message(f\"Entrenando modelo con {num_classes} clases...\", 2)\n",
    "    \n",
    "    model = create_neural_network(X_train.shape[1], num_classes)\n",
    "    \n",
    "    # Callbacks para mejorar el entrenamiento\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True),\n",
    "\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5, \n",
    "            min_lr=0.0001)\n",
    "    ]\n",
    "\n",
    "    # Medir tiempo de entrenamiento\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=50, \n",
    "        batch_size=32, \n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks, \n",
    "        verbose=1)\n",
    "    \n",
    "    # Calcular tiempo de entrenamiento\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    \n",
    "    # Guardar métricas\n",
    "    metrics.update({\n",
    "        'accuracy': test_accuracy,\n",
    "        'val_accuracy': max(history.history['val_accuracy']),\n",
    "        'training_time': training_time,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'report': classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "    })\n",
    "    \n",
    "    # Guardar el modelo y componentes\n",
    "    model.save(model_file)\n",
    "    with open(scaler_file, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(encoder_file, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    is_trained = True\n",
    "    set_message(f\"Modelo entrenado. Precisión: {test_accuracy:.2f}\", 3)\n",
    "    \n",
    "    # Imprimir reporte detallado\n",
    "    print(\"\\n--- Informe del Modelo ---\")\n",
    "    print(f\"Precisión en datos de prueba: {test_accuracy:.4f}\")\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f} segundos\")\n",
    "    print(\"\\nClasificación detallada:\")\n",
    "    print(metrics['report'])\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model():\n",
    "    \"\"\"\n",
    "    Carga un modelo previamente entrenado desde disco.\n",
    "\n",
    "    Returns:\n",
    "        Modelo cargado (tensorflow.keras.Model) o None si falla la carga.\n",
    "    \"\"\"\n",
    "    global scaler, label_encoder, is_trained\n",
    "    try:\n",
    "        # Cargar modelo, scaler y codificador de etiquetas\n",
    "        model = load_model(model_file)\n",
    "        #model = tf.keras.models.load_model(model_file)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(encoder_file, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        is_trained = True\n",
    "        set_message(\"Modelo de red neuronal cargado correctamente\", 2)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo: {str(e)}\")\n",
    "        set_message(\"No se encontró un modelo guardado\", 2)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(landmarks, model):\n",
    "    \"\"\"\n",
    "    Predice la seña a partir de los landmarks usando el modelo.\n",
    "\n",
    "    Args:\n",
    "        landmarks: Lista de landmarks extraídos (126 elementos).\n",
    "        model: Modelo de red neuronal cargado (tensorflow.keras.Model).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Nombre de la seña predicha (str) y confianza (float).\n",
    "    \"\"\"\n",
    "    if not is_trained or model is None:\n",
    "        return None, 0\n",
    "    \n",
    "    # Preprocesar datos\n",
    "    X = np.array([landmarks])\n",
    "    X_scaled = scaler.transform(X)\n",
    "\n",
    "    # Predecir\n",
    "    prediction_probs = model.predict(X_scaled, verbose=0)[0]\n",
    "    prediction_idx = np.argmax(prediction_probs)\n",
    "    confidence = prediction_probs[prediction_idx]\n",
    "    \n",
    "    # Decodificar clase\n",
    "    try:\n",
    "        prediction_label = label_encoder.inverse_transform([prediction_idx])[0]\n",
    "    except:\n",
    "        prediction_label = \"Desconocido\"\n",
    "    \n",
    "    # Solo devolver predicción si la confianza es suficiente\n",
    "    if confidence >= 0.6:\n",
    "        return prediction_label, confidence\n",
    "    return \"Desconocido\", confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gestures():\n",
    "    \"\"\"\n",
    "    Obtiene la lista de gestos disponibles en el conjunto de datos.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de nombres de gestos únicos.\n",
    "    \"\"\"\n",
    "    return list(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUEVO MENSAJE QUE DICE SI EXISTE EL MDELO\n",
    "def check_model_exists():\n",
    "    \"\"\"Verifica si el archivo del modelo existe.\"\"\"\n",
    "    return os.path.exists(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global model, is_trained, data, labels\n",
    "\n",
    "    # Inicialización del sistema\n",
    "    is_trained = False\n",
    "    model = None\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Cargar datos existentes\n",
    "    load_data()\n",
    "    \n",
    "    # Intentar cargar modelo si existe\n",
    "    if check_model_exists():\n",
    "        model = load_saved_model()\n",
    "        is_trained = True\n",
    "    else:\n",
    "        is_trained = False\n",
    "\n",
    "    # Iniciar cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP inicializada correctamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    " \n",
    "    menu_active = True\n",
    "    evaluation_mode = False\n",
    "\n",
    "    #NUEVO MUESTRO LAS SEÑAS GRABADAS\n",
    "    global labels\n",
    "    if not labels:\n",
    "        print(\"No hay señas guardadas. Usa 'R' para recolectar datos.\")\n",
    "        return\n",
    "    \n",
    "    unique_gestures = list(set(labels))  # Obtener valores únicos\n",
    "    print(\"\\\\n--- Señas Guardadas ---\")\n",
    "    for i, gesture in enumerate(unique_gestures, 1):\n",
    "        print(f\"{i}. {gesture}\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Esperando primer frame...\")\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame)\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        # Mostrar menú principal\n",
    "        if menu_active and not evaluation_mode:\n",
    "            y_pos = 50\n",
    "            cv2.putText(frame, \"=== MENU PRINCIPAL ===\", (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            y_pos += 40\n",
    "            cv2.putText(frame, \"1. Recolectar nueva seña\", (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            y_pos += 30\n",
    "            cv2.putText(frame, \"2. Entrenar modelo\", (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            y_pos += 30\n",
    "            cv2.putText(frame, \"3. Evaluar en tiempo real\", (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            y_pos += 30\n",
    "            cv2.putText(frame, \"4. Salir\", (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            y_pos += 40\n",
    "            cv2.putText(frame, f\"Estado: Modelo {'ENTRENADO' if is_trained else 'NO ENTRENADO'} | Datos: {len(data)} muestras\", \n",
    "                        (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0) if is_trained else (0, 0, 255), 1)\n",
    "\n",
    "        # Control de teclas\n",
    "        key = cv2.waitKey(1)\n",
    "        \n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "            \n",
    "        elif key == ord('1') and menu_active:\n",
    "            gesture_name = input(\"Ingrese nombre de la seña (ej. 'Hola'): \")\n",
    "            if gesture_name:\n",
    "                start_collection(gesture_name)\n",
    "                menu_active = False\n",
    "                \n",
    "        elif key == ord('2') and menu_active:\n",
    "            if len(data) > 10:\n",
    "                train_model()\n",
    "                model = load_saved_model() if check_model_exists() else None\n",
    "                is_trained = True\n",
    "                menu_active = True\n",
    "            else:\n",
    "                set_message(\"¡Necesitas al menos 10 muestras para entrenar!\", 2)\n",
    "                \n",
    "        elif key == ord('3') and menu_active:\n",
    "            if is_trained:\n",
    "                evaluation_mode = True\n",
    "                menu_active = False\n",
    "                set_message(\"Modo evaluacion activado\", 2)\n",
    "            else:\n",
    "                set_message(\"¡Entrena el modelo primero (Opcion 2)!\", 2)\n",
    "                \n",
    "        elif key == ord('4'):\n",
    "            break\n",
    "\n",
    "        # Lógica de recolección\n",
    "        if is_collecting:\n",
    "            progress = int((samples_collected / max_samples) * frame_w)\n",
    "            cv2.rectangle(frame, (0, 0), (progress, 20), (0, 255, 0), -1)\n",
    "            cv2.putText(frame, f\"Recolectando: {current_gesture} ({samples_collected}/{max_samples})\", \n",
    "                       (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            if hands_detected:\n",
    "                collect_sample(landmarks)\n",
    "            else:\n",
    "                cv2.putText(frame, \"¡Muestra las manos!\", (frame_w//2 - 100, frame_h//2), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            \n",
    "            if not is_collecting:  # Cuando termina la recolección\n",
    "                menu_active = True\n",
    "                save_data()\n",
    "\n",
    "        # Lógica de evaluación\n",
    "        elif evaluation_mode and is_trained:\n",
    "            if hands_detected:\n",
    "                prediction, confidence = predict(landmarks, model)\n",
    "                color = (0, 255, 0) if confidence > 0.75 else (0, 165, 255)\n",
    "                cv2.putText(frame, f\"Seña: {prediction}\", (10, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "                cv2.putText(frame, f\"Confianza: {confidence:.2%}\", (10, 90), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Acerca las manos a la camara\", (frame_w//4, frame_h//2), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "            \n",
    "            cv2.putText(frame, \"Presiona M para volver al menu\", (10, frame_h - 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "            \n",
    "            if key == ord('m'):\n",
    "                evaluation_mode = False\n",
    "                menu_active = True\n",
    "\n",
    "        # Mensajes temporales\n",
    "        if time.time() < message_until:\n",
    "            cv2.putText(frame, message, (10, frame_h - 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "        cv2.imshow(\"Reconocimiento de Señas - RB\", frame)\n",
    "\n",
    "    # Liberar recursos\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Sistema cerrado correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cámara UDP inicializada correctamente\n",
      "Esperando primer frame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step - accuracy: 0.2629 - loss: 2.1333 - val_accuracy: 0.5938 - val_loss: 1.4018 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7382 - loss: 1.0746 - val_accuracy: 0.8021 - val_loss: 1.0943 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7929 - loss: 0.8166 - val_accuracy: 0.9271 - val_loss: 0.9058 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8187 - loss: 0.7331 - val_accuracy: 0.9375 - val_loss: 0.7895 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8602 - loss: 0.6463 - val_accuracy: 0.9479 - val_loss: 0.7106 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8914 - loss: 0.5619 - val_accuracy: 0.9479 - val_loss: 0.6342 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8829 - loss: 0.5244 - val_accuracy: 0.9583 - val_loss: 0.5781 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8695 - loss: 0.5924 - val_accuracy: 0.9688 - val_loss: 0.5349 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8675 - loss: 0.4846 - val_accuracy: 0.9688 - val_loss: 0.4772 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8895 - loss: 0.4799 - val_accuracy: 0.9688 - val_loss: 0.4397 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9412 - loss: 0.4228 - val_accuracy: 0.9688 - val_loss: 0.4153 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8966 - loss: 0.4472 - val_accuracy: 0.9688 - val_loss: 0.3940 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9452 - loss: 0.3670 - val_accuracy: 0.9688 - val_loss: 0.3761 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9105 - loss: 0.4094 - val_accuracy: 0.9688 - val_loss: 0.3624 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9637 - loss: 0.3307 - val_accuracy: 0.9688 - val_loss: 0.3440 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9588 - loss: 0.3670 - val_accuracy: 0.9688 - val_loss: 0.3169 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9610 - loss: 0.3055 - val_accuracy: 0.9688 - val_loss: 0.2977 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9521 - loss: 0.3165 - val_accuracy: 0.9688 - val_loss: 0.2841 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9412 - loss: 0.3324 - val_accuracy: 0.9688 - val_loss: 0.2742 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9272 - loss: 0.3698 - val_accuracy: 0.9583 - val_loss: 0.2688 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9520 - loss: 0.2960 - val_accuracy: 0.9688 - val_loss: 0.2582 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.2883 - val_accuracy: 0.9688 - val_loss: 0.2555 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9604 - loss: 0.2838 - val_accuracy: 0.9583 - val_loss: 0.2590 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9203 - loss: 0.3345 - val_accuracy: 0.9688 - val_loss: 0.2572 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9580 - loss: 0.2742 - val_accuracy: 0.9688 - val_loss: 0.2551 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9578 - loss: 0.2924 - val_accuracy: 0.9792 - val_loss: 0.2409 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9351 - loss: 0.3493 - val_accuracy: 0.9688 - val_loss: 0.2342 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9780 - loss: 0.2650 - val_accuracy: 0.9688 - val_loss: 0.2324 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9748 - loss: 0.2252 - val_accuracy: 0.9688 - val_loss: 0.2337 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9545 - loss: 0.2789 - val_accuracy: 0.9792 - val_loss: 0.2257 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9555 - loss: 0.2813 - val_accuracy: 0.9688 - val_loss: 0.2315 - learning_rate: 0.0010\n",
      "Epoch 32/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9631 - loss: 0.2651 - val_accuracy: 0.9688 - val_loss: 0.2231 - learning_rate: 0.0010\n",
      "Epoch 33/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9566 - loss: 0.2573 - val_accuracy: 0.9792 - val_loss: 0.2291 - learning_rate: 0.0010\n",
      "Epoch 34/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9710 - loss: 0.2346 - val_accuracy: 0.9792 - val_loss: 0.2311 - learning_rate: 0.0010\n",
      "Epoch 35/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9675 - loss: 0.2374 - val_accuracy: 0.9792 - val_loss: 0.2194 - learning_rate: 0.0010\n",
      "Epoch 36/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9814 - loss: 0.2191 - val_accuracy: 0.9896 - val_loss: 0.2113 - learning_rate: 0.0010\n",
      "Epoch 37/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9905 - loss: 0.1982 - val_accuracy: 0.9896 - val_loss: 0.2098 - learning_rate: 0.0010\n",
      "Epoch 38/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9681 - loss: 0.2341 - val_accuracy: 0.9792 - val_loss: 0.2110 - learning_rate: 0.0010\n",
      "Epoch 39/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9772 - loss: 0.2076 - val_accuracy: 0.9688 - val_loss: 0.2198 - learning_rate: 0.0010\n",
      "Epoch 40/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9582 - loss: 0.2408 - val_accuracy: 0.9792 - val_loss: 0.2060 - learning_rate: 0.0010\n",
      "Epoch 41/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9788 - loss: 0.2055 - val_accuracy: 0.9792 - val_loss: 0.2033 - learning_rate: 0.0010\n",
      "Epoch 42/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9915 - loss: 0.1876 - val_accuracy: 0.9792 - val_loss: 0.2043 - learning_rate: 0.0010\n",
      "Epoch 43/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9712 - loss: 0.2287 - val_accuracy: 0.9792 - val_loss: 0.2078 - learning_rate: 0.0010\n",
      "Epoch 44/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9770 - loss: 0.1928 - val_accuracy: 0.9896 - val_loss: 0.1990 - learning_rate: 0.0010\n",
      "Epoch 45/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9808 - loss: 0.2046 - val_accuracy: 0.9792 - val_loss: 0.2101 - learning_rate: 0.0010\n",
      "Epoch 46/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9701 - loss: 0.2355 - val_accuracy: 0.9792 - val_loss: 0.2057 - learning_rate: 0.0010\n",
      "Epoch 47/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9706 - loss: 0.2153 - val_accuracy: 0.9792 - val_loss: 0.2103 - learning_rate: 0.0010\n",
      "Epoch 48/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9932 - loss: 0.1788 - val_accuracy: 0.9792 - val_loss: 0.1996 - learning_rate: 0.0010\n",
      "Epoch 49/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9673 - loss: 0.2070 - val_accuracy: 0.9688 - val_loss: 0.2123 - learning_rate: 0.0010\n",
      "Epoch 50/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9879 - loss: 0.1953 - val_accuracy: 0.9688 - val_loss: 0.2148 - learning_rate: 5.0000e-04\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9500\n",
      "Tiempo de entrenamiento: 14.05 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      0.90      0.95        20\n",
      "        Chau       0.83      1.00      0.91        20\n",
      "  Como estas       1.00      0.85      0.92        20\n",
      "        Hola       0.95      1.00      0.98        20\n",
      "  bueno dias       1.00      1.00      1.00        20\n",
      " mas o menos       0.95      0.95      0.95        20\n",
      "\n",
      "    accuracy                           0.95       120\n",
      "   macro avg       0.96      0.95      0.95       120\n",
      "weighted avg       0.96      0.95      0.95       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cámara UDP liberada\n",
      "Sistema cerrado correctamente\n",
      "Cámara UDP liberada\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #run()\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
