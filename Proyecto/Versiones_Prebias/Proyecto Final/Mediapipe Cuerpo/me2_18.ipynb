{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\juanp\\Desktop\\Python\\.conda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------PROYECTO FINAL MEJORADO-----------------\n",
    "## 1. IMPORTAR LIBRERIAS\n",
    "import cv2 \n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def speak_async(engine, text):\n",
    "#    threading.Thread(target=lambda: (engine.say(text), engine.runAndWait())).start()\n",
    "num_camara=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. CONFIGURACIÓN AVANZADA\n",
    "class KalmanFilterWrapper:\n",
    "    def __init__(self, process_noise=0.01, measurement_noise=1):\n",
    "        self.kf = KalmanFilter(dim_x=2, dim_z=1)\n",
    "        self.kf.x = np.array([0., 0.])\n",
    "        self.kf.F = np.array([[1., 1.], [0., 1.]])\n",
    "        self.kf.H = np.array([[1., 0.]])\n",
    "        self.kf.P *= 100.\n",
    "        self.kf.R = measurement_noise\n",
    "        self.kf.Q = np.array([[process_noise, 0.], [0., process_noise]])\n",
    "    \n",
    "    def smooth(self, measurement):\n",
    "        self.kf.predict()\n",
    "        self.kf.update(measurement)\n",
    "        return self.kf.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "pose = mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    model_complexity=1  # Reducir la complejidad\n",
    ")\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    max_num_hands=2,\n",
    "    model_complexity=0  # Complejidad mínima\n",
    ")\n",
    "\n",
    "# Variables globales\n",
    "data_dir = \"sign_language_data_JUPYTER\"\n",
    "data_dir_video = \"sign_language_data_JUPYTER_videos\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(data_dir_video, exist_ok=True)\n",
    "\n",
    "sequence_length = 30  # Frames por secuencia\n",
    "n_pose_landmarks = 33 * 3\n",
    "n_hand_landmarks = 21 * 3\n",
    "total_landmarks = n_pose_landmarks + (n_hand_landmarks * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. PREPROCESAMIENTO MEJORADO\n",
    "def preprocess_sequence(sequence):\n",
    "    \"\"\"Normalización robusta con manejo de bordes\"\"\"\n",
    "    sequence = np.array(sequence)\n",
    "    if sequence.size == 0 or np.all(sequence == 0):\n",
    "        return np.zeros_like(sequence)\n",
    "    \n",
    "    # Eliminar outliers extremos\n",
    "    q1 = np.nanquantile(sequence, 0.25, axis=0)\n",
    "    q3 = np.nanquantile(sequence, 0.75, axis=0)\n",
    "    iqr = q3 - q1\n",
    "    sequence = np.clip(sequence, q1 - 1.5*iqr, q3 + 1.5*iqr)\n",
    "    \n",
    "    # Normalización adaptativa\n",
    "    mean = np.nanmean(sequence, axis=0)\n",
    "    std = np.nanstd(sequence, axis=0) + 1e-8  # Evitar división por cero\n",
    "    \n",
    "    return (sequence - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    \"\"\"Procesa un frame y retorna los resultados de pose y manos\"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pose_results = pose.process(rgb_frame)\n",
    "    hands_results = hands.process(rgb_frame)\n",
    "    return pose_results, hands_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(pose_results, hands_results):\n",
    "    \"\"\"Extrae y normaliza los landmarks de pose y manos.\"\"\"\n",
    "    landmarks = []\n",
    "\n",
    "    if pose_results.pose_landmarks:\n",
    "        pose_landmarks = [[lm.x, lm.y, lm.z] for lm in pose_results.pose_landmarks.landmark]\n",
    "        landmarks.extend(np.array(pose_landmarks).flatten())\n",
    "    else:\n",
    "        landmarks.extend([0] * n_pose_landmarks)\n",
    "\n",
    "    hand_landmarks_list = []\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks[:2]:  # Máximo 2 manos\n",
    "            hand_points = [[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]\n",
    "            hand_landmarks_list.extend(np.array(hand_points).flatten())\n",
    "\n",
    "    # Rellenar con ceros si no se detectan ambas manos\n",
    "    while len(hand_landmarks_list) < n_hand_landmarks * 2:\n",
    "        hand_landmarks_list.extend([0] * n_hand_landmarks)\n",
    "\n",
    "    landmarks.extend(hand_landmarks_list)\n",
    "    landmarks = np.array(landmarks)\n",
    "\n",
    "    # Normalización\n",
    "    if np.any(landmarks):\n",
    "        landmarks = (landmarks - np.mean(landmarks)) / np.std(landmarks)\n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(data_dir_video, data_dir, sign_name, sequence_length):\n",
    "    \"\"\"Recolecta secuencias de movimiento para una seña específica y guarda el video de los landmarks\"\"\"\n",
    "    sign_dir = os.path.join(data_dir, sign_name)\n",
    "    sign_dir_video = os.path.join(data_dir_video, f\"{sign_name}\")\n",
    "    os.makedirs(sign_dir, exist_ok=True)\n",
    "    os.makedirs(sign_dir_video, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(num_camara)  # Cambiar índice si usas DroidCam u otra cámara\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir la cámara\")\n",
    "        return\n",
    "\n",
    "    total_sequences = int(input(\"Número de secuencias a recolectar (recomendado: 20-30): \"))\n",
    "\n",
    "    print(\"\\nInstrucciones:\")\n",
    "    print(f\"1. Cada secuencia grabará {sequence_length} frames de movimiento\")\n",
    "    print(\"2. Presiona ESPACIO para iniciar cada secuencia\")\n",
    "    print(\"3. Realiza el movimiento completo de la seña\")\n",
    "    print(\"4. La grabación se detendrá automáticamente\")\n",
    "    print(\"5. Presiona ESC para cancelar\")\n",
    "\n",
    "    sequence_count = 0\n",
    "    frame_count = 0\n",
    "    is_recording = False\n",
    "    current_sequence = []\n",
    "\n",
    "    while sequence_count < total_sequences:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        blank_frame = np.zeros_like(frame)\n",
    "        pose_results, hands_results = process_frame(frame)\n",
    "\n",
    "        # Dibujar landmarks\n",
    "        if pose_results.pose_landmarks:\n",
    "            mp_draw.draw_landmarks(blank_frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(blank_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Mostrar mensajes\n",
    "        if is_recording:\n",
    "            cv2.putText(frame, f\"Grabando secuencia {sequence_count + 1}...\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            landmarks = extract_landmarks(pose_results, hands_results)\n",
    "            current_sequence.append(landmarks)\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count >= sequence_length:\n",
    "                # Guardar la secuencia\n",
    "                sequence_data = np.array(current_sequence)\n",
    "                np.save(os.path.join(sign_dir, f\"sequence_{sequence_count}.npy\"), sequence_data)\n",
    "                print(f\"Secuencia {sequence_count + 1}/{total_sequences} guardada\")\n",
    "                sequence_count += 1\n",
    "                frame_count = 0\n",
    "                is_recording = False\n",
    "                current_sequence = []\n",
    "\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)\n",
    "        cv2.imshow(\"Landmarks\", blank_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 32 and not is_recording:  # Espacio para iniciar grabación\n",
    "            is_recording = True\n",
    "            current_sequence = []\n",
    "            frame_count = 0\n",
    "        elif key == 27:  # ESC para salir\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. AUGMENTACIÓN DE DATOS\n",
    "def augment_sequence(sequence):\n",
    "    \"\"\"Aplica transformaciones aleatorias a la secuencia\"\"\"\n",
    "    # Ruido gaussiano\n",
    "    noise = np.random.normal(0, 0.02, sequence.shape)\n",
    "    sequence += noise\n",
    "    \n",
    "    # Variación temporal\n",
    "    if np.random.rand() > 0.5:\n",
    "        sequence = np.roll(sequence, shift=np.random.randint(-2, 3), axis=0)\n",
    "    \n",
    "    # Escalado no uniforme\n",
    "    scale_factors = np.random.uniform(0.9, 1.1, size=sequence.shape[1])\n",
    "    sequence *= scale_factors\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir, sequence_length, total_landmarks, model_file):\n",
    "    \"\"\"Entrena el modelo utilizando CNN, LSTM y Transformers.\"\"\"\n",
    "    \n",
    "    if not os.listdir(data_dir):\n",
    "        print(\"No hay datos para entrenar\")\n",
    "        return\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "    print(\"Cargando secuencias...\")\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        samples = [f for f in os.listdir(class_dir) if f.startswith('sequence_')]\n",
    "        print(f\"Clase {class_name}: {len(samples)} secuencias\")\n",
    "\n",
    "        for sample_file in samples:\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sequence = np.load(sample_path)\n",
    "            X.append(sequence)\n",
    "            y.append(class_idx)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    # Normalización de los datos\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Definir dimensiones del modelo\n",
    "    input_shape = (sequence_length, total_landmarks)\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "        # Visualización del entrenamiento\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "    plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "    plt.title('Precisión del Modelo')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "    plt.title('Pérdida del Modelo')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. ARQUITECTURA DEL MODELO MEJORADA\n",
    "def build_advanced_model(input_shape, num_classes):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Capa de atención espacial\n",
    "    att = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)\n",
    "    x = tf.keras.layers.Concatenate()([inputs, att])\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    \n",
    "    # Bloques CNN\n",
    "    x = tf.keras.layers.Conv1D(256, 5, activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Bloques LSTM bidireccional\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Atención temporal\n",
    "    att_temp = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    x = tf.keras.layers.Concatenate()([x, att_temp])\n",
    "    x = tf.keras.layers.GlobalAvgPool1D()(x)\n",
    "    \n",
    "    # Capas densas\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer='l2')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        weighted_metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_spoken_word = None  # Variable global para almacenar la última palabra pronunciada\n",
    "\n",
    "def load_model(model_file):\n",
    "    \"\"\"Carga el modelo previamente entrenado.\"\"\"\n",
    "    if not os.path.exists(model_file):\n",
    "        print(\"No se encontró el modelo entrenado\")\n",
    "        return None\n",
    "    #Cargar el modelo entrenado\n",
    "    return tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(data_dir, sequence_length):\n",
    "    \"\"\"Carga y prepara los datos de prueba.\"\"\"\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "    # Crear conjunto de prueba\n",
    "    print(\"Cargando datos de prueba...\")\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        samples = [f for f in os.listdir(class_dir) if f.startswith('sequence_')]\n",
    "\n",
    "        # Seleccionar el 20% de las secuencias como prueba\n",
    "        test_samples = samples[:int(0.2 * len(samples))]\n",
    "        for sample_file in test_samples:\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sequence = np.load(sample_path)\n",
    "            X_test.append(sequence)\n",
    "            y_test.append(class_idx)\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    return X_test, y_test, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evalúa el modelo en los datos de prueba.\"\"\"\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"\\nPérdida en prueba: {loss:.4f}\")\n",
    "    print(f\"Precisión en prueba: {accuracy:.2%}\")\n",
    "\n",
    "    # Mostrar métricas globales\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar([\"Pérdida\", \"Precisión\"], [loss, accuracy], color=['blue', 'green'])\n",
    "    plt.title(\"Métricas globales en prueba\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Ver detección de pose y manos\n",
      "2. Recolectar datos de señas\n",
      "3. Entrenar modelo\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "WARNING:tensorflow:From c:\\Users\\juanp\\Desktop\\Python\\.conda\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\juanp\\Desktop\\Python\\.conda\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Ver detección de pose y manos\n",
      "2. Recolectar datos de señas\n",
      "3. Entrenar modelo\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## 6. EVALUACIÓN EN TIEMPO REAL MEJORADA\n",
    "def evaluate_realtime(model_file, data_dir, sequence_length, mp_draw, mp_pose, mp_hands, num_camara):\n",
    "    \"\"\"Evaluación con filtrado Kalman y procesamiento optimizado\"\"\"\n",
    "    model = load_model(model_file)\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "    kalman_filter = KalmanFilterWrapper()\n",
    "    \n",
    "    cap = cv2.VideoCapture(num_camara)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    current_sequence = []\n",
    "    prediction_buffer = []\n",
    "    confidence_history = []\n",
    "    last_prediction = None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        pose_results, hands_results = process_frame(frame)\n",
    "        \n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            landmarks = extract_landmarks(pose_results, hands_results)\n",
    "            if landmarks is not None:\n",
    "                # Suavizado con Savitzky-Golay\n",
    "                if len(current_sequence) >= 5:\n",
    "                    landmarks = savgol_filter(landmarks, 5, 2)\n",
    "                \n",
    "                current_sequence.append(landmarks)\n",
    "                if len(current_sequence) > sequence_length:\n",
    "                    current_sequence.pop(0)\n",
    "                \n",
    "                if len(current_sequence) == sequence_length:\n",
    "                    # Preprocesamiento y predicción\n",
    "                    sequence_data = preprocess_sequence(current_sequence)\n",
    "                    prediction = model.predict(np.expand_dims(sequence_data, 0), verbose=0)[0]\n",
    "                    \n",
    "                    # Filtrado Kalman\n",
    "                    raw_confidence = np.max(prediction)\n",
    "                    filtered_confidence = kalman_filter.smooth(raw_confidence)\n",
    "                    class_idx = np.argmax(prediction)\n",
    "                    \n",
    "                    # Gestión de predicciones\n",
    "                    prediction_buffer.append(class_names[class_idx])\n",
    "                    if len(prediction_buffer) > 15:\n",
    "                        prediction_buffer.pop(0)\n",
    "                    \n",
    "                    # Determinar predicción estable\n",
    "                    counter = Counter(prediction_buffer)\n",
    "                    most_common = counter.most_common(1)[0]\n",
    "                    stable_pred = most_common[0]\n",
    "                    stability = most_common[1] / len(prediction_buffer)\n",
    "                    \n",
    "                    # Mostrar resultados si supera umbrales\n",
    "                    if stability > 0.7 and filtered_confidence > 0.8:\n",
    "                        last_prediction = stable_pred\n",
    "                        cv2.putText(frame, f\"Seña: {stable_pred} ({filtered_confidence:.1%})\", \n",
    "                                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.imshow(\"Detección en Tiempo Real\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "## 7. ENTRENAMIENTO CON AUGMENTACIÓN\n",
    "def train_model_improved(data_dir, sequence_length, total_landmarks, model_file):\n",
    "    \"\"\"Entrenamiento con augmentación de datos y callbacks avanzados\"\"\"\n",
    "    X, y = [], []\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "    \n",
    "    # Cargar y aumentar datos\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        samples = [f for f in os.listdir(class_dir) if f.startswith('sequence_')]\n",
    "        \n",
    "        for sample_file in samples:\n",
    "            sequence = np.load(os.path.join(class_dir, sample_file))\n",
    "            X.append(sequence)\n",
    "            y.append(class_idx)\n",
    "            \n",
    "            # Generar variaciones aumentadas\n",
    "            X.append(augment_sequence(sequence))\n",
    "            y.append(class_idx)\n",
    "    \n",
    "    # Preprocesamiento\n",
    "    X = np.array(X)\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "    \n",
    "    # Construir y entrenar modelo\n",
    "    model = build_advanced_model((sequence_length, total_landmarks), len(class_names))\n",
    "    \n",
    "    # Callbacks avanzados\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),\n",
    "        tf.keras.callbacks.ModelCheckpoint(model_file, save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Visualización mejorada\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validación')\n",
    "    plt.title('Evolución de la Precisión')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Validación')\n",
    "    plt.title('Evolución de la Pérdida')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "## 8. MENU ACTUALIZADO\n",
    "def main():\n",
    "    #data_dir = \"data\"  # Directorio para los datos recolectados\n",
    "    data_dir = \"sign_language_data_JUPYTER\"\n",
    "    model_file = \"sign_language_model.h5\"\n",
    "    sequence_length = 90  # Longitud de la secuencia para cada muestra\n",
    "    num_camara = 0  # Índice de la cámara\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Ver detección de pose y manos\")\n",
    "        print(\"2. Recolectar datos de señas\")\n",
    "        print(\"3. Entrenar modelo\")\n",
    "        print(\"4. Evaluar en tiempo real\")\n",
    "        print(\"5. Salir\")\n",
    "        \n",
    "        option = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if option == \"1\":\n",
    "            cap = cv2.VideoCapture(num_camara)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                pose_results, hands_results = process_frame(frame)\n",
    "                # Dibuja los landmarks si se detectan\n",
    "                if pose_results.pose_landmarks:\n",
    "                    mp_draw.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "                if hands_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                        mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                cv2.imshow(\"Detección de Pose y Manos\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == 27:  # ESC para salir\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        elif option == \"2\":\n",
    "            sign_name = input(\"Nombre de la seña a recolectar: \")\n",
    "            collect_data(data_dir_video, data_dir, sign_name, sequence_length)\n",
    "        \n",
    "        elif option == \"3\":\n",
    "            train_model(data_dir, sequence_length, total_landmarks, model_file)\n",
    "            \n",
    "        elif option == \"4\":\n",
    "            evaluate_realtime(model_file, data_dir, sequence_length, mp_draw, mp_pose, mp_hands, num_camara)\n",
    "        \n",
    "        elif option == \"5\":\n",
    "            print(\"¡Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Opción no válida.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
