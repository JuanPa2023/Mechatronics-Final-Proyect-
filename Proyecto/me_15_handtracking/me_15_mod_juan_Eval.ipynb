{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd78588",
   "metadata": {},
   "source": [
    "# ------------- PROYECTO FINAL G & S--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067e16c",
   "metadata": {},
   "source": [
    "## LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1bf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Comunicación y cámara\n",
    "import socket\n",
    "import queue\n",
    "\n",
    "# Voz\n",
    "import pyttsx3\n",
    "import threading\n",
    "\n",
    "# Módulos para reconocimiento de voz\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "import io\n",
    "import wave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f43af6",
   "metadata": {},
   "source": [
    "## VOZ A PANTALLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df331d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para el reconocimiento de voz\n",
    "SAMPLE_RATE_IN = 48000  # Tasa del micrófono INMP441\n",
    "SAMPLE_RATE_OUT = 16000  # Tasa requerida por la API de reconocimiento\n",
    "BUFFER_DURATION = 5  # segundos\n",
    "UDP_IP_PI = \"192.168.7.2\"  # IP de la Raspberry Pi\n",
    "UDP_PORT_AUDIO = 5006\n",
    "UDP_PORT_TEXT = 5005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c1645",
   "metadata": {},
   "source": [
    "### RECONOCEDOR DE VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a852357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el reconocedor de voz\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Configuración UDP para voz\n",
    "sock_voice = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Variable para controlar el servicio de reconocimiento de voz\n",
    "speech_recognition_running = True #VA EN EL WHILE.\n",
    "\n",
    "# Variable para almacenar la última transcripción\n",
    "last_transcription = \"\" #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "\n",
    "def recibir_audio():\n",
    "    sock_audio = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    try:\n",
    "        sock_audio.bind((\"0.0.0.0\", UDP_PORT_AUDIO))\n",
    "        \n",
    "        buffer = bytearray()\n",
    "        bytes_needed = SAMPLE_RATE_IN * 4 * BUFFER_DURATION  # 4 bytes por muestra (32-bit)\n",
    "        \n",
    "        while speech_recognition_running:\n",
    "            data, _ = sock_audio.recvfrom(4096)\n",
    "            buffer.extend(data)\n",
    "            \n",
    "            while len(buffer) >= bytes_needed:\n",
    "                # Extraer 5 segundos de audio\n",
    "                chunk = bytes(buffer[:bytes_needed])\n",
    "                del buffer[:bytes_needed]\n",
    "                \n",
    "                # Convertir a formato numpy\n",
    "                audio_int32 = np.frombuffer(chunk, dtype=np.int32)\n",
    "                audio_float32 = audio_int32.astype(np.float32) / 2**31\n",
    "                \n",
    "                # Remuestrear a 16kHz\n",
    "                audio_16k = librosa.resample(\n",
    "                    audio_float32,\n",
    "                    orig_sr=SAMPLE_RATE_IN,\n",
    "                    target_sr=SAMPLE_RATE_OUT\n",
    "                )\n",
    "                \n",
    "                # Convertir a int16 para la API de reconocimiento\n",
    "                audio_int16 = (audio_16k * 32767).astype(np.int16)\n",
    "                \n",
    "                # Crear un archivo WAV en memoria\n",
    "                wav_buffer = io.BytesIO()\n",
    "                with wave.open(wav_buffer, 'wb') as wav_file:\n",
    "                    wav_file.setnchannels(1)  # Mono\n",
    "                    wav_file.setsampwidth(2)  # 2 bytes por muestra (16 bits)\n",
    "                    wav_file.setframerate(SAMPLE_RATE_OUT)\n",
    "                    wav_file.writeframes(audio_int16.tobytes())\n",
    "                \n",
    "                wav_buffer.seek(0)  # Rebobinar el buffer\n",
    "                audio_queue.put(wav_buffer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en recibir_audio: {e}\")\n",
    "    #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY  ver de comentarlo\n",
    "    finally: \n",
    "        sock_audio.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5e666",
   "metadata": {},
   "source": [
    "### PROCESAR AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f2738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_audio():\n",
    "    global last_transcription #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "    while speech_recognition_running:\n",
    "        try:\n",
    "            wav_buffer = audio_queue.get(timeout=1)\n",
    "            \n",
    "            # Crear un objeto AudioData desde el buffer WAV\n",
    "            with sr.AudioFile(wav_buffer) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "            \n",
    "            # Realizar la transcripción usando la API gratuita de Google\n",
    "            transcription = recognizer.recognize_google(audio_data, language=\"es-ES\")\n",
    "            \n",
    "            print(f\"Transcripción: {transcription}\")\n",
    "            last_transcription = transcription #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "            \n",
    "            # Enviar transcripción por UDP si es necesario\n",
    "            sock_voice.sendto(transcription.encode(), (UDP_IP_PI, UDP_PORT_TEXT))\n",
    "            \n",
    "        except queue.Empty:\n",
    "            continue\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"No se detectó voz en el audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Error en la solicitud a la API de Google: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error en la transcripción: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2687dfd",
   "metadata": {},
   "source": [
    "### ACTIVACION DE RECONOCIMIENTO DE VOZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8e679",
   "metadata": {},
   "source": [
    "analizar esto, en main dice que le ingresan dos parametros a esta funcion...\n",
    "capaz lo mejor es que se ejecute asi de una "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcdb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para iniciar el servicio de reconocimiento de voz\n",
    "def start_speech_recognition():\n",
    "    global speech_recognition_running\n",
    "    speech_recognition_running = True\n",
    "    \n",
    "    # Iniciar hilos para el reconocimiento de voz\n",
    "    audio_thread = threading.Thread(target=recibir_audio, daemon=True)\n",
    "    process_thread = threading.Thread(target=procesar_audio, daemon=True)\n",
    "    \n",
    "    audio_thread.start()\n",
    "    process_thread.start()\n",
    "    \n",
    "    print(\"Servicio de reconocimiento de voz iniciado...\")\n",
    "    return audio_thread, process_thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67993b4d",
   "metadata": {},
   "source": [
    "### DETENCION DE RECONOCIMIENTO DE VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfb07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para detener el servicio de reconocimiento de voz\n",
    "def stop_speech_recognition():\n",
    "    global speech_recognition_running\n",
    "    speech_recognition_running = False\n",
    "    sock_voice.close()\n",
    "    print(\"Servicio de reconocimiento de voz detenido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838398c2",
   "metadata": {},
   "source": [
    "## CAMARA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a488357",
   "metadata": {},
   "source": [
    "### CONFIGURACION UDP RB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475b0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del socket UDP\n",
    "UDP_IP_PI = \"192.168.7.2\" # Dirección IP de tu Raspberry Pi\n",
    "UDP_OPEN = '0.0.0.0'\n",
    "UDP_PORT = 5003\n",
    "UDP_PORT_SERVO = 5001  # Puerto para enviar comandos\n",
    "UDP_PORT_CAM = 5002  # Puerto para recibir video\n",
    "MAX_PACKET_SIZE = 1400  # Tamaño máximo del paquete UDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ceab1",
   "metadata": {},
   "source": [
    "### MOTOR TEXTO-VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d554e1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Lógica de prevención de repeticiones \\ndef speak_text(text):\\n    global last_spoken_gesture  # Accede a la variable global\\n    \\n    # Bloquea el acceso concurrente usando with para manejo seguro del recurso\\n    with tts_lock:  # Asegura que solo un hilo use el motor TTS a la vez\\n        \\n        # Verifica si el texto es diferente al último reproducido\\n        if text != last_spoken_gesture:  # Evita repeticiones consecutivas\\n            \\n            # Actualiza el registro del último gesto vocalizado\\n            last_spoken_gesture = text  # Almacena el nuevo texto\\n            \\n            # Añade el texto a la cola de reproducción\\n            tts_engine.say(text)  # Programa la reproducción del texto\\n            \\n            # Ejecuta la reproducción y espera a que termine\\n            tts_engine.runAndWait()  # Bloquea hasta terminar la reproducción'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" SE ESTA USANDO ESTA LINEA EN EL SPEECH TO TEXT, capaz no pasa nada que tambien se use aca\n",
    " capaz es al pedo, porque es la comunicacion udp con la raspberry, pero no se si es necesario\"\"\"\n",
    "# Inicializa el socket UDP (compartido para todos los hilos)\n",
    "udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "# Inicializa el motor de texto-a-voz de pyttsx3\n",
    "tts_engine = pyttsx3.init()  # Crea una instancia del motor TTS\n",
    "tts_engine.setProperty('rate', 150)  # Velocidad del habla (opcional)\n",
    "\n",
    "# Crea un objeto de bloqueo para sincronización de hilos\n",
    "tts_lock = threading.Lock()  # Previene acceso concurrente al motor TTS\n",
    "\n",
    "# Variable global para almacenar la última seña vocalizada\n",
    "last_spoken_gesture = None  # Guarda el texto del último gesto reproducido\n",
    "\n",
    "\"\"\"\n",
    "# Lógica de prevención de repeticiones \n",
    "def speak_text(text):\n",
    "    global last_spoken_gesture  # Accede a la variable global\n",
    "    \n",
    "    # Bloquea el acceso concurrente usando with para manejo seguro del recurso\n",
    "    with tts_lock:  # Asegura que solo un hilo use el motor TTS a la vez\n",
    "        \n",
    "        # Verifica si el texto es diferente al último reproducido\n",
    "        if text != last_spoken_gesture:  # Evita repeticiones consecutivas\n",
    "            \n",
    "            # Actualiza el registro del último gesto vocalizado\n",
    "            last_spoken_gesture = text  # Almacena el nuevo texto\n",
    "            \n",
    "            # Añade el texto a la cola de reproducción\n",
    "            tts_engine.say(text)  # Programa la reproducción del texto\n",
    "            \n",
    "            # Ejecuta la reproducción y espera a que termine\n",
    "            tts_engine.runAndWait()  # Bloquea hasta terminar la reproducción\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8715bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_text(text):\n",
    "    global last_spoken_gesture, udp_socket\n",
    "    \n",
    "    with tts_lock:\n",
    "        if text != last_spoken_gesture:\n",
    "            last_spoken_gesture = text\n",
    "            \n",
    "            try:\n",
    "                # Generar archivo de audio temporal\n",
    "                temp_file = \"temp_audio.wav\"\n",
    "                tts_engine.save_to_file(text, temp_file)\n",
    "                tts_engine.runAndWait()  # Esperar generación del archivo\n",
    "                \n",
    "                # Leer y enviar audio\n",
    "                with open(temp_file, 'rb') as f:\n",
    "                    audio_data = f.read()\n",
    "                    \n",
    "                    # Fragmentar y enviar\n",
    "                    total_chunks = (len(audio_data) + MAX_PACKET_SIZE - 1) // MAX_PACKET_SIZE\n",
    "                    for i in range(total_chunks):\n",
    "                        start = i * MAX_PACKET_SIZE\n",
    "                        end = start + MAX_PACKET_SIZE\n",
    "                        chunk = audio_data[start:end]\n",
    "                        udp_socket.sendto(chunk, (UDP_IP_PI, UDP_PORT))\n",
    "                        time.sleep(0.001)  # Pequeña pausa entre paquetes\n",
    "                \n",
    "                print(f\"Audio enviado: {text}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en generación/envío de audio: {str(e)}\")\n",
    "            finally:\n",
    "                # Limpiar archivo temporal\n",
    "                if os.path.exists(temp_file):\n",
    "                    os.remove(temp_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb60538",
   "metadata": {},
   "source": [
    "### MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afe0a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5, #probar con 0.4\n",
    "    min_tracking_confidence=0.5 #probar con 0.4\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c14f25",
   "metadata": {},
   "source": [
    "### COMUNICACION CAMARA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaba08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDPCamera:\n",
    "    def __init__(self):\n",
    "        self.host = UDP_OPEN\n",
    "        self.port = UDP_PORT_CAM\n",
    "        self.buffer_size = 65536\n",
    "        self.mtu = 1400\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.settimeout(2)\n",
    "        self.frame = None\n",
    "        self.fragments = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Configuración INTRÍNSECA de MediaPipe para el seguimiento\n",
    "        self.hands_tracker = mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.6,\n",
    "            min_tracking_confidence=0.6\n",
    "        )\n",
    "        \n",
    "        # Socket para enviar datos del servo\n",
    "        self.send_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        \n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.sock.bind((self.host, self.port))\n",
    "            self.thread = threading.Thread(target=self._receive_frames, daemon=True)\n",
    "            self.thread.start()\n",
    "\n",
    "    def _process_hand(self, frame):\n",
    "        # Procesamiento específico de la muñeca\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands_tracker.process(frame_rgb)\n",
    "        \n",
    "        if results.multi_hand_landmarks and results.multi_handedness:\n",
    "            for hand_handedness in results.multi_handedness:\n",
    "                if hand_handedness.classification[0].label == 'Left':\n",
    "                    wrist = results.multi_hand_landmarks[0].landmark[mp_hands.HandLandmark.WRIST]\n",
    "                    # Mapear coordenadas de la palma a un rango de -7.5 a 7.5\n",
    "                    x_normalized = int((wrist.x - 0.5) * 15) \n",
    "                    \n",
    "                    # Envío UDP automático\n",
    "                    self.send_sock.sendto(\n",
    "                        str(x_normalized).encode(), \n",
    "                        (UDP_IP_PI, UDP_PORT_SERVO)\n",
    "                    )\n",
    "                    \n",
    "                    # Dibujar punto (opcional)\n",
    "                    wrist_pixel = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                        wrist.x, wrist.y, frame.shape[1], frame.shape[0]\n",
    "                    )\n",
    "                    if wrist_pixel:\n",
    "                        cv2.circle(frame, wrist_pixel, 10, (0, 255, 0), -1)\n",
    "                    \n",
    "                    return x_normalized\n",
    "        return None\n",
    "\n",
    "    def _receive_frames(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                fragment, _ = self.sock.recvfrom(self.buffer_size)\n",
    "                with self.lock:\n",
    "                    self.fragments.append(fragment)\n",
    "                    if len(fragment) < self.mtu:\n",
    "                        frame_bytes = b''.join(self.fragments)\n",
    "                        self.fragments = []\n",
    "                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)\n",
    "                        frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)\n",
    "                        \n",
    "                        # Procesamiento AUTOMÁTICO de la mano\n",
    "                        if frame is not None:\n",
    "                            self._process_hand(frame)\n",
    "                            self.frame = frame  # Almacenar frame procesado\n",
    "                            \n",
    "            except socket.timeout:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def read(self):\n",
    "        with self.lock:\n",
    "            if self.frame is not None:\n",
    "                return True, self.frame.copy()\n",
    "            return False, None\n",
    "\n",
    "    def release(self):\n",
    "        self.running = False\n",
    "        self.hands_tracker.close()\n",
    "        with self.lock:\n",
    "            self.fragments = []\n",
    "            self.frame = None\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        self.sock.close()\n",
    "        self.send_sock.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cd388",
   "metadata": {},
   "source": [
    "### MODELO TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0a7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel:\n",
    "    def __init__(self, model_path):\n",
    "        # Cargar el modelo TFLite\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        \n",
    "        # Obtener detalles de entrada y salida\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        # Asegurar el tipo de dato correcto y agregar dimensión batch si es necesario\n",
    "        input_data = np.array(input_data, dtype=self.input_details[0]['dtype'])\n",
    "        if len(input_data.shape) == len(self.input_details[0]['shape']) - 1:\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        # Establecer la entrada y ejecutar la inferencia\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.interpreter.invoke()\n",
    "        \n",
    "        # Obtener la salida\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "253769a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de TensorFlow para rendimiento\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Configuración de TensorFlow para rendimiento en CPU\n",
    "    try:\n",
    "        # Verificar si hay GPU disponible (para futuras expansiones)\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        if physical_devices:\n",
    "            # Configuración para GPU (no se ejecutará en tu caso)\n",
    "            for device in physical_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(\"GPU disponible para aceleración\")\n",
    "        else:\n",
    "            # Optimización para CPU\n",
    "            tf.config.threading.set_intra_op_parallelism_threads(4)  # Aprovecha núcleos físicos\n",
    "            tf.config.threading.set_inter_op_parallelism_threads(2)  # Paralelismo entre operaciones\n",
    "            print(\"Modo CPU activado: Configuración optimizada para Intel Core i7-7500U\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error de configuración: {str(e)}\")\n",
    "        print(\"Usando configuración por defecto de CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f84a7",
   "metadata": {},
   "source": [
    "### ARCHIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb605c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "data_dir = \"hand_gestures_data_v15\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Modelo y datos de entrenamiento\n",
    "model = None\n",
    "# Inicializar scaler y label encoder\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "model_file = \"hand_gesture_nn_model_v15.h5\"\n",
    "scaler_file = \"hand_gesture_scaler_v15.pkl\"\n",
    "encoder_file = \"hand_gesture_encoder_v15.pkl\"\n",
    "gesture_data = \"gesture_data_v15.pkl\" \n",
    "model_tflite = \"modelo_optimizadotl_v15.tflite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a4ec0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para estado\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Estado del sistema\n",
    "is_trained = False\n",
    "is_collecting = False\n",
    "current_gesture = \"\"\n",
    "samples_collected = 0\n",
    "max_samples = 5000\n",
    "\n",
    "# Control de tiempo para la recolección continua\n",
    "last_sample_time = 0\n",
    "sample_delay = 0.05  # 50ms entre muestras\n",
    "\n",
    "# Temporizador para mostrar mensajes\n",
    "message = \"\"\n",
    "message_until = 0\n",
    "\n",
    "# Para evaluación del modelo\n",
    "metrics = {\n",
    "    'accuracy': 0,\n",
    "    'val_accuracy': 0,\n",
    "    'training_time': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ce453",
   "metadata": {},
   "source": [
    "### EXTRACCION DE LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dd6866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extrae los landmarks de las manos desde un frame de video.\n",
    "\n",
    "    Args:\n",
    "        frame: Imagen capturada por la cámara (en formato BGR).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Lista de landmarks normalizados (126 elementos) y booleano indicando si se detectaron manos.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    landmarks_data = []\n",
    "    hands_detected = False\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hands_detected = True\n",
    "        # Extraer landmarks de hasta dos manos\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Dibujar landmarks en el frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = []\n",
    "\n",
    "            # Extraer coordenadas (x,y,z) de los 21 landmarks\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            landmarks_data.extend(landmarks)\n",
    "    \n",
    "    # Normalizar para 2 manos (si solo hay una o ninguna, rellenar con ceros)\n",
    "    while len(landmarks_data) < 21 * 3 * 2:  # 21 landmarks * 3 coordenadas * 2 manos\n",
    "        landmarks_data.append(0.0)\n",
    "    \n",
    "    # Limitar a exactamente 126 valores (21 landmarks * 3 coordenadas * 2 manos)\n",
    "    landmarks_data = landmarks_data[:21 * 3 * 2]\n",
    "    \n",
    "    return landmarks_data, hands_detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62350ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_message(message_text, duration=2):\n",
    "    global message, message_until\n",
    "    message = message_text\n",
    "    message_until = time.time() + duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3435cd",
   "metadata": {},
   "source": [
    "### CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48579101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global data, labels\n",
    "    try:\n",
    "        with open(f\"{data_dir}/{gesture_data}\", \"rb\") as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "            data = loaded_data[\"features\"]\n",
    "            labels = loaded_data[\"labels\"]\n",
    "        set_message(f\"Datos cargados: {len(data)} muestras\", 2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos: {e}\")\n",
    "        set_message(\"No se encontraron datos previos\", 2)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16263d",
   "metadata": {},
   "source": [
    "### RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72d1d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_exists():\n",
    "    return os.path.exists(model_file) and os.path.exists(scaler_file) and os.path.exists(encoder_file)\n",
    "\n",
    "def create_neural_network(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859d1ad",
   "metadata": {},
   "source": [
    "### CARGA DEL MODELO ENTRENADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "454258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model():\n",
    "    global scaler, label_encoder\n",
    "    try:\n",
    "        model = load_model(model_file)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(encoder_file, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        set_message(\"Modelo cargado correctamente\", 2)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo: {e}\")\n",
    "        set_message(\"Error al cargar el modelo\", 2)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51651eed",
   "metadata": {},
   "source": [
    "### MODELO DE TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e5be08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.5):\n",
    "    try:\n",
    "        # Preprocesar los landmarks\n",
    "        landmarks_array = np.array(landmarks).reshape(1, -1)\n",
    "        landmarks_scaled = scaler.transform(landmarks_array)\n",
    "        \n",
    "        # Realizar predicción\n",
    "        predictions = tflite_model.predict(landmarks_scaled)[0]\n",
    "        \n",
    "        # Obtener la clase con mayor probabilidad\n",
    "        max_prob_idx = np.argmax(predictions)\n",
    "        confidence = predictions[max_prob_idx]\n",
    "        \n",
    "        if confidence >= threshold:\n",
    "            # Decodificar la etiqueta\n",
    "            predicted_label = label_encoder.inverse_transform([max_prob_idx])[0]\n",
    "            return predicted_label, confidence\n",
    "        else:\n",
    "            return \"Desconocido\", confidence\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la predicción: {e}\")\n",
    "        return \"Error\", 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b02471",
   "metadata": {},
   "source": [
    "### CONVERSION A TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32082a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model_file, model_tflite):\n",
    "    try:\n",
    "        if not os.path.exists(model_file):\n",
    "            raise FileNotFoundError(f\"El archivo {model_file} no existe.\")\n",
    "        \n",
    "        # Cargar el modelo entrenado\n",
    "        modelo = load_model(model_file)\n",
    "        \n",
    "        # Convertir a TensorFlow Lite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo convertido\n",
    "        with open(model_tflite, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(\"Modelo convertido a TensorFlow Lite.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al convertir el modelo a TFLite:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b74032",
   "metadata": {},
   "source": [
    "### EVALUACION EN TIEMPO REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95d357db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_mode():\n",
    "    global model_tflite\n",
    "    # Inicializa el modelo TFLite si aún no se ha cargado\n",
    "    if os.path.exists(model_tflite):\n",
    "        tflite_model = TFLiteModel(model_tflite)\n",
    "    else:\n",
    "        print(\"El modelo TFLite no existe. Conviértelo primero.\")\n",
    "        return\n",
    "\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP iniciada para evaluación en tiempo real.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame)\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        if hands_detected:\n",
    "            prediction, confidence = predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.9)\n",
    "            color = (0, 255, 0) if confidence > 0.9 else (0, 165, 255)\n",
    "            cv2.putText(frame, f\"Seña: {prediction}\", (10, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "            cv2.putText(frame, f\"Confianza: {confidence:.2%}\", (10, 90), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Extraer valor escalar en caso de que 'confidence' sea un array\n",
    "            confidence_value = np.max(confidence) if isinstance(confidence, np.ndarray) else confidence\n",
    "\n",
    "            if confidence_value > 0.99 and prediction != \"Desconocido\":\n",
    "                threading.Thread(target=speak_text, args=(prediction,), daemon=True).start()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Acerca las manos a la cámara\", (frame_w//4, frame_h//2), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "        cv2.putText(frame, \"Presiona M para volver al menú\", (10, frame_h - 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.imshow(\"Evaluación en Tiempo Real\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        #if key == ord('m'):\n",
    "        #    break\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b50ac",
   "metadata": {},
   "source": [
    "### FUNCION PRINCIPAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86d7086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_evaluation():\n",
    "    global model, is_trained, data, labels, scaler, label_encoder\n",
    "\n",
    "    # Iniciar reconocimiento de voz (si es necesario)\n",
    "    start_speech_recognition()  # Descomenta si lo necesitas\n",
    "\n",
    "    # Cargar datos existentes\n",
    "    load_data()\n",
    "    \n",
    "    # Cargar modelo y scaler\n",
    "    if check_model_exists():\n",
    "        model = load_saved_model()\n",
    "        is_trained = True\n",
    "        print(\"Modelo cargado correctamente\")\n",
    "    else:\n",
    "        print(\"¡Error! No existe un modelo entrenado.\")\n",
    "        return\n",
    "    \n",
    "    # Verificar scaler\n",
    "    if not hasattr(scaler, 'mean_'):\n",
    "        print(\"¡Error! El scaler no está entrenado.\")\n",
    "        return\n",
    "\n",
    "    # Iniciar evaluación\n",
    "    run_evaluation_mode()\n",
    "    \n",
    "    # Guardar datos al finalizar (opcional)\n",
    "    save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbcb2f",
   "metadata": {},
   "source": [
    "# EJECUTAR PROGRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40955799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Servicio de reconocimiento de voz iniciado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "C:\\Users\\juanp\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado correctamente\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Cuatro\n",
      "Transcripción: Hola hola hola se escucha se escucha\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Transcripción: le\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Transcripción: a Nacho\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: De Nada\n",
      "No se detectó voz en el audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x000001D4FC6EF640>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_8692\\250689476.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_8692\\250689476.py\", line 94, in release\n",
      "  File \"C:\\Users\\juanp\\AppData\\Roaming\\Python\\Python310\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mstart_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 28\u001b[0m, in \u001b[0;36mstart_evaluation\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m run_evaluation_mode()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Guardar datos al finalizar (opcional)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43msave_data\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_data' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c605b",
   "metadata": {},
   "source": [
    "def run_evaluation_mode():\n",
    "    global model_tflite\n",
    "    # Inicializa el modelo TFLite si aún no se ha cargado\n",
    "    if os.path.exists(model_tflite):\n",
    "        tflite_model = TFLiteModel(model_tflite)\n",
    "    else:\n",
    "        print(\"El modelo TFLite no existe. Conviértelo primero.\")\n",
    "        return\n",
    "\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP iniciada para evaluación en tiempo real.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame)\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        if hands_detected:\n",
    "            prediction, confidence = predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.9)\n",
    "            color = (0, 255, 0) if confidence > 0.9 else (0, 165, 255)\n",
    "            cv2.putText(frame, f\"Seña: {prediction}\", (10, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "            cv2.putText(frame, f\"Confianza: {confidence:.2%}\", (10, 90), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Extraer valor escalar en caso de que 'confidence' sea un array\n",
    "            confidence_value = np.max(confidence) if isinstance(confidence, np.ndarray) else confidence\n",
    "\n",
    "            if confidence_value > 0.99 and prediction != \"Desconocido\":\n",
    "                threading.Thread(target=speak_text, args=(prediction,), daemon=True).start()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Acerca las manos a la cámara\", (frame_w//4, frame_h//2), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "        # Mostrar la última transcripción de voz en la pantalla\n",
    "        if last_transcription:\n",
    "            cv2.putText(frame, f\"Voz: {last_transcription}\", (10, frame_h - 70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.putText(frame, \"Presiona ESC para volver al menú\", (10, frame_h - 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.imshow(\"Evaluación en Tiempo Real\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243aa342",
   "metadata": {},
   "source": [
    "def main():\n",
    "    global model, is_trained, data, labels\n",
    "\n",
    "    # Iniciar el servicio de reconocimiento de voz al iniciar el programa\n",
    "    print(\"Iniciando servicio de reconocimiento de voz...\")\n",
    "    audio_thread, process_thread = start_speech_recognition()\n",
    "    \n",
    "    # Iniciar la cámara UDP y el bucle principal\n",
    "    camera = UDPCamera()\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        cv2.imshow('Frame', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Detener el servicio de reconocimiento de voz\n",
    "    stop_speech_recognition()\n",
    "    \n",
    "    # Liberar recursos\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Cargar datos existentes\n",
    "    load_data()\n",
    "    \n",
    "    # Intentar cargar modelo si existe\n",
    "    if check_model_exists():\n",
    "        model = load_saved_model()\n",
    "        is_trained = True\n",
    "    else:\n",
    "        is_trained = False\n",
    "\n",
    "    # Mostrar el menú en la consola\n",
    "    print_menu()\n",
    "\n",
    "    # Bucle principal de selección en consola\n",
    "    while True:\n",
    "        opcion = input(\"\\nSelecciona una opción (Recolectar: 1, Entrenar: 2, Señas: 3, Evaluar: 4, Salir: 5): \").strip()\n",
    "        \n",
    "        if opcion == '1':\n",
    "            # Recolección de señas\n",
    "            gesture_name = input(\"Ingrese nombre de la seña (ej. 'Hola'): \")\n",
    "            if gesture_name:\n",
    "                start_collection(gesture_name)\n",
    "                # Iniciar la cámara para mostrar video durante la recolección\n",
    "                run_collection_mode()\n",
    "                \n",
    "        elif opcion == '2':\n",
    "            if len(data) > 10:\n",
    "                train_model()\n",
    "                model = load_saved_model() if check_model_exists() else None\n",
    "                is_trained = True\n",
    "                print(\"Entrenamiento completado. Modelo entrenado.\")\n",
    "                convert_to_tflite(model_file, model_tflite)\n",
    "                print(\"Convertido a TFLite para evaluación en tiempo real\")\n",
    "            else:\n",
    "                print(\"¡Necesitas al menos 10 muestras para entrenar!\")\n",
    "                \n",
    "        elif opcion == '3':\n",
    "            list_gestures()  # Lista las señas cargadas\n",
    "\n",
    "        elif opcion == '4':\n",
    "            if is_trained:\n",
    "                # Inicializar modo evaluación en tiempo real\n",
    "                print(\"Modo de evaluación activado.\")\n",
    "                run_evaluation_mode()\n",
    "            else:\n",
    "                print(\"¡Entrena el modelo primero (Opción 2)!\")\n",
    "                \n",
    "        elif opcion == '5':\n",
    "            print(\"Deteniendo servicio de reconocimiento de voz...\")\n",
    "            stop_speech_recognition()\n",
    "            print(\"Saliendo del programa...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Opción inválida, intenta nuevamente.\")\n",
    "        \n",
    "        # Mostrar nuevamente el menú luego de finalizar la opción seleccionada.\n",
    "        print_menu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43e9da",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
