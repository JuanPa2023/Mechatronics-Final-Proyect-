{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.saving.saving_api import load_model  # ⚠️ Import no estándar de Keras, otra \"from tensorflow.keras.models import load_model\"\n",
    "import cv2  # OpenCV para procesamiento de imágenes/video\n",
    "import mediapipe as mp  # Framework para detección de posturas corporales\n",
    "import numpy as np  # Manejo de arrays numéricos\n",
    "import os  # Interacción con sistema operativo\n",
    "import tensorflow as tf  # Framework de ML\n",
    "from collections import deque  # Estructura de datos tipo cola\n",
    "from sklearn.model_selection import train_test_split  # División de datasets\n",
    "import socket  # Comunicación en red\n",
    "import threading  # Ejecución paralela\n",
    "import psutil  # Monitoreo de recursos del sistema\n",
    "import time  # Medición de tiempos/FPS\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sistema de análisis en tiempo real\n",
    "\n",
    "|_> Procesamiento multimedia\n",
    "\n",
    "    |_> Captura de video (OpenCV)\n",
    "\n",
    "    |_> Detección de posturas (MediaPipe)\n",
    "\n",
    "|_> Modelo de ML\n",
    "\n",
    "    |_> Carga de modelo (Keras/TensorFlow)\n",
    "\n",
    "    |_> Preprocesamiento de datos (NumPy)\n",
    "\n",
    "|_> Gestión de datos\n",
    "\n",
    "    |_> Almacenamiento temporal (deque)\n",
    "\n",
    "    |_> División de datasets (scikit-learn)\n",
    "\n",
    "|_> Monitoreo del sistema\n",
    "\n",
    "    |_> Rendimiento en red (socket)\n",
    "\n",
    "    |_> Uso de recursos (psutil)\n",
    "\n",
    "    |_> Medición FPS (time)\n",
    "\n",
    "|_> Ejecución paralela\n",
    "\n",
    "    |_> Hilos para tareas concurrentes (threading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircularBuffer:\n",
    "    def __init__(self, size, total_landmarks):\n",
    "        # Inicializa buffer con array numpy pre-dimensionado\n",
    "        self.buffer = np.zeros((size, total_landmarks), dtype=np.float32)  \n",
    "        self.size = size  # Capacidad máxima del buffer\n",
    "        self.idx = 0  # Puntero de escritura actual\n",
    "        self.full = False  # Flag de buffer lleno\n",
    "\n",
    "    def add(self, data):\n",
    "        # Almacena datos en posición actual y actualiza índice\n",
    "        self.buffer[self.idx] = data  # Escritura en posición del puntero\n",
    "        self.idx = (self.idx + 1) % self.size  # Incremento circular\n",
    "        self.full = self.full or self.idx == 0  # Actualiza estado de llenado\n",
    "\n",
    "    def get_sequence(self):\n",
    "        # Devuelve secuencia ordenada temporalmente\n",
    "        if self.full:\n",
    "            # Buffer lleno: concatena final + inicio\n",
    "            return np.concatenate([self.buffer[self.idx:], self.buffer[:self.idx]])\n",
    "        #return self.buffer[:self.idx]  # 🚨 Secuencia invertida si no está lleno\n",
    "        return self.buffer[:self.idx][::-1]  # Invertir orden si no está lleno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestión de datos temporales\n",
    "\n",
    "|_> Inicialización\n",
    "\n",
    "    |_> Array pre-asignado (eficiencia memoria)\n",
    "\n",
    "    |_> Control de capacidad fija\n",
    "\n",
    "|_> Operaciones\n",
    "\n",
    "    |_> Adición de datos\n",
    "\n",
    "        |_> Escritura secuencial circular\n",
    "\n",
    "        |_> Actualización automática de estado\n",
    "\n",
    "    |_> Recuperación de secuencia\n",
    "\n",
    "        |_> Manejo de 2 casos (buffer lleno/vacio)\n",
    "        \n",
    "        |_> Reordenamiento temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import time\n",
    "import threading\n",
    "import logging\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "class UDPCamera:\n",
    "    \"\"\"\n",
    "    Cliente UDP mejorado para recibir video con mayor fiabilidad, \n",
    "    manejo de frames y rendimiento.\n",
    "    \"\"\"\n",
    "    def __init__(self, host='0.0.0.0', port=5000, buffer_size=65536, timeout=0.1, auto_start=False):\n",
    "        \"\"\"\n",
    "        Inicialización con parámetros configurables.\n",
    "        \n",
    "        Args:\n",
    "            host: IP a la que se va a enlazar.\n",
    "            port: Puerto UDP para escuchar.\n",
    "            buffer_size: Tamaño máximo del paquete UDP.\n",
    "            timeout: Tiempo de espera del socket en segundos.\n",
    "            auto_start: Si es True, inicia automáticamente la recepción.\n",
    "        \"\"\"\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        # 1. Configuración del socket con verificación del tamaño del buffer\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * buffer_size)\n",
    "        actual_rcvbuf = self.sock.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF)\n",
    "        if actual_rcvbuf < 4 * buffer_size:\n",
    "            logging.warning(f\"El tamaño del buffer de recepción es {actual_rcvbuf} bytes, menor que el solicitado {4 * buffer_size} bytes.\")\n",
    "        self.sock.settimeout(timeout)\n",
    "        \n",
    "        # 2. Manejo de frames: se eliminan variables no utilizadas\n",
    "        self.current_frame = None\n",
    "        self.last_frame = None\n",
    "        self.frame_fragments = {}    # Almacena fragmentos por frame_id\n",
    "        self.frame_timestamps = {}   # Marca de tiempo para cada frame\n",
    "        \n",
    "        # Variables eliminadas: self.current_frame_id, self.fragment_count\n",
    "        \n",
    "        # 3. Configuración de hilos y sincronización\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # 4. Cálculo de FPS\n",
    "        self.fps = 0\n",
    "        self.frame_times = deque(maxlen=30)\n",
    "        self.last_frame_time = time.time()\n",
    "        self.frame_count = 0\n",
    "        \n",
    "        # 5. Gestión de limpieza de frames incompletos\n",
    "        self.last_cleanup_time = time.time()\n",
    "        self.cleanup_interval = 1.0  # Limpieza cada 1 segundo\n",
    "        \n",
    "        # 6. Se elimina el inicio automático en el constructor\n",
    "        if auto_start:\n",
    "            self.start()\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Inicia el receptor UDP con verificación del estado.\"\"\"\n",
    "        if self.isOpened():\n",
    "            logging.info(\"El receptor UDP ya se encuentra en ejecución.\")\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            self.sock.bind((self.host, self.port))\n",
    "            self.running = True\n",
    "            self.thread = threading.Thread(\n",
    "                target=self._receive_frames, \n",
    "                name=\"UDP_Receiver\",\n",
    "                daemon=False  # Se mantiene para permitir una finalización controlada\n",
    "            )\n",
    "            self.thread.start()\n",
    "            time.sleep(0.1)  # Espera breve para la verificación\n",
    "            if self.thread.is_alive():\n",
    "                logging.info(f\"Escuchando en {self.host}:{self.port}\")\n",
    "                return True\n",
    "            else:\n",
    "                logging.error(\"El hilo receptor no se inició correctamente.\")\n",
    "                self.running = False\n",
    "                return False\n",
    "        except PermissionError:\n",
    "            logging.error(f\"El puerto {self.port} requiere privilegios de administrador.\")\n",
    "        except OSError as e:\n",
    "            logging.error(f\"Error de red o conflicto en el puerto: {e.strerror}\")\n",
    "        return False\n",
    "    \n",
    "    def _receive_frames(self):\n",
    "        \"\"\"Hilo principal para recibir paquetes UDP, ensamblar frames y controlar tiempos.\"\"\"\n",
    "        frame_timeout = 0.5  # Tiempo máximo para recibir todos los fragmentos\n",
    "        iteration_count = 0\n",
    "        \n",
    "        while self.running:\n",
    "            try:\n",
    "                # Recepción del paquete UDP\n",
    "                packet_data, _ = self.sock.recvfrom(self.buffer_size)\n",
    "                # 7. Verificar posible truncamiento de datos\n",
    "                if len(packet_data) == self.buffer_size:\n",
    "                    logging.warning(\"El paquete recibido puede estar truncado debido al tamaño del buffer.\")\n",
    "                \n",
    "                # Validar longitud mínima para procesar la cabecera\n",
    "                if len(packet_data) < 8:\n",
    "                    continue  # Descarta paquetes inválidos\n",
    "                \n",
    "                # 8. Decodificación de la cabecera (formato big-endian)\n",
    "                header = packet_data[:8]\n",
    "                frame_id = int.from_bytes(header[0:4], byteorder='big')\n",
    "                fragment_id = int.from_bytes(header[4:6], byteorder='big')\n",
    "                total_fragments = int.from_bytes(header[6:8], byteorder='big')\n",
    "                fragment_data = packet_data[8:]\n",
    "                \n",
    "                with self.lock:\n",
    "                    # Inicializa la entrada para un nuevo frame\n",
    "                    if frame_id not in self.frame_fragments:\n",
    "                        self.frame_fragments[frame_id] = {}\n",
    "                        self.frame_timestamps[frame_id] = time.time()\n",
    "                    \n",
    "                    # Guarda el fragmento (se sobrescribe en caso de duplicados)\n",
    "                    self.frame_fragments[frame_id][fragment_id] = fragment_data\n",
    "                    \n",
    "                    # 9. Reconstrucción del frame cuando se han recibido todos los fragmentos\n",
    "                    if len(self.frame_fragments[frame_id]) == total_fragments:\n",
    "                        # Verifica que los fragmentos estén completos y en orden\n",
    "                        if sorted(self.frame_fragments[frame_id].keys()) == list(range(total_fragments)):\n",
    "                            # Se ensamblan los fragmentos utilizando join para eficiencia\n",
    "                            fragments = [self.frame_fragments[frame_id][i] for i in range(total_fragments)]\n",
    "                            frame_data = b''.join(fragments)\n",
    "                            try:\n",
    "                                frame_array = np.frombuffer(frame_data, dtype=np.uint8)\n",
    "                                decoded_frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)\n",
    "                                if decoded_frame is not None:\n",
    "                                    self.last_frame = self.current_frame\n",
    "                                    self.current_frame = decoded_frame\n",
    "                                    now = time.time()\n",
    "                                    self.frame_times.append(now)\n",
    "                                    if len(self.frame_times) >= 2:\n",
    "                                        self.fps = len(self.frame_times) / (self.frame_times[-1] - self.frame_times[0])\n",
    "                                else:\n",
    "                                    logging.error(\"Error: cv2.imdecode devolvió None.\")\n",
    "                            except Exception as e:\n",
    "                                logging.error(f\"Error al decodificar el frame: {e}\")\n",
    "                        else:\n",
    "                            logging.warning(f\"Frame incompleto para frame_id {frame_id}. Faltan fragmentos.\")\n",
    "                        \n",
    "                        # Limpieza del frame ya procesado\n",
    "                        del self.frame_fragments[frame_id]\n",
    "                        del self.frame_timestamps[frame_id]\n",
    "                \n",
    "                # 10. Limpieza periódica de frames incompletos para mejorar el rendimiento\n",
    "                iteration_count += 1\n",
    "                current_time = time.time()\n",
    "                if current_time - self.last_cleanup_time > self.cleanup_interval:\n",
    "                    with self.lock:\n",
    "                        expired_frames = [\n",
    "                            fid for fid, ts in self.frame_timestamps.items() \n",
    "                            if current_time - ts > frame_timeout\n",
    "                        ]\n",
    "                        for fid in expired_frames:\n",
    "                            logging.info(f\"Limpieza de frame expirado: frame_id {fid}\")\n",
    "                            del self.frame_fragments[fid]\n",
    "                            del self.frame_timestamps[fid]\n",
    "                    self.last_cleanup_time = current_time\n",
    "                \n",
    "            except socket.timeout:\n",
    "                continue  # Timeout esperado, continuar\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error en el receptor UDP: {e}\")\n",
    "                if not self.running:\n",
    "                    break\n",
    "    \n",
    "    def read(self):\n",
    "        \"\"\"\n",
    "        Retorna el último frame completo o el último frame bueno como fallback.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (True, frame) si hay un frame disponible; de lo contrario (False, None)\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            if self.current_frame is not None:\n",
    "                return True, self.current_frame.copy()\n",
    "            elif self.last_frame is not None:\n",
    "                return True, self.last_frame.copy()\n",
    "            return False, None\n",
    "    \n",
    "    def get_fps(self):\n",
    "        \"\"\"Retorna el FPS estimado actual.\"\"\"\n",
    "        with self.lock:\n",
    "            return self.fps\n",
    "    \n",
    "    def isOpened(self):\n",
    "        \"\"\"Verifica si la cámara UDP está en ejecución.\"\"\"\n",
    "        return self.running and self.thread and self.thread.is_alive()\n",
    "    \n",
    "    def release(self):\n",
    "        \"\"\"Detiene el hilo receptor y libera los recursos.\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        with self.lock:\n",
    "            self.frame_fragments.clear()\n",
    "            self.frame_timestamps.clear()\n",
    "            self.current_frame = None\n",
    "            self.last_frame = None\n",
    "        try:\n",
    "            self.sock.close()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al cerrar el socket: {e}\")\n",
    "        logging.info(\"UDP Camera liberada\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial global\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Optimizar MediaPipe\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.45,  # Reducir confianza\n",
    "    min_tracking_confidence=0.45,\n",
    "    model_complexity=0  # Menor complejidad\n",
    ")\n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "dataset_dir = \"dataset_11_90\"\n",
    "model_path = \"gesture_model_me_12_90_4.h5\"\n",
    "sequence_length = 90\n",
    "total_landmarks = 126\n",
    "gestures = []\n",
    "X_mean = None\n",
    "X_std = None\n",
    "\n",
    "num_camara = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FUNCIONES PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def init_system():\n",
    "    global gestures\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    gestures = get_existing_gestures()\n",
    "    \n",
    "def get_existing_gestures():\n",
    "    return sorted([d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DETECCION DE MANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hands():\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")\n",
    "    cap = UDPCamera()  # <-- Cambio aquí\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow(\"Detección de Manos\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    global gestures\n",
    "    gesture = input(\"\\nIngrese la palabra o letra para la cual desea recolectar datos: \").upper()\n",
    "    num_sequences = int(input(\"Ingrese el número de secuencias a capturar (recomendado: 50): \"))\n",
    "    \n",
    "    save_dir = os.path.join(dataset_dir, gesture)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nRecolectando datos para el gesto '{gesture}'. Presiona 'ESC' para cancelar.\")\n",
    "    print(\"Mantenga la seña frente a la cámara...\")\n",
    "    \n",
    "    cap = UDPCamera()\n",
    "    sequence = []\n",
    "    counter = 0\n",
    "\n",
    "    #NUEVO Configurar ventana de landmarks\n",
    "    landmark_window_name = \"Landmarks en Tiempo Real\"\n",
    "    cv2.namedWindow(landmark_window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(landmark_window_name, 640, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        #NUEVO Crear canvas para landmarks\n",
    "        landmark_canvas = np.zeros((480, 640, 3), dtype=np.uint8)  # Canvas negro 640x480\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            all_landmarks = []\n",
    "\n",
    "            #NUEVO Dibujar landmarks en el canvas\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Dibujar en el canvas negro\n",
    "                mp_draw.draw_landmarks(\n",
    "                    landmark_canvas,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                    mp_draw.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "                )\n",
    "            \n",
    "            # Extraer coordenadas para el dataset\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    all_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar si solo hay una mano\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                all_landmarks += [0.0] * 63\n",
    "            \n",
    "            sequence.append(all_landmarks)\n",
    "\n",
    "            # Dibujar en el frame original\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            np.save(os.path.join(save_dir, f\"secuencia_{counter}.npy\"), sequence)\n",
    "            counter += 1\n",
    "            sequence = []\n",
    "            print(f\"Secuencias capturadas: {counter}/{num_sequences}\")\n",
    "\n",
    "        #NUEVO Mostrar información en ambas ventanas\n",
    "        info_text = f\"Secuencias: {counter}/{num_sequences}\"\n",
    "        cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(landmark_canvas, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)\n",
    "        #NEUVO\n",
    "        cv2.imshow(landmark_window_name, landmark_canvas)\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or counter >= num_sequences:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    gestures = get_existing_gestures()\n",
    "    print(f\"\\nSe recolectaron {counter} secuencias para el gesto '{gesture}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_3d_rotation(angles):\n",
    "    \"\"\"Rotación 3D vectorizada para landmarks\"\"\"\n",
    "    rx = tf.convert_to_tensor([\n",
    "        [1, 0, 0],\n",
    "        [0, tf.cos(angles[0]), -tf.sin(angles[0])],\n",
    "        [0, tf.sin(angles[0]), tf.cos(angles[0])]\n",
    "    ])\n",
    "    \n",
    "    ry = tf.convert_to_tensor([\n",
    "        [tf.cos(angles[1]), 0, tf.sin(angles[1])],\n",
    "        [0, 1, 0],\n",
    "        [-tf.sin(angles[1]), 0, tf.cos(angles[1])]\n",
    "    ])\n",
    "    \n",
    "    rz = tf.convert_to_tensor([\n",
    "        [tf.cos(angles[2]), -tf.sin(angles[2]), 0],\n",
    "        [tf.sin(angles[2]), tf.cos(angles[2]), 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    return tf.matmul(rz, tf.matmul(ry, rx))\n",
    "\n",
    "def augment_landmarks(landmarks):\n",
    "    \"\"\"Aumentación 3D mejorada\"\"\"\n",
    "    angles = tf.random.uniform([3], -15.0, 15.0) * (np.pi / 180.0)\n",
    "    rot_matrix = tf_3d_rotation(angles)\n",
    "    \n",
    "    # Aplicar rotación\n",
    "    original_shape = tf.shape(landmarks)\n",
    "    landmarks = tf.reshape(landmarks, [-1, 3])\n",
    "    landmarks = tf.matmul(landmarks, rot_matrix)\n",
    "    landmarks = tf.reshape(landmarks, original_shape)\n",
    "    \n",
    "    # Añadir ruido diferenciado\n",
    "    noise = tf.random.normal(tf.shape(landmarks), mean=0.0, stddev=0.08)\n",
    "    return landmarks + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_augmentation(sequence):\n",
    "    \"\"\"Aumentación 100% en TensorFlow con rotaciones 3D\"\"\"\n",
    "    sequence = tf.cast(sequence, tf.float32)\n",
    "    \n",
    "    # 1. Aplicar rotación 3D\n",
    "    sequence = augment_landmarks(sequence)  # <--- Aquí se usa\n",
    "    \n",
    "    # 2. Ruido Gaussiano (ya incluido en augment_landmarks, pero puedes añadir más)\n",
    "    noise = tf.random.normal(tf.shape(sequence), mean=0.0, stddev=0.05)\n",
    "    sequence = tf.add(sequence, noise)\n",
    "    \n",
    "    # 3. Escalado aleatorio \n",
    "    scale_factor = tf.random.uniform([], 0.9, 1.1)\n",
    "    sequence = tf.multiply(sequence, scale_factor)\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "# Modificar la función create_dataset\n",
    "def create_dataset(X_data, y_data, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (custom_augmentation(x), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    \n",
    "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(augment=True):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for label_idx, gesture in enumerate(gestures):\n",
    "        gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "        sequences = [f for f in os.listdir(gesture_dir) if f.endswith('.npy')]\n",
    "        print(f\"Gesto '{gesture}' - secuencias encontradas: {len(sequences)}\")\n",
    "        \n",
    "        for seq_file in sequences:\n",
    "            seq_path = os.path.join(gesture_dir, seq_file)\n",
    "            sequence = np.load(seq_path)\n",
    "            \n",
    "            if sequence.shape == (sequence_length, total_landmarks):\n",
    "                X.append(sequence)\n",
    "                y.append(label_idx)\n",
    "            else:\n",
    "                print(f\"Secuencia {seq_file} con forma {sequence.shape} ignorada.\")\n",
    "    \n",
    "    return np.array(X, dtype=np.float32), np.array(y), gestures  # Asegurar tipo float32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir correctamente la función generadora\n",
    "def representative_dataset_gen():  \n",
    "    for _ in range(100):  \n",
    "        yield [np.random.rand(1, sequence_length, total_landmarks).astype(np.float32)]  \n",
    "\n",
    "\n",
    "def train_model():\n",
    "    global X_mean, X_std, gestures\n",
    "    \n",
    "    # 1. Verificar datos de entrenamiento\n",
    "    gestures = get_existing_gestures()\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay datos recolectados. Primero recolecte datos de gestos.\")\n",
    "        return\n",
    "\n",
    "    # 2. Cargar y preparar datos\n",
    "    print(\"\\nCargando datos y preparando el entrenamiento...\")\n",
    "    X, y, gestures = load_data(augment=False)  # Cargar sin aumentación inicial\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    # 3. Dividir datos antes de crear el Dataset\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    # 4. Calcular parámetros de normalización\n",
    "    X_mean = np.mean(X_train, axis=(0, 1)).astype(np.float32)\n",
    "    X_std = np.std(X_train, axis=(0, 1)).astype(np.float32)\n",
    "    X_train = (X_train - X_mean) / X_std\n",
    "    X_val = (X_val - X_mean) / X_std  # Aplicar misma normalización a validación\n",
    "\n",
    "    train_dataset = create_dataset(X_train, y_train, augment=True)\n",
    "    val_dataset = create_dataset(X_val, y_val, augment=False)\n",
    "    \n",
    "\n",
    "    # 4. Guardar parámetros de normalización\n",
    "    np.savez('normalization_params_90_4.npz', mean=X_mean, std=X_std)\n",
    "    \n",
    "    # 5. Arquitectura optimizada del modelo\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))\n",
    "    \n",
    "    # Capa de atención espacial\n",
    "    attention = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)\n",
    "    x = tf.keras.layers.Concatenate()([inputs, attention])\n",
    "    \n",
    "    # Bloques convolucionales\n",
    "    x = tf.keras.layers.Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Attention temporal\n",
    "    x = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=32)(x, x)\n",
    "    \n",
    "    # Pooling final\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Capas densas\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(len(gestures), activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # 6. Compilación y entrenamiento\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, global_clipnorm=1.0),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        weighted_metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,  # Usar dataset de validación explícito\n",
    "        epochs=50,\n",
    "        verbose=1\n",
    "    )\n",
    "    # 7. Guardar modelo y resultados\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nModelo guardado en {model_path}\")\n",
    "    \n",
    "    # 8. Conversión a TFLite con configuraciones especiales\n",
    "    # Conversión TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset_gen  # Función correcta\n",
    "    converter.target_spec.supported_ops = [  \n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter.target_spec.supported_types = [tf.int8]\n",
    "    converter._experimental_default_to_single_batch_in_tensor_list_ops = True\n",
    "    \n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        with open('model_quantized_90_4.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(\"\\nModelo TFLite exportado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError en conversión TFLite: {str(e)}\")\n",
    "    \n",
    "    # Mostrar métricas finales\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Precisión de validación final: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite():\n",
    "    try:\n",
    "        # Cargar el modelo entrenado\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Configurar el conversor con parámetros especiales\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        \n",
    "        # Añadir estas 3 líneas clave para compatibilidad con LSTM\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        converter._experimental_lower_tensor_list_ops = False\n",
    "        converter.allow_custom_ops = True  # Permitir operaciones personalizadas\n",
    "        \n",
    "        # Realizar la conversión\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo cuantizado\n",
    "        with open('model_quantized_90_4.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "            \n",
    "        print(\"\\n✅ Conversión a TFLite exitosa!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error en conversión: {str(e)}\")\n",
    "        print(\"Posibles soluciones:\")\n",
    "        print(\"1. Verifique que el modelo .h5 existe\")\n",
    "        print(\"2. Actualice TensorFlow: pip install --upgrade tensorflow\")\n",
    "        print(\"3. Reinicie el runtime/kernel\")\n",
    "\n",
    "    global gestures\n",
    "    gestures = get_existing_gestures()\n",
    "    print(\"Gestos cargados para evaluación:\", gestures)\n",
    "\n",
    "    print(\"Salida del modelo:\", model.output_shape)\n",
    "\n",
    "\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    # Generador de datos de ejemplo para calibración\n",
    "    for _ in range(100):\n",
    "        yield [np.random.randn(1, sequence_length, total_landmarks).astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_sequence(seq_array):\n",
    "    \"\"\"Extrapolación lineal para secuencias incompletas\"\"\"\n",
    "    last_valid = np.where(seq_array.sum(axis=1) != 0)[0]\n",
    "    if len(last_valid) < 2:\n",
    "        return seq_array\n",
    "    \n",
    "    x = np.array([0, 1])\n",
    "    for col in range(seq_array.shape[1]):\n",
    "        y = seq_array[last_valid[-2:], col]\n",
    "        coeffs = np.polyfit(x, y, 1)\n",
    "        seq_array[last_valid[-1]+1:, col] = np.polyval(coeffs, np.arange(2, 2+len(seq_array)-last_valid[-1]-1))\n",
    "    \n",
    "    return seq_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \"\"\"\n",
    "    Función para evaluar el modelo de reconocimiento de gestos en tiempo real.\n",
    "    Procesa video de la cámara, detecta manos, y predice gestos basados en secuencias\n",
    "    de landmarks de MediaPipe.\n",
    "    \"\"\"\n",
    "    global gestures\n",
    "    gestures = get_existing_gestures()\n",
    "\n",
    "    # Constantes para mantener consistencia\n",
    "    MODEL_PATH = \"model_quantized_90_4.tflite\"\n",
    "    NORMALIZATION_PARAMS_PATH = 'normalization_params_90_4.npz'\n",
    "    \n",
    "    # 1. Verificar si existe el modelo\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"\\n¡Primero debe entrenar y convertir el modelo!\")\n",
    "        return\n",
    "    \n",
    "    # 2. Cargar parámetros y modelo\n",
    "    try:\n",
    "        with np.load(NORMALIZATION_PARAMS_PATH) as data:\n",
    "            X_mean = data['mean']\n",
    "            X_std = data['std']\n",
    "            \n",
    "        # Cargar el modelo TFLite\n",
    "        interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)\n",
    "        interpreter.allocate_tensors()\n",
    "\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "        print(\"Output details shape:\", output_details['shape'])\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError crítico al cargar modelo: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 3. Configuración de cámara\n",
    "    cap = UDPCamera()\n",
    "    \n",
    "    # 4. Variables de estado\n",
    "    buffer = CircularBuffer(sequence_length)  # Usar solo buffer para secuencias\n",
    "    prediction_history = deque(maxlen=15)     # Para suavizado de predicciones\n",
    "    current_gesture = \"Esperando...\"\n",
    "    current_confidence = 0.0\n",
    "    latency = 0.0\n",
    "    fps_counter = deque(maxlen=30)\n",
    "    last_time = time.time()\n",
    "    high_sensitivity_mode = False\n",
    "    unknown_counter = 0\n",
    "\n",
    "    # 5. Configurar afinidad de CPU para mejor rendimiento\n",
    "    try:\n",
    "        p = psutil.Process()\n",
    "        p.cpu_affinity([0, 1])  # Hilo principal en núcleos 0-1\n",
    "    except Exception as e:\n",
    "        print(f\"Advertencia: No se pudo configurar afinidad de CPU: {str(e)}\")\n",
    "\n",
    "    # 6. Bucle principal optimizado\n",
    "    while True:\n",
    "        # Medición de FPS\n",
    "        current_time = time.time()\n",
    "        fps_counter.append(1/(current_time - last_time + 1e-7))\n",
    "        last_time = current_time\n",
    "\n",
    "        # Capturar frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Procesar landmarks de manos\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        # Inicializar landmarks con ceros (para casos sin detección)\n",
    "        landmarks = [0.0] * total_landmarks\n",
    "        \n",
    "        # Extraer landmarks si hay manos detectadas\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:  # Máximo 2 manos\n",
    "                for lm in hand.landmark:\n",
    "                    hand_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar con ceros si es necesario\n",
    "            if len(hand_landmarks) < total_landmarks:\n",
    "                hand_landmarks += [0.0] * (total_landmarks - len(hand_landmarks))\n",
    "            else:\n",
    "                hand_landmarks = hand_landmarks[:total_landmarks]  # Truncar si excede\n",
    "                \n",
    "            landmarks = hand_landmarks\n",
    "\n",
    "        # Añadir al buffer circular\n",
    "        buffer.add(landmarks)\n",
    "        \n",
    "        # Ajustar modo alta sensibilidad si está activado\n",
    "        if high_sensitivity_mode:\n",
    "            frame = cv2.resize(frame, (320, 240))  # Reducir resolución para aumentar velocidad\n",
    "        \n",
    "        # Realizar predicción cuando el buffer esté completo\n",
    "        if buffer.full:\n",
    "            try:\n",
    "                # Obtener secuencia y preprocesar\n",
    "                seq_array = buffer.get_sequence()\n",
    "                seq_array = (seq_array - X_mean) / (X_std + 1e-7)\n",
    "                input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "                \n",
    "                # Verificar forma de los datos de entrada\n",
    "                if input_data.shape != tuple(input_details['shape']):\n",
    "                    print(f\"Error: Forma esperada {input_details['shape']}, obtenida {input_data.shape}\")\n",
    "                    continue\n",
    "                \n",
    "                # Inferencia con medición de latencia\n",
    "                start_time = time.perf_counter()\n",
    "                interpreter.set_tensor(input_details['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                prediction = interpreter.get_tensor(output_details['index'])[0]\n",
    "                latency = (time.perf_counter() - start_time) * 1000  # ms\n",
    "\n",
    "                # Sistema de fallback para gestos desconocidos\n",
    "                if np.max(prediction) < 0.5:\n",
    "                    unknown_counter += 1\n",
    "                    if unknown_counter >= 3:\n",
    "                        high_sensitivity_mode = not high_sensitivity_mode\n",
    "                        print(f\"Cambiando a modo: {'Alta Sensibilidad' if high_sensitivity_mode else 'Normal'}\")\n",
    "                        unknown_counter = 0\n",
    "                else:\n",
    "                    unknown_counter = 0\n",
    "                \n",
    "                # Suavizado temporal con promedio de predicciones recientes\n",
    "                prediction_history.append(prediction)\n",
    "                smoothed_pred = np.mean(prediction_history, axis=0)\n",
    "                predicted_idx = np.argmax(smoothed_pred)\n",
    "                confidence = smoothed_pred[predicted_idx]\n",
    "                \n",
    "                # Umbral dinámico según modo de sensibilidad\n",
    "                confidence_threshold = 0.7 if not high_sensitivity_mode else 0.5\n",
    "                if confidence > confidence_threshold:\n",
    "                    current_gesture = gestures[predicted_idx]\n",
    "                    current_confidence = confidence\n",
    "                else:\n",
    "                    #current_gesture = \"Desconocido\"\n",
    "                    current_confidence = 0.0\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en predicción: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Visualización de métricas en pantalla\n",
    "        fps = np.mean(fps_counter) if fps_counter else 0\n",
    "        \n",
    "        # Información de rendimiento\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 110), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Latencia: {latency:.1f}ms\", (10, 140), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        \n",
    "        # Información de modo\n",
    "        mode_color = (0, 255, 0) if not high_sensitivity_mode else (0, 0, 255)\n",
    "        cv2.putText(frame, f\"Modo: {'Normal' if not high_sensitivity_mode else 'Alta Sensibilidad'}\", \n",
    "                   (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.6, mode_color, 2)\n",
    "        \n",
    "        # Información de predicción\n",
    "        cv2.putText(frame, f\"Prediccion: {current_gesture}\", (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2) #BGR\n",
    "        cv2.putText(frame, f\"Confianza: {current_confidence:.2%}\", (10, 70),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Mostrar frame y detectar tecla ESC para salir\n",
    "        cv2.imshow(\"Predicciones en Tiempo Real\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # Liberar recursos\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las mejoras clave que he hecho a su evaluate() la función incluye:\n",
    "\n",
    "* Manejo de ruta de modelo consistente: He definido constantes en la parte superior de la función tanto para el modelo como para los archivos de parámetros de normalización para garantizar la consistencia.\n",
    "\n",
    "* Gestión de secuencias simplificada: He eliminado el redundante sequence deque y consistentemente utilizó el CircularBuffer para gestionar secuencias de puntos de referencia manuales.\n",
    "\n",
    "* Procesamiento de punto de referencia más limpio: La extracción de puntos de referencia ahora es más sencilla, con un manejo adecuado de los casos en que hay demasiados o muy pocos puntos de referencia.\n",
    "\n",
    "* Mejor manejo de errores: Se agregaron mensajes de error más específicos e implementaron un continue declaración para manejar errores durante la predicción para que el bucle no se bloquee.\n",
    "\n",
    "* Código no utilizado eliminado: Se han eliminado el código comentado y las variables no utilizadas.\n",
    "\n",
    "* Se agregaron comentarios más detallados: Cada sección de la función ahora tiene comentarios claros que explican lo que hace.\n",
    "\n",
    "* Estructura mejorada: La función ahora sigue un flujo más lógico con secciones distintas para la inicialización, el procesamiento del bucle principal y la visualización.\n",
    "\n",
    "* Prevención de errores: Se agregó un bloque de prueba excepto para la configuración de afinidad de la CPU, ya que esto podría no funcionar en todas las plataformas.\n",
    "\n",
    "* Mejor gestión de marcos: Añadido a continue instrucción cuando falla la recuperación de fotogramas en lugar de procesar datos potencialmente no válidos.\n",
    "\n",
    "* Retroalimentación visual para cambios de modo: Se agregó una declaración de impresión para informar cuando el sistema cambia entre los modos normal y de alta sensibilidad.\n",
    "\n",
    "Esta función revisada debe ser más confiable, mantenible y consistente, al tiempo que conserva todas las características avanzadas de su implementación original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menú principal\n",
    "def main():\n",
    "    init_system()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Detectar Manos\")\n",
    "        print(\"2. Recolectar Datos\")\n",
    "        print(\"3. Entrenar Modelo, y despues ir a convertir a TFlite\")\n",
    "        print(\"4. Evaluar\")\n",
    "        print(\"6. Convertir a TFLite\")  # Nueva opción\n",
    "        print(\"7. Salir\")\n",
    "        \n",
    "        choice = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            detect_hands()\n",
    "        elif choice == '2':\n",
    "            collect_data()\n",
    "        elif choice == '3':\n",
    "            train_model()\n",
    "        elif choice == '4':\n",
    "            evaluate()\n",
    "        elif choice == '6':  # Nueva opción de conversión\n",
    "            convert_to_tflite()\n",
    "        elif choice == '7':\n",
    "            print(\"\\n¡Hasta luego!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nOpción inválida. Por favor, intente de nuevo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "Output details shape: [1 5]\n",
      "UDP Camera started on 0.0.0.0:5000\n",
      "Cambiando a modo: Alta Sensibilidad\n",
      "UDP Camera released\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
