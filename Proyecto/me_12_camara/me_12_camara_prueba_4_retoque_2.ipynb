{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.saving.saving_api import load_model  # ⚠️ Import no estándar de Keras, otra \"from tensorflow.keras.models import load_model\"\n",
    "import cv2  # OpenCV para procesamiento de imágenes/video\n",
    "import mediapipe as mp  # Framework para detección de posturas corporales\n",
    "import numpy as np  # Manejo de arrays numéricos\n",
    "import os  # Interacción con sistema operativo\n",
    "import tensorflow as tf  # Framework de ML\n",
    "from collections import deque  # Estructura de datos tipo cola\n",
    "from sklearn.model_selection import train_test_split  # División de datasets\n",
    "\n",
    "import socket  # Comunicación en red\n",
    "import threading  # Ejecución paralela\n",
    "\n",
    "import psutil  # Monitoreo de recursos del sistema\n",
    "import time  # Medición de tiempos/FPS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircularBuffer:\n",
    "    def __init__(self, size):\n",
    "        # Inicializa buffer con array numpy pre-dimensionado\n",
    "        #self.buffer = np.zeros((size), dtype=np.float32)  \n",
    "        self.buffer = np.zeros((size, total_landmarks), dtype=np.float32) #para que funcione ponerlo en los parametros y modificar evaluation, ahi sigue la explicacion \n",
    "        self.size = size  # Capacidad máxima del buffer\n",
    "        self.idx = 0  # Puntero de escritura actual\n",
    "        self.full = False  # Flag de buffer lleno \n",
    "\n",
    "    def add(self, data):\n",
    "        # Almacena datos en posición actual y actualiza índice\n",
    "        self.buffer[self.idx] = data  # Escritura en posición del puntero\n",
    "        self.idx = (self.idx + 1) % self.size  # Incremento circular\n",
    "        self.full = self.full or self.idx == 0  # Actualiza estado de llenado\n",
    "\n",
    "    def get_sequence(self):\n",
    "        # Devuelve secuencia ordenada temporalmente\n",
    "        if self.full:\n",
    "            # Buffer lleno: concatena final + inicio\n",
    "            return np.concatenate([self.buffer[self.idx:], self.buffer[:self.idx]])\n",
    "        return self.buffer[:self.idx]  # 🚨 Secuencia invertida si no está lleno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#class del me_12_4 va con server-UDPv3,4,5,8\n",
    "class UDPCamera:\n",
    "    def __init__(self):\n",
    "        self.host = '0.0.0.0'\n",
    "        self.port = 5000\n",
    "        self.buffer_size = 131072 #128kb\n",
    "        self.mtu = 1400  # Tamaño máximo de fragmento\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.settimeout(2)\n",
    "        self.frame = None\n",
    "        self.fragments = []  # Almacena fragmentos del frame\n",
    "        self.full = False\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.lock = threading.Lock()  # Para acceso thread-safe\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.sock.bind((self.host, self.port))\n",
    "            \n",
    "            self.thread = threading.Thread(target=self._receive_frames, daemon=True)\n",
    "            self.thread.start()\n",
    "\n",
    "    def _receive_frames(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                # Recibir fragmento\n",
    "                fragment, _ = self.sock.recvfrom(self.buffer_size)\n",
    "                \n",
    "                with self.lock:\n",
    "                    self.fragments.append(fragment)\n",
    "\n",
    "                    # Detectar último fragmento (tamaño < MTU)\n",
    "                    if len(fragment) < self.mtu:\n",
    "                        # Reconstruir frame completo\n",
    "                        frame_bytes = b''.join(self.fragments)\n",
    "                        self.fragments = []  # Resetear fragmentos\n",
    "\n",
    "                        # Decodificar y almacenar frame\n",
    "                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)\n",
    "                        self.frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "            except socket.timeout:\n",
    "                #print(\"[WARN] Timeout esperando fragmentos UDP\") \n",
    "                continue\n",
    "            except Exception as e: \n",
    "                print(f\"[ERROR] Recepción UDP: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def read(self):\n",
    "        with self.lock:\n",
    "            if self.frame is not None:\n",
    "                return True, self.frame.copy()  # Copia para thread-safe\n",
    "            return False, None\n",
    "\n",
    "    def isOpened(self):\n",
    "        return self.running\n",
    "\n",
    "    def release(self):\n",
    "        self.running = False\n",
    "        with self.lock:\n",
    "            self.fragments = []\n",
    "            self.frame = None\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        self.sock.close()\n",
    "        print(\"Cámara UDP liberada\")\n",
    "\n",
    "    def __del__(self):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class del me_12_4_retoque va con server-UDPv6 y v7\n",
    "class UDPCamera: #Se define la clase UDPCamera, que implementa un cliente de cámara utilizando el protocolo UDP.    \n",
    "    \"\"\"\n",
    "    Enhanced UDP camera client that receives video frames over UDP with improved\n",
    "    reliability, frame management, and performance.\n",
    "    \"\"\"\n",
    "    #def __init__(self, host='0.0.0.0', port=5000, buffer_size=65536, timeout=0.1): \n",
    "    def __init__(self, host='0.0.0.0', port=5000, buffer_size=131072, timeout=0.1): \n",
    "        #Se define el método constructor __init__\n",
    "        #el cual permite inicializar una instancia de UDPCamera con parámetros configurables:\n",
    "        \"\"\"\n",
    "        Initialize the UDP camera with configurable parameters.\n",
    "        \n",
    "        Args:\n",
    "            host: IP address to bind to\n",
    "            port: UDP port to listen on\n",
    "            buffer_size: Maximum UDP packet size to receive\n",
    "            timeout: Socket timeout in seconds\n",
    "        \"\"\"\n",
    "        self.host = host #Se asigna el valor del parámetro host al atributo de la instancia self.host\n",
    "        self.port = port #Se asigna el valor del parámetro port al atributo self.port.\n",
    "        self.buffer_size = buffer_size #Se asigna el valor del parámetro buffer_size al atributo self.buffer_size.\n",
    "        \n",
    "        # Socket configuration:\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) \n",
    "        #Se crea un socket UDP utilizando la familia de \n",
    "        #direcciones IPv4 (AF_INET)\n",
    "        #el tipo de socket datagrama (SOCK_DGRAM). Este socket se asigna al atributo self.sock.\n",
    "\n",
    "        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * buffer_size)  \n",
    "        #Se configura la opción del socket para ampliar el tamaño del buffer de recepción. \n",
    "        # Se utiliza setsockopt con el nivel SOL_SOCKET y la opción SO_RCVBUF para establecer \n",
    "        # un buffer 4 veces mayor que el tamaño indicado en buffer_size, \n",
    "        # lo cual ayuda a recibir paquetes grandes o múltiples paquetes sin perder datos.\n",
    "\n",
    "        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)  # Permitir reusar el puerto (NUEVO EXPERIMENTAL)\n",
    "\n",
    "\n",
    "        self.sock.settimeout(timeout)\n",
    "        #Se establece un timeout en el socket con el valor del parámetro timeout,\n",
    "        #lo que permite que las operaciones de recepción sean más ágiles y evita bloqueos prolongados.\n",
    "\n",
    "        \n",
    "        # Frame management\n",
    "        self.current_frame = None #esta variable almacenará el frame actualmente reconstruido.\n",
    "        self.last_frame = None  # Esta variable guarda el último frame válido recibido, sirviendo como respaldo en caso de errores o ausencia de nuevos frames.\n",
    "        self.frame_fragments = {}  #diccionario vacío. Se utilizará para almacenar fragmentos de datos de cada frame, indexados por el identificador del frame.\n",
    "        self.frame_timestamps = {}  #diccionario vacío, para registrar el momento en que se empezó a recibir cada frame. Esto ayuda a gestionar tiempos de espera y descartar frames incompletos.\n",
    "        self.fragment_count = 0 #contará el número de fragmentos recibidos para el frame actual.\n",
    "        \n",
    "        # Thread control\n",
    "        self.running = False #Esta bandera indicará si el hilo de recepción está activo.\n",
    "        self.thread = None #más adelante contendrá el objeto del hilo encargado de recibir los frames.\n",
    "        self.lock = threading.Lock() #Se crea un objeto Lock a través de threading.Lock() y se asigna a self.lock. Este lock se usará para sincronizar el acceso a variables compartidas entre hilos.\n",
    "        self.fps = 0 #la variable que almacenará la tasa de frames por segundo (FPS) calculada.\n",
    "        self.last_frame_time = time.time() #Se guarda el tiempo actual en self.last_frame_time mediante time.time(), para posteriormente calcular la diferencia de tiempo entre frames.\n",
    "        self.frame_count = 0 #llevará el conteo de frames recibidos.\n",
    "        self.frame_times = deque(maxlen=30)  # For FPS calculation\n",
    "        #Se crea una estructura deque con un máximo de 30 elementos asignada a self.frame_times. \n",
    "        # Este deque se utilizará para almacenar los tiempos de recepción de los últimos frames \n",
    "        # y calcular el FPS de forma promedio.\n",
    "\n",
    "        \n",
    "        # Start receiving thread\n",
    "        self.start() \n",
    "        #Se llama al método self.start(), el cual se encargará de iniciar el proceso en segundo plano \n",
    "        # para recibir y reconstruir los frames entrantes por UDP.\n",
    "    \n",
    "    def start(self):  # Define el método start de la clase UDPCamera, encargado de iniciar el hilo receptor UDP\n",
    "        \n",
    "        \"\"\"Start the UDP receiver thread.\"\"\"  # Docstring que describe la función del método\n",
    "        if not self.running:  # Verifica si el receptor UDP aún no está en ejecución (self.running es False)\n",
    "            try:  # Inicia un bloque try para capturar posibles errores al iniciar el receptor\n",
    "                self.sock.bind((self.host, self.port))  # Asocia (bind) el socket a la dirección IP (self.host) y puerto (self.port) configurados\n",
    "                self.running = True  # Marca el receptor como activo estableciendo self.running a True\n",
    "                self.thread = threading.Thread(target=self._receive_frames, daemon=True)  # Crea un nuevo hilo que ejecutará el método _receive_frames; el hilo es daemon para que se cierre al finalizar el programa principal\n",
    "                self.thread.start()  # Inicia la ejecución del hilo creado para la recepción de frames\n",
    "                print(f\"UDP Camera started on {self.host}:{self.port}\")  # Imprime un mensaje en consola indicando que la cámara UDP ha iniciado correctamente en la dirección y puerto especificados\n",
    "            except Exception as e:  # Captura cualquier excepción que pueda ocurrir durante la inicialización del receptor\n",
    "                print(f\"Failed to start UDP camera: {e}\")  # Imprime un mensaje de error detallando la excepción encontrada\n",
    "\n",
    "    \n",
    "    def _receive_frames(self):  # Método privado que se ejecuta en un hilo para recibir y reconstruir frames de video desde paquetes UDP\n",
    "        \"\"\"Background thread that receives and reconstructs video frames from UDP packets.\"\"\"  # Docstring que describe la función del método\n",
    "        frame_timeout = 0.5  # Define el tiempo máximo (0.5 segundos) para recibir todos los fragmentos de un frame\n",
    "        \n",
    "        while self.running:  # Bucle que se ejecuta continuamente mientras la cámara esté en funcionamiento\n",
    "            try:  # Intenta ejecutar el siguiente bloque, capturando errores si ocurren\n",
    "                # Receive a packet\n",
    "                packet_data, _ = self.sock.recvfrom(self.buffer_size)  # Recibe un paquete UDP con un tamaño máximo definido por buffer_size\n",
    "                \n",
    "                # First 8 bytes: frame_id (4 bytes) + fragment_id (2 bytes) + total_fragments (2 bytes)\n",
    "                if len(packet_data) < 8:  # Verifica que el paquete tenga al menos 8 bytes (el tamaño mínimo del encabezado)\n",
    "                    continue  # Si el paquete es demasiado corto, se ignora y se continúa con la siguiente iteración\n",
    "                \n",
    "                header = packet_data[:8]  # Extrae los primeros 8 bytes del paquete, que contienen el encabezado con identificadores\n",
    "                frame_id = int.from_bytes(header[0:4], byteorder='big')  # Convierte los primeros 4 bytes en un entero que representa el ID del frame, usando orden big-endian\n",
    "                fragment_id = int.from_bytes(header[4:6], byteorder='big')  # Convierte los siguientes 2 bytes en un entero que representa el ID del fragmento\n",
    "                total_fragments = int.from_bytes(header[6:8], byteorder='big')  # Convierte los últimos 2 bytes en un entero que indica el número total de fragmentos que componen el frame\n",
    "                \n",
    "                # Image data starts after header\n",
    "                fragment_data = packet_data[8:]  # Extrae los datos de imagen que siguen al encabezado de 8 bytes\n",
    "                \n",
    "                with self.lock:  # Adquiere un bloqueo para garantizar el acceso seguro a las variables compartidas entre hilos\n",
    "                    # Initialize tracking for new frame\n",
    "                    if frame_id not in self.frame_fragments:  # Si el frame_id aún no existe en el diccionario de fragmentos\n",
    "                        self.frame_fragments[frame_id] = {}  # Inicializa un diccionario para almacenar los fragmentos de este frame\n",
    "                        self.frame_timestamps[frame_id] = time.time()  # Registra el tiempo actual como el inicio de la recepción de este frame\n",
    "                    \n",
    "                    # Store this fragment\n",
    "                    self.frame_fragments[frame_id][fragment_id] = fragment_data  # Almacena el fragmento recibido, usando fragment_id como clave\n",
    "                    \n",
    "                    # Check if we have all fragments for this frame\n",
    "                    if len(self.frame_fragments[frame_id]) == total_fragments:  # Si se han recibido tantos fragmentos como se esperaba para el frame\n",
    "                        # Reconstruct the complete frame\n",
    "                        frame_data = b''  # Inicializa una variable de bytes para concatenar los fragmentos y reconstruir el frame\n",
    "                        for i in range(total_fragments):  # Itera sobre el rango de fragmentos esperados (de 0 a total_fragments-1)\n",
    "                            if i in self.frame_fragments[frame_id]:  # Verifica que el fragmento con índice i esté presente\n",
    "                                frame_data += self.frame_fragments[frame_id][i]  # Añade el fragmento al frame_data en orden\n",
    "                            else:\n",
    "                                # Missing fragment, discard frame\n",
    "                                break  # Si falta algún fragmento, sale del bucle y se descarta la reconstrucción del frame\n",
    "                        else:  # Este bloque se ejecuta solo si no se interrumpe el bucle (es decir, si se recibieron todos los fragmentos)\n",
    "                            # All fragments received, decode frame\n",
    "                            try:  # Intenta decodificar el frame reconstruido\n",
    "                                frame_array = np.frombuffer(frame_data, dtype=np.uint8)  # Convierte los datos binarios en un array de NumPy de tipo uint8\n",
    "                                decoded_frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)  # Decodifica el array en una imagen a color usando OpenCV\n",
    "                                \n",
    "                                if decoded_frame is not None:  # Si la imagen se decodificó correctamente\n",
    "                                    # Successfully decoded a valid frame\n",
    "                                    self.last_frame = self.current_frame  # Actualiza last_frame con el frame actual antes de reemplazarlo\n",
    "                                    self.current_frame = decoded_frame  # Establece el frame decodificado como el frame actual\n",
    "                                    \n",
    "                                    # Calculate FPS\n",
    "                                    now = time.time()  # Obtiene el tiempo actual para el cálculo del FPS\n",
    "                                    self.frame_times.append(now)  # Agrega el tiempo actual a la lista de tiempos de frames\n",
    "                                    if len(self.frame_times) >= 2:  # Si hay al menos dos tiempos registrados\n",
    "                                        self.fps = len(self.frame_times) / (self.frame_times[-1] - self.frame_times[0])  # Calcula los FPS dividiendo el número de frames por la diferencia de tiempo entre el primer y el último frame registrado\n",
    "                            except Exception as e:  # Captura excepciones que puedan ocurrir durante la decodificación del frame\n",
    "                                print(f\"Error decoding frame: {e}\")  # Imprime un mensaje de error con la descripción de la excepción\n",
    "                        \n",
    "                        # Clean up this frame's fragments regardless of success\n",
    "                        del self.frame_fragments[frame_id]  # Elimina los fragmentos almacenados para el frame procesado, liberando memoria\n",
    "                        del self.frame_timestamps[frame_id]  # Elimina el timestamp asociado al frame procesado\n",
    "                \n",
    "                # Clean up old incomplete frames\n",
    "                current_time = time.time()  # Obtiene el tiempo actual para comparar con los timestamps de frames incompletos\n",
    "                with self.lock:  # Adquiere nuevamente el bloqueo para manipular datos compartidos de manera segura\n",
    "                    expired_frames = [fid for fid, ts in self.frame_timestamps.items() \n",
    "                                     if current_time - ts > frame_timeout]  # Crea una lista de IDs de frames cuyo tiempo transcurrido supera el frame_timeout, indicando que están incompletos\n",
    "                    for frame_id in expired_frames:  # Itera sobre cada frame que ha excedido el tiempo límite\n",
    "                        del self.frame_fragments[frame_id]  # Elimina los fragmentos del frame expirado\n",
    "                        del self.frame_timestamps[frame_id]  # Elimina el timestamp del frame expirado\n",
    "            except Exception as e:  # Captura excepciones generales que puedan ocurrir en el bloque principal de recepción\n",
    "                print(f\"Error in UDP receiver: {e}\")  # Imprime un mensaje de error indicando la falla en la recepción\n",
    "\n",
    "            \n",
    "            except socket.timeout:\n",
    "                # This is normal, just continue\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(f\"Error in UDP receiver: {e}\")\n",
    "                if not self.running:\n",
    "                    break\n",
    "    \n",
    "    def read(self):  # Define el método 'read' que se utiliza para obtener el último frame completo recibido\n",
    "        \"\"\"\n",
    "        Read the latest complete frame.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (success, frame) where success is True if a frame is available\n",
    "        \"\"\"  # Docstring que explica que el método retorna una tupla (éxito, frame) donde éxito es True si hay un frame disponible\n",
    "        with self.lock:  # Adquiere el bloqueo (lock) para asegurar que el acceso a los frames sea thread-safe\n",
    "            if self.current_frame is not None:  # Verifica si hay un frame actual disponible\n",
    "                return True, self.current_frame.copy()  # Devuelve True y una copia del frame actual para evitar modificaciones accidentales\n",
    "            elif self.last_frame is not None:  # Si no hay un frame actual, verifica si existe un último frame válido (fallback)\n",
    "                # Return last good frame as fallback\n",
    "                return True, self.last_frame.copy()  # Devuelve True y una copia del último frame válido, como respaldo\n",
    "            return False, None  # Si ninguno de los frames está disponible, retorna False y None\n",
    "\n",
    "    \n",
    "    def get_fps(self):  # Método para obtener los FPS (frames por segundo) estimados actualmente\n",
    "            \"\"\"Get the current estimated FPS.\"\"\"  # Docstring que explica que retorna los FPS estimados\n",
    "            with self.lock:  # Adquiere el lock para asegurar acceso thread-safe a la variable compartida\n",
    "                return self.fps  # Retorna el valor actual de FPS\n",
    "\n",
    "    def isOpened(self):  # Método que verifica si la cámara está en funcionamiento\n",
    "            \"\"\"Check if the camera is running.\"\"\"  # Docstring que indica que verifica el estado de la cámara\n",
    "            return self.running and self.thread and self.thread.is_alive()  # Retorna True si 'running' es True, existe un hilo y dicho hilo está activo\n",
    "\n",
    "    def release(self):  # Método para detener el hilo receptor y liberar los recursos asociados\n",
    "            \"\"\"Stop the receiver thread and release resources.\"\"\"  # Docstring que explica el propósito del método\n",
    "            self.running = False  # Detiene el proceso de recepción, marcando 'running' como False\n",
    "            if self.thread and self.thread.is_alive():  # Verifica si existe un hilo y si este sigue activo\n",
    "                self.thread.join(timeout=1)  # Espera hasta 1 segundo a que el hilo finalice su ejecución\n",
    "            \n",
    "            with self.lock:  # Adquiere el lock para modificar de forma segura las variables compartidas\n",
    "                self.frame_fragments.clear()  # Limpia el diccionario de fragmentos de frames\n",
    "                self.frame_timestamps.clear()  # Limpia el diccionario de timestamps asociados a cada frame\n",
    "                self.current_frame = None  # Resetea el frame actual a None\n",
    "                self.last_frame = None  # Resetea el último frame válido a None\n",
    "            \n",
    "            try:  # Intenta cerrar el socket\n",
    "                self.sock.shutdown(socket.SHUT_RDWR)  # Cierra la conexión del socket (NUEVO EXPERIMENTAL)\n",
    "                self.sock.close()  # Cierra el socket UDP para liberar el recurs    \n",
    "            except:  # Si ocurre algún error al cerrar el socket\n",
    "                pass  # Ignora el error y continúa sin interrumpir el flujo\n",
    "            \n",
    "            print(\"UDP Camera released\")  # Imprime un mensaje indicando que la cámara UDP ha sido liberada\n",
    " \n",
    "    def __del__(self):  # Método destructor que se llamaría al eliminar la instancia (comentado para no ejecutarse automáticamente)\n",
    "        self.release()  # Llama al método release para asegurarse de liberar los recursos al eliminar el objeto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial global\n",
    "mp_hands = mp.solutions.hands  # Se asigna el módulo de detección de manos de MediaPipe a la variable 'mp_hands'\n",
    "\n",
    "# Optimizar MediaPipe\n",
    "hands = mp_hands.Hands(  # Se crea una instancia del detector de manos con parámetros optimizados para video en tiempo real\n",
    "    static_image_mode=False,  # Se desactiva el modo de imagen estática para permitir el procesamiento continuo de video\n",
    "    max_num_hands=2,  # Se limita la detección a un máximo de 2 manos por frame\n",
    "    min_detection_confidence=0.45,  # Se establece la confianza mínima para la detección de manos (valor reducido para mayor velocidad)\n",
    "    min_tracking_confidence=0.45,  # Se establece la confianza mínima para el seguimiento de manos entre frames\n",
    "    model_complexity=0  # Se utiliza un modelo de menor complejidad para acelerar el procesamiento\n",
    ")\n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils  # Se asigna la utilidad de dibujo de MediaPipe a 'mp_draw' para dibujar los landmarks en los frames\n",
    "\n",
    "\"\"\"\n",
    "MODEL_PATH = \"model_quantized_90_4_modelo_del_v3.tflite\"\n",
    "NORMALIZATION_PARAMS_PATH = 'normalization_params_90_4_modelo_del_v3.npz'\n",
    "dataset_dir = \"dataset_11_90\"  # Se define el directorio donde se guardarán las secuencias de datos (dataset)\n",
    "model_path = \"gesture_model_me_12_90_4_modelo_del_v3.h5\"  # Se especifica la ruta y nombre del archivo del modelo de reconocimiento de gestos\n",
    "\"\"\"\n",
    "MODEL_PATH = \"model_quantized_90_4.tflite\"\n",
    "NORMALIZATION_PARAMS_PATH = 'normalization_params_90_4.npz'\n",
    "dataset_dir = \"dataset_11_90\"  # Se define el directorio donde se guardarán las secuencias de datos (dataset)\n",
    "model_path = \"gesture_model_me_12_90_4.h5\"  # Se especifica la ruta y nombre del archivo del modelo de reconocimiento de gestos\n",
    "\n",
    "\n",
    "\n",
    "sequence_length = 90  # Se define la longitud de cada secuencia (número de frames por secuencia) para el análisis de gestos\n",
    "total_landmarks = 126  # Se establece el número total de landmarks (puntos de referencia) que se esperan por secuencia (por ejemplo, 21 puntos por mano * 3 coordenadas x2 manos = 126)\n",
    "gestures = []  # Se inicializa una lista vacía que posteriormente contendrá los nombres de los gestos disponibles\n",
    "X_mean = None  # Se inicializa la variable para la media de los datos, la cual se usará para normalizar las secuencias\n",
    "X_std = None  # Se inicializa la variable para la desviación estándar de los datos, para normalización de las secuencias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FUNCIONES PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def init_system():  # Función que inicializa el sistema preparando el entorno de trabajo\n",
    "    global gestures  # Declara que se usará la variable global 'gestures' para actualizar la lista de gestos disponibles\n",
    "    os.makedirs(dataset_dir, exist_ok=True)  # Crea el directorio definido en 'dataset_dir' si no existe (exist_ok=True evita errores si ya existe)\n",
    "    gestures = get_existing_gestures()  # Asigna a 'gestures' la lista de gestos existentes obtenida de la función get_existing_gestures()\n",
    "\n",
    "def get_existing_gestures():  # Función que retorna una lista ordenada de gestos existentes en el directorio del dataset\n",
    "    return sorted([  # Devuelve la lista ordenada alfabéticamente\n",
    "        d for d in os.listdir(dataset_dir)  # Itera sobre cada elemento 'd' en el listado de archivos y carpetas del directorio 'dataset_dir'\n",
    "        if os.path.isdir(os.path.join(dataset_dir, d))  # Incluye 'd' en la lista solo si es un directorio (lo que indica un gesto)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DETECCION DE MANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hands():  # Define la función detect_hands para iniciar la detección de manos en tiempo real\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")  # Imprime un mensaje informativo al usuario sobre el inicio del proceso y cómo salir\n",
    "    cap = UDPCamera()  # Crea una instancia de UDPCamera para capturar frames de video vía UDP\n",
    "\n",
    "    while True:  # Inicia un bucle infinito para procesar continuamente los frames capturados\n",
    "        ret, frame = cap.read()  # Llama al método read() para obtener el frame actual; ret indica éxito y frame contiene la imagen\n",
    "        if not ret:  # Si no se pudo leer un frame correctamente\n",
    "            continue  # Omite la iteración actual y vuelve a intentar leer un nuevo frame\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convierte el frame de BGR (formato de OpenCV) a RGB (requerido por MediaPipe)\n",
    "        results = hands.process(rgb_frame)  # Procesa el frame convertido con MediaPipe para detectar landmarks de manos\n",
    "\n",
    "        if results.multi_hand_landmarks:  # Si se han detectado landmarks en el frame\n",
    "            for hand_landmarks in results.multi_hand_landmarks:  # Itera por cada conjunto de landmarks detectados (una por mano)\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)  # Dibuja los landmarks y las conexiones en el frame usando las utilidades de MediaPipe\n",
    "\n",
    "        cv2.imshow(\"Detección de Manos\", frame)  # Muestra el frame procesado en una ventana titulada \"Detección de Manos\"\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # Espera 1 milisegundo por una tecla; si se detecta la tecla ESC (código 27), se sale del bucle\n",
    "            break  # Rompe el bucle infinito para detener la detección de manos\n",
    "\n",
    "    cap.release()  # Libera los recursos de la cámara UDP al finalizar la captura\n",
    "    cv2.destroyAllWindows()  # Cierra todas las ventanas abiertas por OpenCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    global gestures  # Accede a la variable global 'gestures' para actualizarla posteriormente\n",
    "    gesture = input(\"\\nIngrese la palabra o letra para la cual desea recolectar datos: \").upper()  # Solicita al usuario el nombre del gesto y lo convierte a mayúsculas\n",
    "    num_sequences = int(input(\"Ingrese el número de secuencias a capturar (recomendado: 50): \"))  # Define cuántas secuencias de video se capturarán\n",
    "    \n",
    "    save_dir = os.path.join(dataset_dir, gesture)  # Crea la ruta del directorio donde se guardarán los datos\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Crea el directorio si no existe, evitando errores si ya está presente\n",
    "\n",
    "    print(f\"\\nRecolectando datos para el gesto '{gesture}'. Presiona 'ESC' para cancelar.\")  # Mensaje informativo\n",
    "    print(\"Mantenga la seña frente a la cámara...\")  # Instrucción para el usuario\n",
    "    \n",
    "    cap = UDPCamera()  # Inicializa la cámara UDP personalizada\n",
    "    sequence = []  # Lista para almacenar temporalmente los datos de landmarks de cada secuencia\n",
    "    counter = 0  # Contador de secuencias capturadas\n",
    "\n",
    "    # Configurar ventana de landmarks\n",
    "    landmark_window_name = \"Landmarks en Tiempo Real\"  # Nombre de la ventana auxiliar\n",
    "    cv2.namedWindow(landmark_window_name, cv2.WINDOW_NORMAL)  # Crea una ventana redimensionable\n",
    "    cv2.resizeWindow(landmark_window_name, 640, 480)  # Establece el tamaño de la ventana\n",
    "\n",
    "\n",
    "    while True:  # Bucle principal de captura\n",
    "        ret, frame = cap.read()  # Lee un frame de la cámara\n",
    "        if not ret:  # Si falla la lectura, sale del bucle\n",
    "            break\n",
    "\n",
    "        # Crear canvas para landmarks\n",
    "        landmark_canvas = np.zeros((480, 640, 3), dtype=np.uint8)  # Crea una imagen negra de 640x480 píxeles\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convierte el frame de BGR a RGB para MediaPipe\n",
    "        results = hands.process(rgb_frame)  # Procesa el frame para detectar landmarks de manos\n",
    "\n",
    "        if results.multi_hand_landmarks:  # Si se detectan manos:\n",
    "            all_landmarks = []  # Lista para almacenar coordenadas de landmarks\n",
    "\n",
    "            # Dibujar landmarks en el canvas\n",
    "            for hand_landmarks in results.multi_hand_landmarks:  # Itera sobre cada mano detectada\n",
    "                # Dibuja landmarks y conexiones en el canvas negro\n",
    "                mp_draw.draw_landmarks(\n",
    "                    landmark_canvas,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,  # Especifica las conexiones anatómicas\n",
    "                    mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),  # Configuración visual de puntos\n",
    "                    mp_draw.DrawingSpec(color=(0, 0, 255), thickness=2)  # Configuración visual de líneas\n",
    "                )\n",
    "            \n",
    "            # Extraer coordenadas para el dataset\n",
    "            for hand in results.multi_hand_landmarks[:2]:  # Considera máximo 2 manos\n",
    "                for lm in hand.landmark:  # Itera sobre cada landmark de la mano\n",
    "                    all_landmarks.extend([lm.x, lm.y, lm.z])  # Almacena coordenadas normalizadas (x,y,z)\n",
    "            \n",
    "            # Rellenar si solo hay una mano\n",
    "            if len(results.multi_hand_landmarks) < 2:  # Si hay menos de 2 manos detectadas\n",
    "                all_landmarks += [0.0] * 63  # Añade 63 ceros (21 landmarks * 3 coordenadas por mano faltante)\n",
    "            \n",
    "            sequence.append(all_landmarks)\n",
    "\n",
    "            # Dibujar en el frame original\n",
    "            for hand_landmarks in results.multi_hand_landmarks:  # Dibuja landmarks en el frame de video original\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if len(sequence) == sequence_length:  # Cuando se completa una secuencia\n",
    "            np.save(os.path.join(save_dir, f\"secuencia_{counter}.npy\"), sequence)  # Guarda la secuencia en un archivo .npy\n",
    "            counter += 1  # Incrementa el contador de secuencias\n",
    "            sequence = []  # Reinicia la lista para la próxima secuencia\n",
    "            print(f\"Secuencias capturadas: {counter}/{num_sequences}\")  # Muestra progreso\n",
    "\n",
    "\n",
    "        # Mostrar información en ambas ventanas\n",
    "        info_text = f\"Secuencias: {counter}/{num_sequences}\"  # Texto con el progreso actual\n",
    "        cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)  # Superpone texto en el frame\n",
    "        cv2.putText(landmark_canvas, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)  # Texto en el canvas\n",
    "\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)  # Muestra el frame original con landmarks\n",
    "        cv2.imshow(landmark_window_name, landmark_canvas)  # Muestra el canvas con solo los landmarks\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or counter >= num_sequences:  # Termina con ESC o al completar las secuencias\n",
    "            break\n",
    "\n",
    "    cap.release()  # Libera los recursos de la cámara\n",
    "    cv2.destroyAllWindows()  # Cierra todas las ventanas de OpenCV\n",
    "    gestures = get_existing_gestures()  # Actualiza la lista global de gestos\n",
    "    print(f\"\\nSe recolectaron {counter} secuencias para el gesto '{gesture}'\")  # Resumen final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_3d_rotation(angles):\n",
    "    \"\"\"Rotación 3D vectorizada para landmarks\"\"\"\n",
    "    # Matriz de rotación alrededor del eje X (ángulo en radianes)\n",
    "    rx = tf.convert_to_tensor([\n",
    "        [1, 0, 0],  # Componente X sin rotar\n",
    "        [0, tf.cos(angles[0]), -tf.sin(angles[0])],  # Componentes Y y Z afectadas por el ángulo\n",
    "        [0, tf.sin(angles[0]), tf.cos(angles[0])]  # Componentes Y y Z rotadas\n",
    "    ])\n",
    "    \n",
    "    # Matriz de rotación alrededor del eje Y\n",
    "    ry = tf.convert_to_tensor([\n",
    "        [tf.cos(angles[1]), 0, tf.sin(angles[1])],  # Componentes X y Z afectadas\n",
    "        [0, 1, 0],  # Componente Y sin rotar\n",
    "        [-tf.sin(angles[1]), 0, tf.cos(angles[1])]  # Componentes X y Z rotadas\n",
    "    ])\n",
    "    \n",
    "    # Matriz de rotación alrededor del eje Z\n",
    "    rz = tf.convert_to_tensor([\n",
    "        [tf.cos(angles[2]), -tf.sin(angles[2]), 0],  # Componentes X y Y afectadas\n",
    "        [tf.sin(angles[2]), tf.cos(angles[2]), 0],  # Componentes X y Y rotadas\n",
    "        [0, 0, 1]  # Componente Z sin rotar\n",
    "    ])\n",
    "    \n",
    "    # Combinación de rotaciones: Z -> Y -> X (multiplicación matricial en orden inverso)\n",
    "    return tf.matmul(rz, tf.matmul(ry, rx))  # rz * ry * rx = rotación compuesta\n",
    "\n",
    "def augment_landmarks(landmarks):\n",
    "    \"\"\"Aumentación 3D mejorada\"\"\"\n",
    "    # Generar ángulos aleatorios en grados (-15° a 15°) y convertir a radianes\n",
    "    angles = tf.random.uniform([3], -15.0, 15.0) * (np.pi / 180.0)\n",
    "    \n",
    "    # Obtener matriz de rotación compuesta\n",
    "    rot_matrix = tf_3d_rotation(angles)\n",
    "    \n",
    "    # Aplicar rotación a los landmarks\n",
    "    original_shape = tf.shape(landmarks)  # Guardar forma original (ej: [secuencia, landmarks, coordenadas])\n",
    "    landmarks = tf.reshape(landmarks, [-1, 3])  # Aplanar a matriz 2D: (N*M, 3) donde N=secuencias, M=landmarks\n",
    "    landmarks = tf.matmul(landmarks, rot_matrix)  # Rotar cada punto 3D\n",
    "    landmarks = tf.reshape(landmarks, original_shape)  # Restaurar forma original\n",
    "    \n",
    "    # Añadir ruido gaussiano para mayor variabilidad\n",
    "    noise = tf.random.normal(tf.shape(landmarks), mean=0.0, stddev=0.08)  # Ruido con desviación estándar 0.08\n",
    "    return landmarks + noise  # Landmarks rotados + ruido (aumentación combinada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación clave:**\n",
    "1. `tf_3d_rotation`: Crea matrices de rotación 3D para cada eje y las combina para generar una transformación espacial compuesta.\n",
    "2. `augment_landmarks`: Aplica transformaciones aleatorias a los landmarks para:\n",
    "   - Mejorar la generalización del modelo\n",
    "   - Simular variaciones en la captura de movimientos\n",
    "   - Prevenir sobreajuste mediante aumento de datos\n",
    "3. **Flujo completo:** Rotación 3D + ruido = transformaciones realistas que mantienen la estructura anatómica pero introducen variabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_augmentation(sequence):\n",
    "    \"\"\"Aumentación 100% en TensorFlow con rotaciones 3D\"\"\"\n",
    "    sequence = tf.cast(sequence, tf.float32)  # Conversión a float32 para operaciones de TensorFlow\n",
    "    \n",
    "    # 1. Aplicar rotación 3D y ruido base (definido en augment_landmarks)\n",
    "    sequence = augment_landmarks(sequence)  # Aplica rotación 3D y ruido inicial\n",
    "    \n",
    "    # 2. Ruido Gaussiano adicional para mayor variabilidad\n",
    "    noise = tf.random.normal(tf.shape(sequence), mean=0.0, stddev=0.05)  # Genera ruido con desviación estándar 5%\n",
    "    sequence = tf.add(sequence, noise)  # Suma el ruido a la secuencia\n",
    "    \n",
    "    # 3. Escalado aleatorio para simular variaciones de tamaño\n",
    "    scale_factor = tf.random.uniform([], 0.9, 1.1)  # Factor de escala entre 90% y 110%\n",
    "    sequence = tf.multiply(sequence, scale_factor)  # Aplica escalado a todos los puntos\n",
    "    \n",
    "    return sequence  # Devuelve secuencia aumentada\n",
    "\n",
    "def create_dataset(X_data, y_data, augment=False):\n",
    "    \"\"\"Crea pipeline de datos optimizado para entrenamiento\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))  # Crea dataset básico\n",
    "    \n",
    "    if augment:  # Solo aplica aumentación si está activado\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (custom_augmentation(x), y),  # Aplica aumentación a features manteniendo labels\n",
    "            num_parallel_calls=tf.data.AUTOTUNE  # Paralelismo automático\n",
    "        )\n",
    "        dataset = dataset.shuffle(1000)  # Mezcla datos con buffer de 1000 elementos\n",
    "    \n",
    "    # Configuración final del pipeline\n",
    "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)    # Lotes de 32 muestras # Precarga datos para maximizar throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación detallada:**\n",
    "\n",
    "1. **`custom_augmentation`** (Aumentación de datos):\n",
    "   - *Propósito:* Mejorar la generalización del modelo mediante transformaciones espaciales y numéricas.\n",
    "   - *Flujo:*\n",
    "     1. **Rotación 3D + Ruido base:** Aplica transformaciones espaciales realistas usando la función `augment_landmarks`.\n",
    "     2. **Ruido adicional:** Añade variabilidad numérica extra (5% de desviación estándar).\n",
    "     3. **Escalado:** Simula cambios de tamaño (+/-10%) manteniendo proporciones anatómicas.\n",
    "\n",
    "2. **`create_dataset`** (Pipeline de datos):\n",
    "   - *Optimizaciones clave:*\n",
    "     - **Paralelismo inteligente:** `num_parallel_calls=tf.data.AUTOTUNE` permite que TensorFlow optimice automáticamente el uso de CPU.\n",
    "     - **Prefetching:** `prefetch` solapa la preparación de datos y el entrenamiento para evitar cuellos de botella.\n",
    "     - **Mezcla estratégica:** `shuffle(1000)` mantiene diversidad en lotes sin consumo excesivo de memoria.\n",
    "\n",
    "3. **Efecto combinado:**\n",
    "   - Genera **variaciones sintéticas** de los gestos originales.\n",
    "   - **Triplica la diversidad efectiva** del dataset sin recolectar nuevos datos.\n",
    "   - **Mejora resistencia** a:\n",
    "     - Cambios de perspectiva\n",
    "     - Tamaños de mano variables\n",
    "     - Ruido en detección de landmarks\n",
    "\n",
    "**Ejemplo de transformación:**\n",
    "Un gesto original de \"A\" se convierte en:\n",
    "- Versión rotada 12° a la derecha\n",
    "- Con landmarks ligeramente desplazados (ruido)\n",
    "- Escalado al 95% de tamaño original\n",
    "→ Todo manteniendo la etiqueta \"A\" para aprendizaje supervisado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(augment=True):\n",
    "    X = []  # Lista para almacenar secuencias de landmarks\n",
    "    y = []  # Lista para almacenar etiquetas numéricas\n",
    "    \n",
    "    for label_idx, gesture in enumerate(gestures):  # Iterar sobre cada gesto con su índice\n",
    "        gesture_dir = os.path.join(dataset_dir, gesture)  # Ruta completa al directorio del gesto\n",
    "        sequences = [f for f in os.listdir(gesture_dir) if f.endswith('.npy')]  # Listar archivos .npy\n",
    "        \n",
    "        print(f\"Gesto '{gesture}' - secuencias encontradas: {len(sequences)}\")  # Debug: mostrar cantidad\n",
    "        \n",
    "        for seq_file in sequences:  # Procesar cada secuencia\n",
    "            seq_path = os.path.join(gesture_dir, seq_file)  # Ruta completa al archivo\n",
    "            sequence = np.load(seq_path)  # Cargar secuencia desde disco\n",
    "            \n",
    "            if sequence.shape == (sequence_length, total_landmarks):  # Verificar dimensiones correctas\n",
    "                X.append(sequence)  # Agregar secuencia válida\n",
    "                y.append(label_idx)  # Agregar etiqueta correspondiente\n",
    "            else:\n",
    "                print(f\"Secuencia {seq_file} con forma {sequence.shape} ignorada.\")  # Advertencia formato incorrecto\n",
    "    \n",
    "    return np.array(X, dtype=np.float32), np.array(y), gestures  # Devolver datos como arrays numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación línea por línea:**\n",
    "\n",
    "1. **`def load_data(augment=True):`**  \n",
    "   - *Función principal* para cargar datos desde disco  \n",
    "   - `augment`: Parámetro no utilizado (posible herencia de versión anterior)\n",
    "\n",
    "2. **`X = []`, `y = []`**  \n",
    "   - Inicializa listas vacías:  \n",
    "     - `X`: Almacenará secuencias de landmarks (features)  \n",
    "     - `y`: Almacenará índices numéricos de los gestos (labels)\n",
    "\n",
    "3. **`for label_idx, gesture in enumerate(gestures):`**  \n",
    "   - Itera sobre cada gesto disponible  \n",
    "   - `label_idx`: Índice numérico del gesto (0 para \"A\", 1 para \"B\", etc.)  \n",
    "   - `gesture`: Nombre del gesto (ej: \"A\")\n",
    "\n",
    "4. **`gesture_dir = os.path.join(...)`**  \n",
    "   - Construye la ruta al directorio del gesto (ej: \"dataset/A\")\n",
    "\n",
    "5. **`sequences = [f for f ...]`**  \n",
    "   - Lista todos los archivos .npy en el directorio  \n",
    "   - Cada archivo representa una secuencia guardada\n",
    "\n",
    "6. **`print(f\"Gesto '{gesture}'...`**  \n",
    "   - Muestra diagnóstico de cuántas secuencias se encontraron  \n",
    "   - Útil para detectar gestos con pocos datos\n",
    "\n",
    "7. **`for seq_file in sequences:`**  \n",
    "   - Procesa cada archivo de secuencia individualmente\n",
    "\n",
    "8. **`seq_path = os.path.join(...)`**  \n",
    "   - Obtiene la ruta completa al archivo .npy\n",
    "\n",
    "9. **`sequence = np.load(seq_path)`**  \n",
    "   - Carga la secuencia desde el archivo numpy  \n",
    "   - Formato esperado: (sequence_length, total_landmarks)\n",
    "\n",
    "10. **`if sequence.shape == (...)`**  \n",
    "    - Verificación crítica de dimensiones:  \n",
    "      - `sequence_length`: Número de frames por secuencia (ej: 90)  \n",
    "      - `total_landmarks`: Landmarks por frame (ej: 126 = 2 manos × 21 landmarks × 3 coordenadas)\n",
    "\n",
    "11. **`X.append(sequence)`**  \n",
    "    - Agrega secuencia válida al conjunto de features\n",
    "\n",
    "12. **`y.append(label_idx)`**  \n",
    "    - Agrega el índice numérico correspondiente al gesto\n",
    "\n",
    "13. **`else: print(...)`**  \n",
    "    - Manejo de errores: secuencias con formato incorrecto se ignoran  \n",
    "    - Previene problemas de dimensiones en el modelo\n",
    "\n",
    "14. **`return np.array(...), ...`**  \n",
    "    - Retorna:  \n",
    "      - `X`: Array de secuencias (float32 para eficiencia en GPUs)  \n",
    "      - `y`: Array de etiquetas  \n",
    "      - `gestures`: Lista original de nombres de gestos\n",
    "\n",
    "**Flujo de trabajo:**  \n",
    "1. Recorre estructura de directorios del dataset  \n",
    "2. Carga y valida cada secuencia guardada  \n",
    "3. Construye arrays estructurados para entrenamiento  \n",
    "4. Asegura consistencia en tipos de datos y dimensiones\n",
    "\n",
    "**Ejemplo de salida:**  \n",
    "Para un dataset con 3 gestos (\"A\", \"B\", \"C\") y 50 secuencias cada uno:  \n",
    "```python\n",
    "X.shape → (150, 90, 126)  # 150 muestras, 90 frames, 126 landmarks\n",
    "y.shape → (150,)           # 150 etiquetas (0,1,2)\n",
    "gestures → [\"A\", \"B\", \"C\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representative_dataset_gen():  \n",
    "    # Generador de datos de ejemplo para calibración de cuantización\n",
    "    for _ in range(100):  # Generar 100 muestras de calibración\n",
    "        yield [  # Devolver lote de entrada en formato esperado por el modelo\n",
    "            np.random.rand(  # Datos sintéticos con distribución uniforme [0, 1)\n",
    "                1,  # Tamaño de batch (1 muestra por lote)\n",
    "                sequence_length,  # Número de frames por secuencia (ej: 90)\n",
    "                total_landmarks  # Total de landmarks 3D (ej: 126 = 21 landmarks/mano * 2 manos * 3 coordenadas)\n",
    "            ).astype(np.float32)  # Tipo requerido para la conversión TFLite\n",
    "        ]\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    global X_mean, X_std, gestures  # Accede a variables globales para normalización y gestos\n",
    "    global MODEL_PATH, NORMALIZATION_PARAMS_PATH  \n",
    "\n",
    "    # 1. Verificación inicial de datos\n",
    "    gestures = get_existing_gestures()  # Obtiene lista de gestos desde el directorio\n",
    "    if not gestures:  # Si no hay gestos detectados\n",
    "        print(\"\\nNo hay datos recolectados. Primero recolecte datos de gestos.\")\n",
    "        return  # Detiene la ejecución\n",
    "\n",
    "    # 2. Carga y preparación de datos\n",
    "    print(\"\\nCargando datos y preparando el entrenamiento...\")\n",
    "    X, y, gestures = load_data(augment=False)  # Carga datos sin aumentación inicial\n",
    "    y = tf.keras.utils.to_categorical(y)  # Convierte etiquetas a one-hot encoding\n",
    "    \n",
    "    # 3. División de datos en entrenamiento/validación\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "        test_size=0.2,  # 20% para validación\n",
    "        stratify=y,  # Mantiene distribución de clases\n",
    "        random_state=42  # Semilla para reproducibilidad\n",
    "    )\n",
    "\n",
    "    # 4. Normalización de datos\n",
    "    X_mean = np.mean(X_train, axis=(0, 1)).astype(np.float32)  # Media de entrenamiento\n",
    "    X_std = np.std(X_train, axis=(0, 1)).astype(np.float32)  # Desviación estándar\n",
    "    X_train = (X_train - X_mean) / X_std  # Normaliza datos de entrenamiento\n",
    "    X_val = (X_val - X_mean) / X_std  # Aplica misma normalización a validación\n",
    "\n",
    "    # Creación de datasets con pipeline optimizado\n",
    "    train_dataset = create_dataset(X_train, y_train, augment=True)  # Dataset con aumentación\n",
    "    val_dataset = create_dataset(X_val, y_val, augment=False)  # Validación sin aumentación\n",
    "    \n",
    "\n",
    "    # 5. Guardado de parámetros de normalización\n",
    "    np.savez(NORMALIZATION_PARAMS_PATH, mean=X_mean, std=X_std)  # Guarda en archivo .npz\n",
    "    \n",
    "    # 6. Arquitectura del modelo híbrido (Conv + Attention)\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))  # Capa de entrada\n",
    "    \n",
    "    # Mecanismo de atención espacial\n",
    "    attention = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=4,  # 4 cabezas de atención\n",
    "        key_dim=64  # Dimensión de claves/valores\n",
    "    )(inputs, inputs)  # Auto-atención\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([inputs, attention])  # Conexión residual\n",
    "    \n",
    "    # Bloque convolucional 1\n",
    "    x = tf.keras.layers.Conv1D(\n",
    "        128, 5,  # 128 filtros, kernel de tamaño 5\n",
    "        activation='relu', #funcion de activacion, recta lineal\n",
    "        padding='same'  # Mantiene dimensiones\n",
    "    )(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)  # Reducción temporal\n",
    "\n",
    "    # Bloque convolucional 2\n",
    "    x = tf.keras.layers.Conv1D(\n",
    "        64, 3,  # 64 filtros, kernel de tamaño 3\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )(x)\n",
    "\n",
    "    # Atención temporal\n",
    "    x = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=2,  # 2 cabezas para temporal\n",
    "        key_dim=32  # Dimensión reducida\n",
    "    )(x, x)  # Atención sobre secuencia procesada\n",
    "\n",
    "    # Reducción dimensional final\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)  # Pooling temporal\n",
    "\n",
    "    # Capas densas para clasificación\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)  # Capa oculta\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        len(gestures),  # Neuronas de salida = número de gestos\n",
    "        activation='softmax'  # Clasificación multiclase\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)  # Construcción del modelo completo\n",
    "\n",
    "   # 7. Compilación del modelo\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.0001,  # Tasa de aprendizaje baja\n",
    "            global_clipnorm=1.0  # Evita gradientes explosivos\n",
    "        ),\n",
    "        loss='categorical_crossentropy',  # Función de pérdida para clasificación\n",
    "        metrics=['accuracy'],  # Métrica principal\n",
    "        weighted_metrics=['accuracy']  # Métricas adicionales\n",
    "    )\n",
    "    \n",
    "    model.summary()  # Muestra resumen de arquitectura\n",
    "\n",
    "    # 8. Entrenamiento del modelo\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,  # Datos de validación\n",
    "        epochs=50,  # 50 pasadas completas\n",
    "        verbose=1  # Muestra progreso\n",
    "    )\n",
    "\n",
    "    # 9. Guardado del modelo entrenado\n",
    "    model.save(model_path)  # Guarda en formato Keras\n",
    "    print(f\"\\nModelo guardado en {model_path}\")\n",
    "\n",
    "    # 10. Conversión a TFLite para despliegue\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Cuantización básica\n",
    "    converter.representative_dataset = representative_dataset_gen  # Calibración\n",
    "    converter.target_spec.supported_ops = [  # Operaciones soportadas\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,  # Ops nativas\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS  # Ops especiales de TF\n",
    "    ]\n",
    "    converter.target_spec.supported_types = [tf.int8]  # Cuantización a 8 bits\n",
    "    converter._experimental_default_to_single_batch_in_tensor_list_ops = True  # Optimización\n",
    "\n",
    "    try:  # Manejo de errores en conversión\n",
    "        tflite_model = converter.convert()\n",
    "        with open(MODEL_PATH, 'wb') as f:\n",
    "            f.write(tflite_model)  # Escribe modelo cuantizado\n",
    "        print(\"\\nModelo TFLite exportado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError en conversión TFLite: {str(e)}\")\n",
    "\n",
    "    # 11. Métricas finales\n",
    "    val_accuracy = history.history['val_accuracy'][-1]  # Última precisión de validación\n",
    "    print(f\"Precisión de validación final: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef representative_dataset_gen():  # Define una función generadora para proporcionar ejemplos al convertir el modelo\\n    # Generador de datos de ejemplo para calibración\\n    for _ in range(100):  # Itera 100 veces para generar ejemplos de entrada\\n        yield [np.random.randn(1, sequence_length, total_landmarks).astype(np.float32)]  \\n        # Genera un array de números aleatorios con forma (1, sequence_length, total_landmarks)\\n        # Lo convierte a tipo de datos float32, requerido por TensorFlow Lite'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_tflite():  # Define la función para convertir el modelo entrenado a formato TFLite\n",
    "    global MODEL_PATH, NORMALIZATION_PARAMS_PATH\n",
    "    try:  # Inicia un bloque try para capturar errores en la conversión\n",
    "        # Cargar el modelo entrenado\n",
    "        model = tf.keras.models.load_model(model_path)  # Carga el modelo entrenado desde la ruta especificada en 'model_path'\n",
    "        \n",
    "        # Configurar el conversor con parámetros especiales\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)  # Crea un conversor TFLite a partir del modelo cargado\n",
    "        \n",
    "        # Añadir estas 3 líneas clave para compatibilidad con LSTM\n",
    "        converter.target_spec.supported_ops = [  # Especifica las operaciones que deben ser compatibles en el modelo convertido\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,  # Usa las operaciones estándar de TensorFlow Lite\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS  # Permite operaciones adicionales de TensorFlow que no están incluidas por defecto en TFLite\n",
    "        ]\n",
    "        converter._experimental_lower_tensor_list_ops = False  # Desactiva la optimización de listas de tensores, útil para modelos LSTM\n",
    "        converter.allow_custom_ops = True  # Habilita operaciones personalizadas que puedan estar presentes en el modelo\n",
    "        \n",
    "        # Realizar la conversión\n",
    "        tflite_model = converter.convert()  # Convierte el modelo de Keras a formato TFLite\n",
    "        \n",
    "        # Guardar el modelo cuantizado\n",
    "        with open(MODEL_PATH, 'wb') as f:  # Abre un archivo en modo escritura binaria\n",
    "            f.write(tflite_model)  # Guarda el modelo convertido en el archivo\n",
    "        \n",
    "        print(\"\\n✅ Conversión a TFLite exitosa!\")  # Mensaje de éxito en la conversión\n",
    "        \n",
    "    except Exception as e:  # Captura cualquier error que ocurra durante la conversión\n",
    "        print(f\"\\n❌ Error en conversión: {str(e)}\")  # Imprime el mensaje de error\n",
    "        print(\"Posibles soluciones:\")  # Lista de sugerencias para solucionar errores comunes\n",
    "        print(\"1. Verifique que el modelo .h5 existe\")  # Sugerencia 1: Comprobar que el archivo del modelo existe\n",
    "        print(\"2. Actualice TensorFlow: pip install --upgrade tensorflow\")  # Sugerencia 2: Actualizar TensorFlow\n",
    "        print(\"3. Reinicie el runtime/kernel\")  # Sugerencia 3: Reiniciar el entorno de ejecución (útil en Google Colab o Jupyter Notebook)\n",
    "\n",
    "    global gestures  # Se usa la variable global 'gestures'\n",
    "    gestures = get_existing_gestures()  # Se actualiza la lista de gestos disponibles después de la conversión\n",
    "    print(\"Gestos cargados para evaluación:\", gestures)  # Muestra los gestos detectados en el dataset\n",
    "\n",
    "    print(\"Salida del modelo:\", model.output_shape)  # Imprime la forma de salida del modelo, útil para depuración\n",
    "\n",
    "\"\"\"\n",
    "def representative_dataset_gen():  # Define una función generadora para proporcionar ejemplos al convertir el modelo\n",
    "    # Generador de datos de ejemplo para calibración\n",
    "    for _ in range(100):  # Itera 100 veces para generar ejemplos de entrada\n",
    "        yield [np.random.randn(1, sequence_length, total_landmarks).astype(np.float32)]  \n",
    "        # Genera un array de números aleatorios con forma (1, sequence_length, total_landmarks)\n",
    "        # Lo convierte a tipo de datos float32, requerido por TensorFlow Lite\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_sequence(seq_array):  # Define la función para extrapolar secuencias incompletas\n",
    "    \"\"\"Extrapolación lineal para secuencias incompletas\"\"\"  # Docstring que indica que esta función rellena secuencias incompletas con valores extrapolados\n",
    "    \n",
    "    last_valid = np.where(seq_array.sum(axis=1) != 0)[0]  # Encuentra los índices de las filas que contienen datos (es decir, que no son completamente ceros)\n",
    "    if len(last_valid) < 2:  # Si hay menos de 2 filas con datos, no se puede extrapolar una tendencia\n",
    "        return seq_array  # Retorna la secuencia original sin modificaciones\n",
    "    \n",
    "    x = np.array([0, 1])  # Define los valores de x para la extrapolación (dos puntos de referencia)\n",
    "    \n",
    "    for col in range(seq_array.shape[1]):  # Itera sobre cada columna (cada coordenada de los landmarks)\n",
    "        y = seq_array[last_valid[-2:], col]  # Toma los dos últimos valores válidos en la columna actual\n",
    "        coeffs = np.polyfit(x, y, 1)  # Calcula los coeficientes de una regresión lineal de primer grado (pendiente e intercepto)\n",
    "        seq_array[last_valid[-1]+1:, col] = np.polyval(coeffs, np.arange(2, 2+len(seq_array)-last_valid[-1]-1))  \n",
    "        # Aplica la ecuación de la recta para extrapolar valores desde el último punto válido en adelante\n",
    "    \n",
    "    return seq_array  # Retorna la secuencia con los valores extrapolados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explicación detallada:**\n",
    "1. **Identificación de datos válidos:**  \n",
    "   - Se usa `np.where(seq_array.sum(axis=1) != 0)[0]` para encontrar las filas que contienen valores distintos de cero.\n",
    "   - Si hay menos de 2 valores no nulos, se devuelve la secuencia sin cambios, ya que no es posible hacer una extrapolación.\n",
    "\n",
    "2. **Definición de puntos para regresión lineal:**  \n",
    "   - `x = np.array([0, 1])` define dos posiciones de referencia para la extrapolación.\n",
    "\n",
    "3. **Aplicación de regresión lineal por columna:**  \n",
    "   - Para cada columna (correspondiente a una coordenada de los landmarks), se toman los dos últimos valores válidos.\n",
    "   - `np.polyfit(x, y, 1)` ajusta una línea recta a estos dos puntos, generando coeficientes para la ecuación de la recta `y = mx + b`.\n",
    "\n",
    "4. **Extrapolación de valores faltantes:**  \n",
    "   - Se usa `np.polyval(coeffs, np.arange(2, 2+len(seq_array)-last_valid[-1]-1))` para calcular valores predichos basados en la ecuación de la recta y rellenar los datos faltantes.\n",
    "\n",
    "### **Ejemplo práctico:**\n",
    "Supongamos que tenemos una secuencia donde los últimos dos valores son conocidos:\n",
    "\n",
    "```python\n",
    "seq_array = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [0, 0],  # Falta un valor aquí\n",
    "    [0, 0]   # Falta otro valor aquí\n",
    "])\n",
    "```\n",
    "\n",
    "Después de ejecutar `extrapolate_sequence(seq_array)`, los valores extrapolados en la tercera y cuarta fila serán generados usando la tendencia de las dos primeras filas.\n",
    "\n",
    "Esta técnica es útil en procesamiento de señales cuando faltan datos en una secuencia y se quiere mantener una continuidad basada en los valores previos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(): #encargada de realizar inferencias con el modelo en tiempo real.\n",
    "    \"\"\" \n",
    "    Función para evaluar el modelo de reconocimiento de gestos en tiempo real.\n",
    "    Procesa video de la cámara, detecta manos, y predice gestos basados en secuencias\n",
    "    de landmarks de MediaPipe.\n",
    "    \"\"\"\n",
    "    global gestures #variable global para actualizar la lista de gestos detectados en el dataset.\n",
    "    gestures = get_existing_gestures() #se llama a la funcion, carga la lista de gestos disponibles en el dataset.\n",
    "\n",
    "    # Constantes para mantener consistencia\n",
    "    # Se definen constantes con las rutas del modelo TFLite y los parámetros de normalización.\n",
    "    global MODEL_PATH \n",
    "    global NORMALIZATION_PARAMS_PATH  \n",
    "    \n",
    "    # 1. Verificar si existe el modelo\n",
    "    #Se verifica si el modelo TFLite existe; si no, muestra un mensaje y finaliza la función\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"\\n¡Primero debe entrenar y convertir el modelo!\")\n",
    "        return\n",
    "    \n",
    "    # 2. Cargar parámetros y modelo\n",
    "    # Se intenta cargar los parámetros de normalización desde un archivo .npz.\n",
    "    try:\n",
    "        with np.load(NORMALIZATION_PARAMS_PATH) as data:\n",
    "            X_mean = data['mean']\n",
    "            X_std = data['std']\n",
    "            \n",
    "        # Cargar el modelo TFLite\n",
    "        interpreter = tf.lite.Interpreter(model_path=MODEL_PATH) #Se carga el modelo TFLite en un tf.lite.Interpreter\n",
    "        interpreter.allocate_tensors() #Se asigna memoria para los tensores del modelo.\n",
    "\n",
    "        #Se obtienen detalles de entrada y salida del modelo.\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "        # Se imprime la forma del tensor de salida para verificar compatibilidad.\n",
    "        print(\"Output details shape:\", output_details['shape'])\n",
    "    # Si hay un error al cargar el modelo o los parámetros, se muestra un mensaje y se finaliza la función.\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError crítico al cargar modelo: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 3. Configuración de cámara\n",
    "    cap = UDPCamera()#Se inicializa una instancia de UDPCamera para recibir video.\n",
    "    \n",
    "    # 4. Variables de estado\n",
    "    # Se configuran variables para manejar el buffer de datos (CircularBuffer) \n",
    "    # y almacenar información de gestos, confianza y latencia.\n",
    "    buffer = CircularBuffer(sequence_length)  # Usar solo buffer para secuencias\n",
    "    #buffer = CircularBuffer(sequence_length,total_landmarks)  # la continuacion de la expliacion de class circulabuffer (probar)\n",
    "    prediction_history = deque(maxlen=15)     #el suavizado de predicciones (deque), Para suavizado de predicciones\n",
    "    current_gesture = \"Esperando...\"\n",
    "    current_confidence = 0.0\n",
    "    latency = 0.0\n",
    "    fps_counter = deque(maxlen=30)\n",
    "    last_time = time.time()\n",
    "    high_sensitivity_mode = False\n",
    "    unknown_counter = 0\n",
    "\n",
    "    # 5. Configurar afinidad de CPU para mejor rendimiento\n",
    "    # Se intenta fijar la afinidad del proceso a los primeros dos núcleos de la CPU para mejorar el rendimiento.\n",
    "    try:\n",
    "        p = psutil.Process()\n",
    "        p.cpu_affinity([0, 1])  # Hilo principal en núcleos 0-1\n",
    "    except Exception as e:\n",
    "        print(f\"Advertencia: No se pudo configurar afinidad de CPU: {str(e)}\")\n",
    "\n",
    "    # 6. Bucle principal optimizado\n",
    "    while True:\n",
    "        # Medición de FPS/Se mide el FPS usando la diferencia de tiempo entre frames.\n",
    "        current_time = time.time()\n",
    "        fps_counter.append(1/(current_time - last_time + 1e-7))\n",
    "        last_time = current_time\n",
    "\n",
    "        # Se captura un frame de la cámara; si falla, se omite la iteración.\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Procesar landmarks de manos\n",
    "        # Se convierte el frame de BGR a RGB y se procesan landmarks con MediaPipe.\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        # Inicializar landmarks con ceros (para casos sin detección)\n",
    "        # Se extraen landmarks de la detección y se rellenan con ceros si hay menos de los esperados.\n",
    "        landmarks = [0.0] * total_landmarks\n",
    "        \n",
    "        # Extraer landmarks si hay manos detectadas\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:  # Máximo 2 manos\n",
    "                for lm in hand.landmark:\n",
    "                    hand_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar con ceros si es necesario\n",
    "            if len(hand_landmarks) < total_landmarks:\n",
    "                hand_landmarks += [0.0] * (total_landmarks - len(hand_landmarks))\n",
    "            else:\n",
    "                hand_landmarks = hand_landmarks[:total_landmarks]  # Truncar si excede\n",
    "                \n",
    "            landmarks = hand_landmarks\n",
    "\n",
    "        # Añadir al buffer circular /Se añade la secuencia al buffer circular.\n",
    "        buffer.add(landmarks)\n",
    "\n",
    "        #NUEVO EXPIREMNTAL, SE PUEDE ELIMINAR SI INTERFIERE\n",
    "        #aplico la extrapolacion, la funcion declarada anteriormente \n",
    "        if not buffer.full:\n",
    "            current_sequence = buffer.get_sequence()\n",
    "            extrapolated_sequence = extrapolate_sequence(current_sequence)\n",
    "            # Opcional: reemplazar el buffer con la secuencia extrapolada\n",
    "            buffer.buffer[:len(current_sequence)] = extrapolated_sequence\n",
    "            \n",
    "        # Ajustar modo alta sensibilidad si está activado\n",
    "        if high_sensitivity_mode:\n",
    "            frame = cv2.resize(frame, (320, 240))  # Reducir resolución para aumentar velocidad\n",
    "        \n",
    "        #Inferencia del modelo\n",
    "        # Realizar predicción cuando el buffer esté completo\n",
    "        # Se obtiene una secuencia completa del buffer, se normaliza y se pasa al modelo TFLite.\n",
    "        if buffer.full:\n",
    "            try:\n",
    "                # Obtener secuencia y preprocesar\n",
    "                seq_array = buffer.get_sequence()\n",
    "                seq_array = (seq_array - X_mean) / (X_std + 1e-7)\n",
    "                input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "                \n",
    "                # Verificar forma de los datos de entrada\n",
    "                if input_data.shape != tuple(input_details['shape']):\n",
    "                    print(f\"Error: Forma esperada {input_details['shape']}, obtenida {input_data.shape}\")\n",
    "                    continue\n",
    "                \n",
    "                # Inferencia con medición de latencia\n",
    "                start_time = time.perf_counter()\n",
    "                interpreter.set_tensor(input_details['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                prediction = interpreter.get_tensor(output_details['index'])[0]\n",
    "\n",
    "                latency = (time.perf_counter() - start_time) * 1000  # Se mide la latencia de la inferencia en ms\n",
    "                \"\"\"\n",
    "                # Sistema de fallback para gestos desconocidos\n",
    "                # Si la confianza es baja en varias iteraciones, cambia entre modos normal y de alta sensibilidad.\n",
    "                if np.max(prediction) < 0.5:\n",
    "                    unknown_counter += 1\n",
    "                    if unknown_counter >= 3:\n",
    "                        high_sensitivity_mode = not high_sensitivity_mode\n",
    "                        print(f\"Cambiando a modo: {'Alta Sensibilidad' if high_sensitivity_mode else 'Normal'}\")\n",
    "                        unknown_counter = 0\n",
    "                else:\n",
    "                    unknown_counter = 0\n",
    "                \"\"\"\n",
    "                # Suavizado temporal con promedio de predicciones recientes\n",
    "                # Se suaviza la predicción y se decide si la confianza es suficiente para mostrar el gesto detectado.\n",
    "                prediction_history.append(prediction)\n",
    "                smoothed_pred = np.mean(prediction_history, axis=0)\n",
    "                predicted_idx = np.argmax(smoothed_pred)\n",
    "                confidence = smoothed_pred[predicted_idx]\n",
    "                \n",
    "                # Umbral dinámico según modo de sensibilidad\n",
    "                confidence_threshold = 0.7 if not high_sensitivity_mode else 0.5\n",
    "                if confidence > confidence_threshold:\n",
    "                    current_gesture = gestures[predicted_idx]\n",
    "                    current_confidence = confidence\n",
    "                else:\n",
    "                    #current_gesture = \"Desconocido\"\n",
    "                    current_confidence = 0.0\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en predicción: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Visualización de métricas en pantalla\n",
    "        fps = np.mean(fps_counter) if fps_counter else 0\n",
    "        \n",
    "        #Cv2 configuracion:\n",
    "        \"\"\"cv2.putText(\n",
    "            frame,                             # 1. Imagen donde se dibujará (matriz NumPy)\n",
    "            f\"Prediccion: {current_gesture}\",  # 2. Texto a mostrar (cadena formateada)\n",
    "            (10, 30),                          # 3. Posición (x,y) desde esquina superior izquierda\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,          # 4. Tipo de fuente (estilo de letra)\n",
    "            0.9,                               # 5. Escala de fuente (tamaño relativo)\n",
    "            (0, 0, 255),                       # 6. Color en formato BGR (Azul, Verde, Rojo)\n",
    "            2                                  # 7. Grosor del texto en píxeles\n",
    "        )\"\"\"\n",
    "\n",
    "        \n",
    "        # Información de rendimiento\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 110), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Latencia: {latency:.1f}ms\", (10, 140), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "        \n",
    "        # Información de modo\n",
    "        mode_color = (0, 255, 0) if not high_sensitivity_mode else (0, 0, 255)\n",
    "        cv2.putText(frame, f\"Modo: {'Normal' if not high_sensitivity_mode else 'Alta Sensibilidad'}\", \n",
    "                   (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.6, mode_color, 2)\n",
    "        \n",
    "        # Información de predicción\n",
    "        cv2.putText(frame, f\"Prediccion: {current_gesture}\", (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2) #BGR\n",
    "        cv2.putText(frame, f\"Confianza: {current_confidence:.2%}\", (10, 70),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Visualización y salida\n",
    "        # Mostrar frame y detectar tecla ESC para salir\n",
    "        cv2.imshow(\"Predicciones en Tiempo Real\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # Liberar recursos\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `evaluate()` es responsable de ejecutar en tiempo real el reconocimiento de gestos a partir de secuencias de video recibidas por UDP. A lo largo de su ejecución, realiza múltiples tareas como cargar el modelo, procesar el video, extraer landmarks de las manos, realizar predicciones con el modelo de aprendizaje profundo y mostrar los resultados en pantalla.  \n",
    "\n",
    "A continuación, se analiza detalladamente cada bloque de la función, explicando su propósito y su funcionamiento.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Inicio de la función y carga del modelo**\n",
    "El primer paso de `evaluate()` es cargar el modelo TFLite y los parámetros de normalización necesarios para preprocesar los datos antes de la inferencia.  \n",
    "\n",
    "### **1.1 Carga de la lista de gestos**\n",
    "```python\n",
    "global gestures\n",
    "gestures = get_existing_gestures()\n",
    "```\n",
    "- `gestures` es una variable global que almacena los nombres de los gestos registrados en el dataset.  \n",
    "- `get_existing_gestures()` busca en el directorio de datos los gestos previamente recolectados y devuelve una lista ordenada con los nombres de las clases.  \n",
    "\n",
    "### **1.2 Definición de rutas del modelo y verificación de su existencia**\n",
    "```python\n",
    "MODEL_PATH = \"model_quantized_90_4.tflite\"\n",
    "NORMALIZATION_PARAMS_PATH = 'normalization_params_90_4.npz'\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"\\n¡Primero debe entrenar y convertir el modelo!\")\n",
    "    return\n",
    "```\n",
    "- `MODEL_PATH` almacena la ubicación del modelo TFLite previamente entrenado.  \n",
    "- `NORMALIZATION_PARAMS_PATH` almacena los parámetros de normalización del dataset.  \n",
    "- `os.path.exists(MODEL_PATH)` verifica si el modelo existe; si no, se muestra un mensaje de error y se detiene la función con `return`.  \n",
    "\n",
    "### **1.3 Carga de los parámetros de normalización y del modelo TFLite**\n",
    "```python\n",
    "try:\n",
    "    with np.load(NORMALIZATION_PARAMS_PATH) as data:\n",
    "        X_mean = data['mean']\n",
    "        X_std = data['std']\n",
    "```\n",
    "- `np.load(NORMALIZATION_PARAMS_PATH)` abre el archivo que contiene los valores de la media (`X_mean`) y la desviación estándar (`X_std`) de los datos de entrenamiento.  \n",
    "- Estos valores son necesarios para normalizar las nuevas entradas antes de pasarlas al modelo.  \n",
    "\n",
    "```python\n",
    "    interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    print(\"Output details shape:\", output_details['shape'])\n",
    "```\n",
    "- `tf.lite.Interpreter(model_path=MODEL_PATH)` carga el modelo TFLite en un intérprete de TensorFlow Lite.  \n",
    "- `allocate_tensors()` asigna memoria a los tensores del modelo.  \n",
    "- `get_input_details()` y `get_output_details()` extraen información sobre la estructura de los tensores de entrada y salida.  \n",
    "\n",
    "Si ocurre un error en la carga del modelo, se captura y se imprime el mensaje:  \n",
    "```python\n",
    "except Exception as e:\n",
    "    print(f\"\\nError crítico al cargar modelo: {str(e)}\")\n",
    "    return\n",
    "```\n",
    "Esto evita que el programa continúe si el modelo no pudo cargarse correctamente.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Configuración del sistema**\n",
    "Antes de iniciar la evaluación, se configuran los componentes necesarios para capturar video y realizar la inferencia.  \n",
    "\n",
    "### **2.1 Inicialización de la cámara UDP**\n",
    "```python\n",
    "cap = UDPCamera()\n",
    "```\n",
    "- Se crea una instancia de `UDPCamera()`, que es la clase encargada de recibir el video en tiempo real desde la red a través de paquetes UDP.  \n",
    "\n",
    "### **2.2 Variables de estado**\n",
    "```python\n",
    "buffer = CircularBuffer(sequence_length)  # Usar solo buffer para secuencias\n",
    "prediction_history = deque(maxlen=15)     # Para suavizado de predicciones\n",
    "current_gesture = \"Esperando...\"\n",
    "current_confidence = 0.0\n",
    "latency = 0.0\n",
    "fps_counter = deque(maxlen=30)\n",
    "last_time = time.time()\n",
    "high_sensitivity_mode = False\n",
    "unknown_counter = 0\n",
    "```\n",
    "- **`buffer = CircularBuffer(sequence_length)`**: Se inicializa un buffer circular para almacenar secuencias de landmarks antes de enviarlas al modelo.  \n",
    "- **`prediction_history = deque(maxlen=15)`**: Se utiliza una cola de tamaño 15 para almacenar las últimas predicciones y suavizar las salidas del modelo.  \n",
    "- **`fps_counter = deque(maxlen=30)`**: Almacena las últimas 30 mediciones de FPS para calcular la tasa de actualización en tiempo real.  \n",
    "- **`high_sensitivity_mode`**: Activa o desactiva un modo de alta sensibilidad cuando el modelo tiene dificultades para detectar gestos.  \n",
    "- **`unknown_counter`**: Lleva el conteo de predicciones inciertas para alternar entre modos de sensibilidad.  \n",
    "\n",
    "### **2.3 Optimización del rendimiento**\n",
    "```python\n",
    "try:\n",
    "    p = psutil.Process()\n",
    "    p.cpu_affinity([0, 1])  # Hilo principal en núcleos 0-1\n",
    "except Exception as e:\n",
    "    print(f\"Advertencia: No se pudo configurar afinidad de CPU: {str(e)}\")\n",
    "```\n",
    "- Se intenta fijar la ejecución del proceso en los núcleos 0 y 1 de la CPU para mejorar la eficiencia en sistemas multinúcleo.  \n",
    "- Si el sistema no lo permite, se muestra una advertencia sin afectar la ejecución.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Bucle principal de procesamiento**\n",
    "La función entra en un bucle donde captura video, extrae landmarks, realiza predicciones y muestra los resultados.  \n",
    "\n",
    "### **3.1 Cálculo de FPS**\n",
    "```python\n",
    "while True:\n",
    "    current_time = time.time()\n",
    "    fps_counter.append(1/(current_time - last_time + 1e-7))\n",
    "    last_time = current_time\n",
    "```\n",
    "- Calcula la tasa de frames por segundo usando el tiempo transcurrido entre iteraciones.  \n",
    "\n",
    "### **3.2 Captura de video**\n",
    "```python\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    continue\n",
    "```\n",
    "- Se obtiene un frame de la cámara UDP.  \n",
    "- Si no se recibe un frame válido, se omite la iteración.  \n",
    "\n",
    "### **3.3 Procesamiento de landmarks**\n",
    "```python\n",
    "rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(rgb_frame)\n",
    "```\n",
    "- Convierte el frame a formato RGB y lo procesa con MediaPipe para detectar manos.  \n",
    "\n",
    "Si se detectan manos, se extraen sus landmarks:  \n",
    "```python\n",
    "landmarks = [0.0] * total_landmarks\n",
    "\n",
    "if results.multi_hand_landmarks:\n",
    "    hand_landmarks = []\n",
    "    for hand in results.multi_hand_landmarks[:2]:  \n",
    "        for lm in hand.landmark:\n",
    "            hand_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "    if len(hand_landmarks) < total_landmarks:\n",
    "        hand_landmarks += [0.0] * (total_landmarks - len(hand_landmarks))\n",
    "    else:\n",
    "        hand_landmarks = hand_landmarks[:total_landmarks]  \n",
    "        \n",
    "    landmarks = hand_landmarks\n",
    "```\n",
    "- Si se detectan menos puntos de los esperados, se rellenan con ceros.  \n",
    "\n",
    "Los landmarks se añaden al buffer circular:  \n",
    "```python\n",
    "buffer.add(landmarks)\n",
    "```\n",
    "\n",
    "### **3.4 Inferencia del modelo**\n",
    "Cuando el buffer está lleno, se realiza la predicción:  \n",
    "```python\n",
    "if buffer.full:\n",
    "    try:\n",
    "        seq_array = buffer.get_sequence()\n",
    "        seq_array = (seq_array - X_mean) / (X_std + 1e-7)\n",
    "        input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "        \n",
    "        interpreter.set_tensor(input_details['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "        prediction = interpreter.get_tensor(output_details['index'])[0]\n",
    "```\n",
    "- La secuencia se normaliza y se pasa al modelo para obtener una predicción.  \n",
    "\n",
    "### **3.5 Modo de alta sensibilidad**\n",
    "Si la predicción es incierta en varias iteraciones, el sistema ajusta su modo de operación:  \n",
    "```python\n",
    "if np.max(prediction) < 0.5:\n",
    "    unknown_counter += 1\n",
    "    if unknown_counter >= 3:\n",
    "        high_sensitivity_mode = not high_sensitivity_mode\n",
    "        unknown_counter = 0\n",
    "```\n",
    "\n",
    "### **3.6 Suavizado de predicciones**\n",
    "```python\n",
    "prediction_history.append(prediction)\n",
    "smoothed_pred = np.mean(prediction_history, axis=0)\n",
    "predicted_idx = np.argmax(smoothed_pred)\n",
    "confidence = smoothed_pred[predicted_idx]\n",
    "```\n",
    "- Se usa una media móvil para evitar fluctuaciones en las predicciones.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Visualización y salida**\n",
    "El resultado se muestra en pantalla con OpenCV, y el programa finaliza si se presiona ESC.  \n",
    "```python\n",
    "cv2.imshow(\"Predicciones en Tiempo Real\", frame)\n",
    "if cv2.waitKey(1) & 0xFF == 27:\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "Esta implementación optimiza el rendimiento y la robustez del sistema para reconocimiento de gestos en tiempo real. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las mejoras clave que he hecho a su evaluate() la función incluye:\n",
    "\n",
    "* Manejo de ruta de modelo consistente: He definido constantes en la parte superior de la función tanto para el modelo como para los archivos de parámetros de normalización para garantizar la consistencia.\n",
    "\n",
    "* Gestión de secuencias simplificada: He eliminado el redundante sequence deque y consistentemente utilizó el CircularBuffer para gestionar secuencias de puntos de referencia manuales.\n",
    "\n",
    "* Procesamiento de punto de referencia más limpio: La extracción de puntos de referencia ahora es más sencilla, con un manejo adecuado de los casos en que hay demasiados o muy pocos puntos de referencia.\n",
    "\n",
    "* Mejor manejo de errores: Se agregaron mensajes de error más específicos e implementaron un continue declaración para manejar errores durante la predicción para que el bucle no se bloquee.\n",
    "\n",
    "* Código no utilizado eliminado: Se han eliminado el código comentado y las variables no utilizadas.\n",
    "\n",
    "* Se agregaron comentarios más detallados: Cada sección de la función ahora tiene comentarios claros que explican lo que hace.\n",
    "\n",
    "* Estructura mejorada: La función ahora sigue un flujo más lógico con secciones distintas para la inicialización, el procesamiento del bucle principal y la visualización.\n",
    "\n",
    "* Prevención de errores: Se agregó un bloque de prueba excepto para la configuración de afinidad de la CPU, ya que esto podría no funcionar en todas las plataformas.\n",
    "\n",
    "* Mejor gestión de marcos: Añadido a continue instrucción cuando falla la recuperación de fotogramas en lugar de procesar datos potencialmente no válidos.\n",
    "\n",
    "* Retroalimentación visual para cambios de modo: Se agregó una declaración de impresión para informar cuando el sistema cambia entre los modos normal y de alta sensibilidad.\n",
    "\n",
    "Esta función revisada debe ser más confiable, mantenible y consistente, al tiempo que conserva todas las características avanzadas de su implementación original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menú principal\n",
    "def main():\n",
    "    init_system()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Detectar Manos\")\n",
    "        print(\"2. Recolectar Datos\")\n",
    "        print(\"3. Entrenar Modelo, y despues ir a convertir a TFlite\")\n",
    "        print(\"4. Evaluar\")\n",
    "        print(\"5. Convertir a TFLite\")  # Nueva opción\n",
    "        print(\"6. Salir\")\n",
    "        \n",
    "        choice = input(\"\\nSeleccione una opción: 1. Detectar Manos, 2. Recolectar Datos, 3. Entrenar Modelo, 4. Evaluar, 5. Convertir a TFLite, 6. Salir \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            detect_hands()\n",
    "        elif choice == '2':\n",
    "            collect_data()\n",
    "        elif choice == '3':\n",
    "            train_model()\n",
    "        elif choice == '4':\n",
    "            evaluate()\n",
    "        elif choice == '5':  # Nueva opción de conversión\n",
    "            convert_to_tflite()\n",
    "        elif choice == '6':\n",
    "            print(\"\\n¡Hasta luego!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nOpción inválida. Por favor, intente de nuevo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Convertir a TFLite\n",
      "6. Salir\n",
      "Output details shape: [1 5]\n",
      "UDP Camera started on 0.0.0.0:5000\n",
      "UDP Camera released\n",
      "UDP Camera released\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Convertir a TFLite\n",
      "6. Salir\n",
      "\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
