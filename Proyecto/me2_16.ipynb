{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import mediapipe as mp # se te corto el audio es verdad \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import time\n",
    "#import threading\n",
    "#import pyttsx3\n",
    "from collections import Counter\n",
    "#from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def speak_async(engine, text):\n",
    "#    threading.Thread(target=lambda: (engine.say(text), engine.runAndWait())).start()\n",
    "num_camara=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "pose = mp_pose.Pose(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    model_complexity=1  # Reducir la complejidad\n",
    ")\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    max_num_hands=2,\n",
    "    model_complexity=0  # Complejidad mínima\n",
    ")\n",
    "\n",
    "# Variables globales\n",
    "data_dir = \"sign_language_data_JUPYTER\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "sequence_length = 30  # Frames por secuencia\n",
    "n_pose_landmarks = 33 * 3\n",
    "n_hand_landmarks = 21 * 3\n",
    "total_landmarks = n_pose_landmarks + (n_hand_landmarks * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PROCESAR LOS FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    \"\"\"Procesa un frame y retorna los resultados de pose y manos\"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pose_results = pose.process(rgb_frame)\n",
    "    hands_results = hands.process(rgb_frame)\n",
    "    return pose_results, hands_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EXTRAER LOS LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(pose_results, hands_results):\n",
    "    \"\"\"Extrae y normaliza los landmarks de pose y manos\"\"\"\n",
    "    landmarks = []\n",
    "\n",
    "    if pose_results.pose_landmarks:\n",
    "        pose_landmarks = [[lm.x, lm.y, lm.z] for lm in pose_results.pose_landmarks.landmark]\n",
    "        landmarks.extend(np.array(pose_landmarks).flatten())\n",
    "    else:\n",
    "        landmarks.extend([0] * n_pose_landmarks)\n",
    "\n",
    "    hand_landmarks_list = []\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks[:2]:\n",
    "            hand_points = [[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]\n",
    "            hand_landmarks_list.extend(np.array(hand_points).flatten())\n",
    "\n",
    "    while len(hand_landmarks_list) < n_hand_landmarks * 2:\n",
    "        hand_landmarks_list.extend([0] * n_hand_landmarks)\n",
    "\n",
    "    landmarks.extend(hand_landmarks_list)\n",
    "    return np.array(landmarks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(data_dir, sign_name, sequence_length):\n",
    "    \"\"\"Recolecta secuencias de movimiento para una seña específica\"\"\"\n",
    "    sign_dir = os.path.join(data_dir, sign_name)\n",
    "    os.makedirs(sign_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(num_camara)  # Cambiar índice si usas DroidCam u otra cámara\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir la cámara\")\n",
    "        return\n",
    "\n",
    "    total_sequences = int(input(\"Número de secuencias a recolectar (recomendado: 20-30): \"))\n",
    "\n",
    "    print(\"\\nInstrucciones:\")\n",
    "    print(f\"1. Cada secuencia grabará {sequence_length} frames de movimiento\")\n",
    "    print(\"2. Presiona ESPACIO para iniciar cada secuencia\")\n",
    "    print(\"3. Realiza el movimiento completo de la seña\")\n",
    "    print(\"4. La grabación se detendrá automáticamente\")\n",
    "    print(\"5. Presiona ESC para cancelar\")\n",
    "\n",
    "    sequence_count = 0\n",
    "    frame_count = 0\n",
    "    is_recording = False\n",
    "    current_sequence = []\n",
    "\n",
    "    frame_skip = 2  # Procesar un frame de cada 2\n",
    "    frame_counter = 0\n",
    "\n",
    "    while sequence_count < total_sequences:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_counter += 1\n",
    "        if frame_counter % frame_skip != 0:\n",
    "            continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        pose_results, hands_results = process_frame(frame)\n",
    "\n",
    "        if pose_results.pose_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if not is_recording:\n",
    "            cv2.putText(frame, \"Presione ESPACIO para grabar secuencia\",\n",
    "                        (10, frame.shape[0] - 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, f\"Grabando secuencia... Frame {frame_count}/{sequence_length}\",\n",
    "                        (10, frame.shape[0] - 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "            landmarks = extract_landmarks(pose_results, hands_results)\n",
    "            current_sequence.append(landmarks)\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count >= sequence_length:\n",
    "                sequence_data = np.array(current_sequence)\n",
    "                np.save(os.path.join(sign_dir, f\"sequence_{sequence_count}.npy\"), sequence_data)\n",
    "                print(f\"Secuencia {sequence_count + 1}/{total_sequences} guardada\")\n",
    "\n",
    "                sequence_count += 1\n",
    "                frame_count = 0\n",
    "                is_recording = False\n",
    "                current_sequence = []\n",
    "\n",
    "        cv2.putText(frame, f\"Secuencias: {sequence_count}/{total_sequences}\",\n",
    "                    (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        resized_frame = cv2.resize(frame, (640, 480))\n",
    "        cv2.imshow(\"Recolección de Datos\", resized_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 32 and not is_recording:\n",
    "            is_recording = True\n",
    "            current_sequence = []\n",
    "            frame_count = 0\n",
    "            print(f\"\\nGrabando secuencia {sequence_count + 1}...\")\n",
    "        elif key == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\nTotal de secuencias guardadas: {sequence_count}\")\n",
    "    print(f\"Ubicación: {sign_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir, sequence_length, total_landmarks, model_file):\n",
    "    \"\"\"Entrena el modelo utilizando CNN, LSTM y Transformers.\"\"\"\n",
    "    \n",
    "    if not os.listdir(data_dir):\n",
    "        print(\"No hay datos para entrenar\")\n",
    "        return\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "    print(\"Cargando secuencias...\")\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        samples = [f for f in os.listdir(class_dir) if f.startswith('sequence_')]\n",
    "        print(f\"Clase {class_name}: {len(samples)} secuencias\")\n",
    "\n",
    "        for sample_file in samples:\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sequence = np.load(sample_path)\n",
    "            X.append(sequence)\n",
    "            y.append(class_idx)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    # Normalización de los datos\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Definir dimensiones del modelo\n",
    "    input_shape = (sequence_length, total_landmarks)\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Arquitectura del modelo\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Bloque CNN para extracción de características espaciales\n",
    "    cnn = tf.keras.layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    cnn = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(cnn)\n",
    "    cnn = tf.keras.layers.MaxPooling1D(pool_size=2)(cnn)\n",
    "    cnn = tf.keras.layers.Dropout(0.3)(cnn)\n",
    "\n",
    "    # Bloque LSTM para modelado temporal a largo plazo\n",
    "    lstm = tf.keras.layers.LSTM(128, return_sequences=True)(cnn)\n",
    "    lstm = tf.keras.layers.Dropout(0.3)(lstm)\n",
    "\n",
    "    # Bloque Transformer para relaciones temporales complejas\n",
    "    transformer = tf.keras.layers.LayerNormalization()(lstm)\n",
    "    transformer = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=128)(transformer, transformer)\n",
    "    transformer = tf.keras.layers.Add()([transformer, lstm])  # Residual connection\n",
    "    transformer = tf.keras.layers.LayerNormalization()(transformer)\n",
    "    transformer = tf.keras.layers.Dense(128, activation='relu')(transformer)\n",
    "\n",
    "    # Capas finales\n",
    "    global_pool = tf.keras.layers.GlobalAveragePooling1D()(transformer)\n",
    "    dense = tf.keras.layers.Dense(64, activation='relu')(global_pool)\n",
    "    dense = tf.keras.layers.Dropout(0.4)(dense)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"\\nResumen del modelo:\")\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save(model_file)\n",
    "    print(f\"\\nModelo guardado en {model_file}\")\n",
    "\n",
    "    # Visualización del entrenamiento\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "    plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "    plt.title('Precisión del Modelo')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "    plt.title('Pérdida del Modelo')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARGA DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_spoken_word = None  # Variable global para almacenar la última palabra pronunciada\n",
    "\n",
    "def load_model(model_file):\n",
    "    \"\"\"Carga el modelo previamente entrenado.\"\"\"\n",
    "    if not os.path.exists(model_file):\n",
    "        print(\"No se encontró el modelo entrenado\")\n",
    "        return None\n",
    "    #Cargar el modelo entrenado\n",
    "    return tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARGA DE LA INFORMACION DE PRUEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(data_dir, sequence_length):\n",
    "    \"\"\"Carga y prepara los datos de prueba.\"\"\"\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "    # Crear conjunto de prueba\n",
    "    print(\"Cargando datos de prueba...\")\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        samples = [f for f in os.listdir(class_dir) if f.startswith('sequence_')]\n",
    "\n",
    "        # Seleccionar el 20% de las secuencias como prueba\n",
    "        test_samples = samples[:int(0.2 * len(samples))]\n",
    "        for sample_file in test_samples:\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sequence = np.load(sample_path)\n",
    "            X_test.append(sequence)\n",
    "            y_test.append(class_idx)\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    return X_test, y_test, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUEBA DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evalúa el modelo en los datos de prueba.\"\"\"\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"\\nPérdida en prueba: {loss:.4f}\")\n",
    "    print(f\"Precisión en prueba: {accuracy:.2%}\")\n",
    "\n",
    "    # Mostrar métricas globales\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar([\"Pérdida\", \"Precisión\"], [loss, accuracy], color=['blue', 'green'])\n",
    "    plt.title(\"Métricas globales en prueba\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRUEBA DEL MODEELO EN TIEMPO REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_realtime(model_file, data_dir, sequence_length, mp_draw, mp_pose, mp_hands, num_camara):\n",
    "    \"\"\"Realiza la evaluación en tiempo real usando una cámara.\"\"\"\n",
    "\n",
    "    # Cargar el modelo entrenado\n",
    "    model = load_model(model_file)\n",
    "    if model is None:\n",
    "        return\n",
    "    class_names = sorted(os.listdir(data_dir))  # Nombres de las clases (carpetas)\n",
    "\n",
    "    cap = cv2.VideoCapture(num_camara)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir la cámara.\")\n",
    "        return\n",
    "\n",
    "    current_sequence = []\n",
    "    predictions_buffer = []\n",
    "    hand_detected = False\n",
    "    frame_counter = 0\n",
    "\n",
    "    # Variables para métricas de tiempo real\n",
    "    prediction_confidences = []  # Guardar las confianzas de las predicciones\n",
    "    prediction_stabilities = []  # Guardar estabilidad de la predicción\n",
    "    frame_count = 0\n",
    "\n",
    "\n",
    "    print(\"\\nEvaluando en tiempo real. Presione ESC para salir.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error al capturar el frame.\")\n",
    "            break\n",
    "\n",
    "        frame_counter += 1\n",
    "\n",
    "        # Solo procesar cada 2 frames para reducir carga computacional\n",
    "            #if frame_counter % 2 != 0:\n",
    "            #    continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        pose_results, hands_results = process_frame(frame)\n",
    "\n",
    "        # Dibujar pose y manos si están disponibles\n",
    "        if pose_results.pose_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, pose_results.pose_landmarks,\n",
    "                                            mp_pose.POSE_CONNECTIONS)\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                    mp_draw.draw_landmarks(frame, hand_landmarks,\n",
    "                                                mp_hands.HAND_CONNECTIONS)\n",
    "            hand_detected = True\n",
    "        else:\n",
    "            hand_detected = False\n",
    "\n",
    "        \"\"\"# Procesar landmarks para la predicción\n",
    "        landmarks = extract_landmarks(pose_results, hands_results)\n",
    "        if landmarks is not None:\n",
    "            current_sequence.append(landmarks)\"\"\"\n",
    "\n",
    "        if hand_detected:\n",
    "            landmarks = extract_landmarks(pose_results, hands_results)\n",
    "            current_sequence.append(landmarks)\n",
    "\n",
    "            if len(current_sequence) > sequence_length:\n",
    "                current_sequence.pop(0)\n",
    "\n",
    "            if len(current_sequence) == sequence_length:\n",
    "                sequence_data = np.array([current_sequence])\n",
    "                prediction = model.predict(sequence_data, verbose=0)\n",
    "                predicted_class = class_names[np.argmax(prediction[0])]\n",
    "                confidence = np.max(prediction[0])\n",
    "\n",
    "                # Registro de predicciones y confianzas\n",
    "                prediction_confidences.append(confidence)\n",
    "\n",
    "                # Historial de predicciones para obtener estabilidad\n",
    "                predictions_buffer.append(predicted_class)\n",
    "                if len(predictions_buffer) > 5:\n",
    "                    predictions_buffer.pop(0)\n",
    "\n",
    "                from collections import Counter\n",
    "                most_common = Counter(predictions_buffer).most_common(1)[0]\n",
    "                stable_prediction = most_common[0]\n",
    "                stability = most_common[1] / len(predictions_buffer)\n",
    "                prediction_stabilities.append(stability)\n",
    "\n",
    "                # Mostrar la predicción si la estabilidad es alta\n",
    "                if stability > 0.6:\n",
    "                    cv2.putText(frame, f\"Seña: {stable_prediction}\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"Confianza: {confidence:.2%}\", (10, 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                    \"\"\"if confidence > 0.70 and stable_prediction != last_spoken_word:\n",
    "                        last_spoken_word = stable_prediction\n",
    "                        #speak_async(self.engine, stable_prediction)\"\"\"    \n",
    "        \n",
    "        # Redimensionar y mostrar el frame\n",
    "        resized_frame = cv2.resize(frame, (640, 480))\n",
    "        cv2.imshow(\"Evaluación\", resized_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # ESC para salir\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Mostrar métricas de tiempo real al final\n",
    "    if prediction_confidences:\n",
    "        avg_confidence = np.mean(prediction_confidences)\n",
    "        avg_stability = np.mean(prediction_stabilities)\n",
    "\n",
    "        print(f\"\\nPromedio de confianza: {avg_confidence:.2%}\")\n",
    "        print(f\"Promedio de estabilidad: {avg_stability:.2%}\")\n",
    "\n",
    "# Visualización de la confianza y estabilidad\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(prediction_confidences, label=\"Confianza\")\n",
    "        plt.title(\"Confianza de Predicciones en Tiempo Real\")\n",
    "        plt.xlabel(\"Frame\")\n",
    "        plt.ylabel(\"Confianza\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(prediction_stabilities, label=\"Estabilidad\")\n",
    "        plt.title(\"Estabilidad de Predicciones en Tiempo Real\")\n",
    "        plt.xlabel(\"Frame\")\n",
    "        plt.ylabel(\"Estabilidad\")\n",
    "        plt.legend()\n",
    "        plt.show()    # Graficar confianza y estabilidad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MENU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINICION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #data_dir = \"data\"  # Directorio para los datos recolectados\n",
    "    data_dir = \"sign_language_data_JUPYTER\"\n",
    "    model_file = \"sign_language_model.h5\"\n",
    "    sequence_length = 30  # Longitud de la secuencia para cada muestra\n",
    "    num_camara = 0  # Índice de la cámara\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Ver detección de pose y manos\")\n",
    "        print(\"2. Recolectar datos de señas\")\n",
    "        print(\"3. Entrenar modelo\")\n",
    "        print(\"4. Evaluar en tiempo real\")\n",
    "        print(\"5. Salir\")\n",
    "        \n",
    "        option = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if option == \"1\":\n",
    "            cap = cv2.VideoCapture(num_camara)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                pose_results, hands_results = process_frame(frame)\n",
    "                # Dibuja los landmarks si se detectan\n",
    "                if pose_results.pose_landmarks:\n",
    "                    mp_draw.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "                \n",
    "                if hands_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                        mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                cv2.imshow(\"Detección de Pose y Manos\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == 27:  # ESC para salir\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        elif option == \"2\":\n",
    "            sign_name = input(\"Nombre de la seña a recolectar: \")\n",
    "            collect_data(data_dir, sign_name, sequence_length)\n",
    "        \n",
    "        elif option == \"3\":\n",
    "            train_model(data_dir, sequence_length, total_landmarks, model_file)\n",
    "        \n",
    "        elif option == \"4\":\n",
    "            evaluate_realtime(model_file, data_dir, sequence_length, mp_draw, mp_pose, mp_hands, num_camara)\n",
    "        \n",
    "        elif option == \"5\":\n",
    "            print(\"¡Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Opción no válida.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOSTRAR EL MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
