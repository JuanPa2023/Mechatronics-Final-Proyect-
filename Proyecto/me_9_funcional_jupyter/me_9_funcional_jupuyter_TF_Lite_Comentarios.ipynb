{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.saving.saving_api import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from collections import deque  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este Programa no cambia del me_8 hasta la etapa de entrenamiento en adelante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial global\n",
    "# Importa el módulo de manos de MediaPipe para la detección de landmarks\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Inicializa el detector de manos con configuración específica:\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,  # Modo video (mejor para secuencias)\n",
    "    max_num_hands=2,          # Máximo número de manos a detectar\n",
    "    min_detection_confidence=0.45,  # Confianza mínima para detección inicial\n",
    "    min_tracking_confidence=0.45,   # Confianza mínima para seguimiento continuo\n",
    "    model_complexity=1        # Balance entre precisión y rendimiento (0=ligero, 1=medio, 2=completo)\n",
    ")\n",
    "\n",
    "# Utilidades para dibujar los landmarks y conexiones en la imagen\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "# Directorio donde se almacenarán los datos del dataset\n",
    "dataset_dir = \"dataset_11\"\n",
    "# Ruta donde se guardará/recuperará el modelo entrenado\n",
    "model_path = \"gesture_model_me_10.h5\"\n",
    "# Longitud de las secuencias temporales (número de frames por muestra)\n",
    "sequence_length = 30\n",
    "# Número total de landmarks (21 puntos por mano * 3 coordenadas (x,y,z) * 2 manos)\n",
    "total_landmarks = 126\n",
    "# Lista para almacenar los nombres de los gestos/clases\n",
    "gestures = []\n",
    "# Variables para normalización de datos (media y desviación estándar)\n",
    "X_mean = None\n",
    "X_std = None\n",
    "\n",
    "\n",
    "num_camara = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FUNCIONES PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def init_system():\n",
    "    global gestures # Accede a la variable global para almacenar los gestos\n",
    "\n",
    "    # Crea el directorio principal del dataset si no existe\n",
    "    # exist_ok=True evita errores si el directorio ya existe    \n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Obtiene la lista de gestos existentes del disco\n",
    "    gestures = get_existing_gestures()\n",
    "    \n",
    "# Función para obtener los gestos ya registrados\n",
    "def get_existing_gestures():\n",
    "    #Lista comprensiva que recorre todos los elementos en el directorio del dataset\n",
    "    return [d for d in os.listdir(dataset_dir) \n",
    "            # Verifica si es un directorio (y no un archivo)\n",
    "           if os.path.isdir(os.path.join(dataset_dir, d))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DETECCION DE MANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hands():\n",
    "    # Muestra mensaje de inicio para el usuario\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")\n",
    "    # Inicializa la captura de video desde la cámara predeterminada (índice 0)\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Bucle principal de procesamiento de frames\n",
    "    while True:\n",
    "        # Lee un frame de la cámara\n",
    "        ret, frame = cap.read()\n",
    "        # Verifica si la lectura del frame fue exitosa\n",
    "        if not ret:\n",
    "            break  # Sale del bucle si hay error\n",
    "\n",
    "        # Convierte el frame de BGR (OpenCV) a RGB (MediaPipe)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Procesa el frame con el modelo de detección de manos\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        # Si se detectaron landmarks de manos\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Itera sobre cada mano detectada\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Dibuja los landmarks y conexiones en el frame\n",
    "                mp_draw.draw_landmarks(\n",
    "                    frame, \n",
    "                    hand_landmarks, \n",
    "                    mp_hands.HAND_CONNECTIONS  # Dibuja conexiones entre landmarks\n",
    "                )\n",
    "\n",
    "        # Muestra el frame procesado en una ventana\n",
    "        cv2.imshow(\"Detección de Manos\", frame)\n",
    "        \n",
    "        # Verifica si se presionó la tecla ESC (código 27)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break  # Sale del bucle\n",
    "\n",
    "    # Libera los recursos de la cámara\n",
    "    cap.release()\n",
    "    \n",
    "    # Cierra todas las ventanas de OpenCV\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    global gestures  # Accede a la variable global de gestos\n",
    "    \n",
    "    # Solicita datos al usuario\n",
    "    gesture = input(\"\\nIngrese la palabra o letra para la cual desea recolectar datos: \").upper()\n",
    "    num_sequences = int(input(\"Ingrese el número de secuencias a capturar (recomendado: 50): \"))\n",
    "    \n",
    "    # Crea directorio para almacenar los datos\n",
    "    save_dir = os.path.join(dataset_dir, gesture)\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Crea el directorio si no existe\n",
    "\n",
    "    # Mensajes informativos\n",
    "    print(f\"\\nRecolectando datos para el gesto '{gesture}'. Presiona 'ESC' para cancelar.\")\n",
    "    print(\"Mantenga la seña frente a la cámara...\")\n",
    "    \n",
    "    # Inicializa captura de video\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    sequence = []  # Almacena la secuencia actual\n",
    "    counter = 0    # Contador de secuencias guardadas\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Procesamiento del frame\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            # Extrae landmarks de hasta 2 manos\n",
    "            all_landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:  # Máximo 2 manos\n",
    "                for lm in hand.landmark:\n",
    "                    all_landmarks.extend([lm.x, lm.y, lm.z])  # Coordenadas normalizadas\n",
    "            \n",
    "            # Rellena con ceros si solo hay una mano detectada\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                all_landmarks += [0.0] * 63  # 21 landmarks * 3 coordenadas\n",
    "            \n",
    "            sequence.append(all_landmarks)  # Agrega a la secuencia\n",
    "            \n",
    "            # Dibuja landmarks en el frame\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Guarda la secuencia cuando alcanza la longitud deseada\n",
    "        if len(sequence) == sequence_length:\n",
    "            np.save(os.path.join(save_dir, f\"secuencia_{counter}.npy\"), sequence)\n",
    "            counter += 1\n",
    "            sequence = []  # Reinicia la secuencia\n",
    "            print(f\"Secuencias capturadas: {counter}/{num_sequences}\")\n",
    "\n",
    "        # Muestra vista previa\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)\n",
    "        \n",
    "        # Condiciones de salida: ESC o completar secuencias\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or counter >= num_sequences:\n",
    "            break\n",
    "\n",
    "    # Libera recursos\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Actualiza lista de gestos\n",
    "    gestures = get_existing_gestures()\n",
    "    print(f\"\\nSe recolectaron {counter} secuencias para el gesto '{gesture}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Listas para almacenar características (X) y etiquetas (y)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Iterar sobre cada gesto con su índice como etiqueta\n",
    "    for label_idx, gesture in enumerate(gestures):\n",
    "        # Construir ruta al directorio del gesto\n",
    "        gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "        \n",
    "        # Listar solo archivos .npy válidos (30,126)\n",
    "        sequences = [f for f in os.listdir(gesture_dir) if f.endswith('.npy')]\n",
    "        \n",
    "        # Procesar cada secuencia del gesto actual\n",
    "        for seq_file in sequences:\n",
    "            seq_path = os.path.join(gesture_dir, seq_file)\n",
    "            sequence = np.load(seq_path)  # Cargar secuencia desde disco\n",
    "            \n",
    "            # Validar dimensiones del dato (30 frames × 126 landmarks)\n",
    "            if sequence.shape == (sequence_length, total_landmarks):\n",
    "                X.append(sequence)       # Agregar secuencia a características\n",
    "                y.append(label_idx)      # Agregar índice como etiqueta\n",
    "    \n",
    "    # Convertir a arrays numpy y retornar con nombres de gestos\n",
    "    return np.array(X), np.array(y), gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global X_mean, X_std, gestures  # Accede a variables globales para normalización y gestos\n",
    "    \n",
    "    # 1. Verificación inicial de datos\n",
    "    gestures = get_existing_gestures()  # Obtiene lista de gestos desde el directorio\n",
    "    if not gestures:  # Valida existencia de datos\n",
    "        print(\"\\nNo hay datos recolectados. Primero recolecte datos de gestos.\")\n",
    "        return\n",
    "\n",
    "    # 2. Carga y preparación de datos\n",
    "    print(\"\\nCargando datos y preparando el entrenamiento...\")\n",
    "    X, y, gestures = load_data()  # Carga secuencias y etiquetas desde .npy\n",
    "    y = tf.keras.utils.to_categorical(y)  # Convierte etiquetas a one-hot encoding\n",
    "\n",
    "    # 3. Normalización de datos\n",
    "    X_mean = np.mean(X, axis=(0, 1))  # Calcula media por landmark\n",
    "    X_std = np.std(X, axis=(0, 1))  # Calcula desviación estándar\n",
    "    X = (X - X_mean) / X_std  # Estandarización de características\n",
    "\n",
    "    # 4. Persistencia de parámetros\n",
    "    np.savez('normalization_params.npz', mean=X_mean, std=X_std)  # Guarda stats para inferencia\n",
    "    \n",
    "    # 5. Construcción del modelo\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))  # Capa de entrada\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)  # Extracción de patrones locales\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)  # Reducción dimensional\n",
    "    x = tf.keras.layers.LSTM(64, return_sequences=False)(x)  # Procesamiento temporal\n",
    "    outputs = tf.keras.layers.Dense(len(gestures), activation='softmax')(x)  # Clasificación final\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)  # Ensamblado del modelo\n",
    "\n",
    "    # 6. Configuración del entrenamiento\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Optimizador adaptativo\n",
    "        loss='categorical_crossentropy',  # Función de pérdida para clasificación\n",
    "        metrics=['accuracy']  # Métrica principal\n",
    "    )\n",
    "\n",
    "    # 7. Proceso de entrenamiento\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,  # Máximo de iteraciones\n",
    "        batch_size=32,  # Tamaño de lote\n",
    "        validation_split=0.2,  # 20% datos para validación\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],  # Control overfitting\n",
    "        verbose=1  # Mostrar progreso\n",
    "    )\n",
    "\n",
    "    # 8. Persistencia del modelo\n",
    "    model.save(model_path)  # Guardado en formato HDF5\n",
    "    print(f\"\\nModelo guardado en {model_path}\")\n",
    "    \n",
    "    # 9. Conversión para despliegue\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)  # Inicializa conversor\n",
    "    converter.target_spec.supported_ops = [  # Configura compatibilidad\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter._experimental_lower_tensor_list_ops = False  # Para compatibilidad con LSTM\n",
    "    \n",
    "    try:  # Manejo de errores en conversión\n",
    "        tflite_model = converter.convert()\n",
    "        with open('model_quantized.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(\"\\nModelo TFLite exportado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError en conversión TFLite: {str(e)}\")\n",
    "    \n",
    "    # 10. Reporte final\n",
    "    val_accuracy = history.history['val_accuracy'][-1]  # Última métrica de validación\n",
    "    print(f\"Precisión de validación final: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite():\n",
    "    try:\n",
    "        # PASO 1: Cargar el modelo entrenado (ej: 'mi_modelo.h5')\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        # Imagina que model_path es como la dirección de tu casa donde guardaste el modelo\n",
    "        \n",
    "        # PASO 2: Preparar el \"traductor\" (conversor)\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        # Crear un conversor especializado para tu modelo\n",
    "        \n",
    "        # PASO 3: Configurar compatibilidad para LSTM (¡CRUCIAL!)\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,  # Operaciones básicas de TFLite\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS     # Operaciones especiales de TensorFlow\n",
    "        ]\n",
    "        # Esto es como decirle al traductor: \"Usa el diccionario básico y agrega palabras técnicas\"\n",
    "        \n",
    "        converter._experimental_lower_tensor_list_ops = False\n",
    "        # Desactiva modificaciones automáticas en las operaciones de listas (necesario para LSTM)\n",
    "        \n",
    "        converter.allow_custom_ops = True\n",
    "        # Permite operaciones personalizadas (como un traductor que acepta jerga local)\n",
    "        \n",
    "        # PASO 4: Realizar la conversión\n",
    "        tflite_model = converter.convert()\n",
    "        # ¡Traducción completada! Ahora tenemos el modelo en \"idioma móvil\"\n",
    "        \n",
    "        # PASO 5: Guardar el nuevo modelo\n",
    "        with open('model_quantized.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        # Guardamos el libro traducido en un nuevo archivo .tflite\n",
    "            \n",
    "        print(\"\\n✅ Conversión a TFLite exitosa!\")\n",
    "        # ¡Éxito! El modelo ahora es compatible con apps móviles\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error en conversión: {str(e)}\")\n",
    "        print(\"Posibles soluciones:\")\n",
    "        print(\"1. Verifique que el modelo .h5 existe\")\n",
    "        print(\"2. Actualice TensorFlow: pip install --upgrade tensorflow\")\n",
    "        print(\"3. Reinicie el runtime/kernel\")\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    # Genera 100 muestras de datos sintéticos\n",
    "    for _ in range(100):\n",
    "        # Crea un tensor de ejemplo con forma (1, 30, 225) si sequence_length=30 y total_landmarks=225\n",
    "        ejemplo = np.random.randn(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "        yield [ejemplo]\n",
    "        # Entrega los datos de ejemplo al conversor como un lote de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ IMPORTS ADICIONALES NECESARIOS ------------------\n",
    "from collections import deque  # Cola eficiente tipo FIFO (First-In First-Out)\n",
    "from threading import Thread   # Para ejecutar tareas en paralelo\n",
    "from queue import Queue        # Colas seguras para hilos (thread-safe)\n",
    "\n",
    "# Ejemplo: Imagina estos imports como contratar especialistas:\n",
    "# - deque: Un organizador de tareas\n",
    "# - Thread: Un equipo de trabajadores\n",
    "# - Queue: Cintas transportadoras entre estaciones de trabajo\n",
    "\n",
    "# ------------------ SECCIÓN DE CONFIGURACIÓN ------------------\n",
    "# Creación de 3 cintas transportadoras (colas) con capacidades diferentes\n",
    "frame_queue = Queue(maxsize=30)      # Almacena hasta 30 fotogramas\n",
    "landmark_queue = Queue(maxsize=20)   # Almacena hasta 20 conjuntos de landmarks\n",
    "prediction_queue = Queue(maxsize=10) # Almacena hasta 10 predicciones\n",
    "\n",
    "# Analogía: Una fábrica con 3 cintas transportadoras:\n",
    "# 1. Para materias primas (frames)\n",
    "# 2. Para piezas procesadas (landmarks)\n",
    "# 3. Para productos terminados (predicciones)\n",
    "\n",
    "# ------------------ FUNCIÓN CAMERA_CAPTURE ------------------\n",
    "def camera_capture(cap):\n",
    "    while True:  # Bucle infinito\n",
    "        ret, frame = cap.read()  # Leer cámara (ej: 640x480)\n",
    "        if ret:  # Si el frame es válido\n",
    "            # Reducir resolución para ahorrar procesamiento (320x240)\n",
    "            resized_frame = cv2.resize(frame, (320, 240))\n",
    "            frame_queue.put(resized_frame)  # Poner en la cinta transportadora\n",
    "\n",
    "# Ejemplo práctico:\n",
    "# - Frame original: 640x480 (307,200 píxeles)\n",
    "# - Frame redimensionado: 320x240 (76,800 píxeles) -> 75% menos datos\n",
    "\n",
    "# ------------------ FUNCIÓN LANDMARK_PROCESSING ------------------\n",
    "def landmark_processing():\n",
    "    while True:\n",
    "        frame = frame_queue.get()  # Tomar frame de la cinta\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convertir color\n",
    "        results = hands.process(rgb_frame)  # Detectar manos con MediaPipe\n",
    "        \n",
    "        # Crear contenedor para 21 landmarks * 2 manos * 3 coordenadas (x,y,z)\n",
    "        all_landmarks = np.zeros(total_landmarks, dtype=np.float32)\n",
    "        \n",
    "        if results.multi_hand_landmarks:  # Si detecta manos\n",
    "            idx = 0  # Puntero de posición en el array\n",
    "            # Procesar máximo 2 manos ([:2]) por si hay dos en pantalla\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:  # Por cada landmark de la mano\n",
    "                    # Almacenar coordenadas normalizadas\n",
    "                    all_landmarks[idx:idx+3] = [lm.x, lm.y, lm.z]\n",
    "                    idx += 3  # Mover puntero 3 posiciones\n",
    "        \n",
    "        landmark_queue.put(all_landmarks)  # Pasar a la siguiente cinta\n",
    "\n",
    "# Ejemplo de datos:\n",
    "# all_landmarks = [0.5, 0.3, 0.1, 0.6, 0.2, 0.9, ...]  # 126 valores en total\n",
    "\n",
    "# ------------------ FUNCIÓN MODEL_INFERENCE ------------------\n",
    "def model_inference(interpreter, input_details, output_details):\n",
    "    sequence = deque(maxlen=sequence_length)  # Historial de últimos N landmarks\n",
    "    \n",
    "    while True:\n",
    "        data = landmark_queue.get()  # Obtener landmarks de la cinta\n",
    "        if data is not None:\n",
    "            sequence.append(data)  # Agregar al historial\n",
    "            sequence = sequence[-sequence_length:]  # Mantener sólo los últimos\n",
    "            \n",
    "            if len(sequence) == sequence_length:  # Cuando tenemos una secuencia completa\n",
    "                # Normalización de datos (restar media y dividir por desviación)\n",
    "                seq_array = (np.array(sequence) - X_mean) / X_std\n",
    "                # Reformatear para el modelo: (1, 30, 126) si sequence_length=30\n",
    "                input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "                \n",
    "                # Ejecutar modelo TFLite\n",
    "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                interpreter.invoke()  # Lanzar cálculo\n",
    "                prediction = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "                \n",
    "                # Obtener resultados\n",
    "                predicted_class = np.argmax(prediction)  # Índice de mayor probabilidad\n",
    "                confidence = np.max(prediction)  # Valor de confianza\n",
    "                prediction_queue.put((gestures[predicted_class], confidence))\n",
    "\n",
    "# Ejemplo de predicción:\n",
    "# prediction = [0.1, 0.8, 0.05, 0.05] -> predicted_class=1 (gestures[1]=\"Hola\")\n",
    "# confidence=0.8 (80% de seguridad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def evaluate():\n",
    "    # 1. Verificación inicial del modelo\n",
    "    if not os.path.exists(\"model_quantized.tflite\"):\n",
    "        print(\"\\n¡Primero debe entrenar y convertir el modelo!\")\n",
    "        return\n",
    "    # Ejemplo: Es como querer usar un teléfono sin batería, primero debes cargarlo\n",
    "\n",
    "    # 2. Carga de parámetros y modelo\n",
    "    try:\n",
    "        # 2.1 Cargar estadísticas de normalización (media y desviación)\n",
    "        with np.load('normalization_params.npz') as data:\n",
    "            X_mean = data['mean']  # Ej: [0.45, 0.32, ...] (126 valores)\n",
    "            X_std = data['std']    # Ej: [0.12, 0.08, ...]\n",
    "        \n",
    "        # 2.2 Configurar intérprete de TensorFlow Lite\n",
    "        interpreter = tf.lite.Interpreter(model_path=\"model_quantized.tflite\")\n",
    "        interpreter.allocate_tensors()  # Preparar memoria para el modelo\n",
    "        # Analogía: Cargar un videojuego y asignar memoria de la consola\n",
    "        \n",
    "        input_details = interpreter.get_input_details()[0]  # Detalles entrada\n",
    "        output_details = interpreter.get_output_details()[0]  # Detalles salida\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError crítico: {str(e)}\")  # Ej: Si el archivo .npz no existe\n",
    "        return\n",
    "\n",
    "    # 3. Configuración de la cámara\n",
    "    cap = cv2.VideoCapture(0)  # Iniciar cámara (ID 0 = cámara predeterminada)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)   # Ancho de 640px\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)  # Alto de 480px\n",
    "    # Ejemplo: Configurar una cámara de seguridad con resolución específica\n",
    "\n",
    "    # 4. Variables de estado\n",
    "    current_gesture = None  # Almacena la última seña detectada\n",
    "    current_confidence = 0.0  # Nivel de confianza (0.0 a 1.0)\n",
    "    sequence = deque(maxlen=sequence_length)  # Buffer de últimos N frames\n",
    "    frame_counter = 0  # Contador para saltar frames\n",
    "\n",
    "    # 5. Bucle principal de procesamiento\n",
    "    while True:\n",
    "        # 5.1 Capturar frame\n",
    "        ret, frame = cap.read()  # Leer frame de la cámara\n",
    "        if not ret:  # Si hay error en la captura\n",
    "            print(\"\\nError en captura de frame\")\n",
    "            break\n",
    "            \n",
    "        frame_counter += 1  # Incrementar contador de frames\n",
    "        \n",
    "        # 5.2 Procesar cada 2 frames (reducción de carga de procesamiento)\n",
    "        if frame_counter % 2 != 0:\n",
    "            continue  # Saltar frames impares\n",
    "        # Ejemplo: Analizar 15 fps en lugar de 30 fps\n",
    "\n",
    "        # 5.3 Detección de landmarks\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convertir a RGB\n",
    "        results = hands.process(rgb_frame)  # Procesar con MediaPipe\n",
    "        \n",
    "        if results.multi_hand_landmarks:  # Si detecta manos\n",
    "            # 5.4 Extraer coordenadas de landmarks\n",
    "            landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:  # Máximo 2 manos\n",
    "                for lm in hand.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z])  # Aplanar coordenadas\n",
    "            # Ejemplo de datos: [0.5, 0.3, 0.1, 0.7, 0.2, ...] (126 valores)\n",
    "            \n",
    "            # 5.5 Rellenar con ceros si solo hay una mano detectada\n",
    "            if len(landmarks) < total_landmarks:\n",
    "                landmarks += [0.0] * (total_landmarks - len(landmarks))\n",
    "            \n",
    "            sequence.append(landmarks)  # Agregar al buffer circular\n",
    "\n",
    "            # 5.6 Dibujar landmarks en el frame\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # 5.7 Realizar predicción cada 30 frames (~1 segundo)\n",
    "            if len(sequence) == sequence_length and frame_counter % 30 == 0:\n",
    "                try:\n",
    "                    # Preprocesamiento de datos\n",
    "                    seq_array = np.array(sequence)  # Convertir a numpy array\n",
    "                    seq_array = (seq_array - X_mean) / (X_std + 1e-7)  # Normalización\n",
    "                    \n",
    "                    # Reformatear para el modelo (1, 30, 126)\n",
    "                    input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "                    \n",
    "                    # 5.8 Ejecutar inferencia del modelo\n",
    "                    interpreter.set_tensor(input_details['index'], input_data)\n",
    "                    interpreter.invoke()  # Lanzar cálculo\n",
    "                    prediction = interpreter.get_tensor(output_details['index'])[0]\n",
    "                    \n",
    "                    # 5.9 Procesar resultados\n",
    "                    predicted_idx = np.argmax(prediction)  # Índice de mayor probabilidad\n",
    "                    confidence = np.max(prediction)  # Valor de confianza\n",
    "                    \n",
    "                    # 5.10 Actualizar UI solo si confianza > 75%\n",
    "                    if confidence > 0.75:\n",
    "                        current_gesture = gestures[predicted_idx]\n",
    "                        current_confidence = confidence\n",
    "                        print(f\"Detección: {current_gesture} ({confidence:.2%})\")\n",
    "                        \n",
    "                        # Dibujar texto en el frame\n",
    "                        cv2.putText(frame, f\"{current_gesture} ({confidence:.2%})\", \n",
    "                                   (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error en predicción: {str(e)}\")\n",
    "                    current_gesture = None\n",
    "\n",
    "        # 5.11 Mostrar frame con resultados\n",
    "        cv2.imshow(\"Predicciones\", cv2.resize(frame, (640, 480)))\n",
    "        \n",
    "        # 5.12 Salir con tecla ESC\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # 6. Liberar recursos\n",
    "    cap.release()  # Apagar cámara\n",
    "    cv2.destroyAllWindows()  # Cerrar ventanas\n",
    "    print(\"\\nSistema de evaluación detenido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. REENTRENAR GESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_gesture():\n",
    "    global gestures\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay gestos para reentrenar. Primero recolecte datos.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nGestos disponibles para reentrenar:\")\n",
    "    for i, gesture in enumerate(gestures):\n",
    "        print(f\"{i+1}. {gesture}\")\n",
    "\n",
    "    try:\n",
    "        choice = int(input(\"\\nSeleccione el número del gesto a reentrenar: \")) - 1\n",
    "        if 0 <= choice < len(gestures):\n",
    "            gesture = gestures[choice]\n",
    "            gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "            \n",
    "            for file in os.listdir(gesture_dir):\n",
    "                os.remove(os.path.join(gesture_dir, file))\n",
    "            \n",
    "            print(f\"\\nDatos anteriores de '{gesture}' eliminados.\")\n",
    "            collect_data()\n",
    "            train_model()\n",
    "        else:\n",
    "            print(\"\\nSelección inválida.\")\n",
    "    except ValueError:\n",
    "        print(\"\\nPor favor, ingrese un número válido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menú principal\n",
    "def main():\n",
    "    init_system()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Detectar Manos\")\n",
    "        print(\"2. Recolectar Datos\")\n",
    "        print(\"3. Entrenar Modelo, y despues ir a convertir a TFlite\")\n",
    "        print(\"4. Evaluar\")\n",
    "        print(\"5. Reentrenar Gesto\")\n",
    "        print(\"6. Convertir a TFLite\")  # Nueva opción\n",
    "        print(\"7. Salir\")\n",
    "        \n",
    "        choice = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            detect_hands()\n",
    "        elif choice == '2':\n",
    "            collect_data()\n",
    "        elif choice == '3':\n",
    "            train_model()\n",
    "        elif choice == '4':\n",
    "            evaluate()\n",
    "        elif choice == '5':\n",
    "            retrain_gesture()\n",
    "        elif choice == '6':  # Nueva opción de conversión\n",
    "            convert_to_tflite()\n",
    "        elif choice == '7':\n",
    "            print(\"\\n¡Hasta luego!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nOpción inválida. Por favor, intente de nuevo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Cargando datos y preparando el entrenamiento...\n",
      "\n",
      "Iniciando entrenamiento...\n",
      "Epoch 1/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 285ms/step - accuracy: 0.0164 - loss: 1.5384 - val_accuracy: 0.0000e+00 - val_loss: 1.4505\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0750 - loss: 1.4369 - val_accuracy: 0.0000e+00 - val_loss: 1.4151\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2250 - loss: 1.3468 - val_accuracy: 0.0000e+00 - val_loss: 1.3807\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4688 - loss: 1.2624 - val_accuracy: 0.1500 - val_loss: 1.3458\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.7711 - loss: 1.1734 - val_accuracy: 0.4500 - val_loss: 1.3100\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8422 - loss: 1.1180 - val_accuracy: 0.4500 - val_loss: 1.2723\n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8828 - loss: 1.0709 - val_accuracy: 0.5000 - val_loss: 1.2333\n",
      "Epoch 8/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9008 - loss: 1.0007 - val_accuracy: 0.5500 - val_loss: 1.1999\n",
      "Epoch 9/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8797 - loss: 0.9685 - val_accuracy: 0.5500 - val_loss: 1.1680\n",
      "Epoch 10/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8938 - loss: 0.9189 - val_accuracy: 0.5500 - val_loss: 1.1350\n",
      "Epoch 11/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9133 - loss: 0.8543 - val_accuracy: 0.5500 - val_loss: 1.1055\n",
      "Epoch 12/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9078 - loss: 0.8261 - val_accuracy: 0.5500 - val_loss: 1.0755\n",
      "Epoch 13/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9219 - loss: 0.7659 - val_accuracy: 0.6500 - val_loss: 1.0483\n",
      "Epoch 14/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9672 - loss: 0.6917 - val_accuracy: 0.6500 - val_loss: 1.0215\n",
      "Epoch 15/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9656 - loss: 0.6734 - val_accuracy: 0.7000 - val_loss: 0.9906\n",
      "Epoch 16/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9719 - loss: 0.6360 - val_accuracy: 0.7000 - val_loss: 0.9591\n",
      "Epoch 17/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9797 - loss: 0.6131 - val_accuracy: 0.8000 - val_loss: 0.9268\n",
      "Epoch 18/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9719 - loss: 0.5750 - val_accuracy: 0.8000 - val_loss: 0.8967\n",
      "Epoch 19/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.9641 - loss: 0.5576 - val_accuracy: 0.8000 - val_loss: 0.8661\n",
      "Epoch 20/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9758 - loss: 0.5050 - val_accuracy: 0.8000 - val_loss: 0.8388\n",
      "Epoch 21/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9641 - loss: 0.4874 - val_accuracy: 0.8500 - val_loss: 0.8079\n",
      "Epoch 22/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9820 - loss: 0.4482 - val_accuracy: 0.8500 - val_loss: 0.7780\n",
      "Epoch 23/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9820 - loss: 0.4321 - val_accuracy: 0.8500 - val_loss: 0.7482\n",
      "Epoch 24/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.3840 - val_accuracy: 0.8500 - val_loss: 0.7193\n",
      "Epoch 25/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.3615 - val_accuracy: 0.8500 - val_loss: 0.6899\n",
      "Epoch 26/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 1.0000 - loss: 0.3405 - val_accuracy: 0.8500 - val_loss: 0.6618\n",
      "Epoch 27/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.3182 - val_accuracy: 0.8500 - val_loss: 0.6336\n",
      "Epoch 28/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.3103 - val_accuracy: 0.8500 - val_loss: 0.6048\n",
      "Epoch 29/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.2668 - val_accuracy: 0.8500 - val_loss: 0.5787\n",
      "Epoch 30/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.2694 - val_accuracy: 0.8500 - val_loss: 0.5518\n",
      "Epoch 31/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.2508 - val_accuracy: 0.8500 - val_loss: 0.5271\n",
      "Epoch 32/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.2356 - val_accuracy: 0.8500 - val_loss: 0.5026\n",
      "Epoch 33/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.2190 - val_accuracy: 0.8500 - val_loss: 0.4792\n",
      "Epoch 34/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.2065 - val_accuracy: 0.8500 - val_loss: 0.4567\n",
      "Epoch 35/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.1950 - val_accuracy: 0.8500 - val_loss: 0.4364\n",
      "Epoch 36/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.1793 - val_accuracy: 0.8500 - val_loss: 0.4170\n",
      "Epoch 37/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.1725 - val_accuracy: 0.8500 - val_loss: 0.3979\n",
      "Epoch 38/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.1638 - val_accuracy: 0.8500 - val_loss: 0.3804\n",
      "Epoch 39/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.1564 - val_accuracy: 0.9000 - val_loss: 0.3641\n",
      "Epoch 40/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.1438 - val_accuracy: 0.9000 - val_loss: 0.3494\n",
      "Epoch 41/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 0.1408 - val_accuracy: 0.9000 - val_loss: 0.3364\n",
      "Epoch 42/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.1315 - val_accuracy: 0.9000 - val_loss: 0.3242\n",
      "Epoch 43/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.1256 - val_accuracy: 0.9000 - val_loss: 0.3131\n",
      "Epoch 44/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 1.0000 - loss: 0.1153 - val_accuracy: 0.9000 - val_loss: 0.3028\n",
      "Epoch 45/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.1152 - val_accuracy: 0.9000 - val_loss: 0.2936\n",
      "Epoch 46/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.1079 - val_accuracy: 0.9000 - val_loss: 0.2847\n",
      "Epoch 47/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0994 - val_accuracy: 0.9000 - val_loss: 0.2770\n",
      "Epoch 48/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.1001 - val_accuracy: 0.9000 - val_loss: 0.2696\n",
      "Epoch 49/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0960 - val_accuracy: 0.9000 - val_loss: 0.2626\n",
      "Epoch 50/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0883 - val_accuracy: 0.9000 - val_loss: 0.2564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo guardado en gesture_model_me_10.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp70f0nf6z\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp70f0nf6z\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp70f0nf6z'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 126), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1788561062048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788561198240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788561207392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788561204928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788561207040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788561207920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788561204576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "Modelo TFLite exportado exitosamente\n",
      "Precisión de validación final: 90.00%\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpwqb7gwii\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpwqb7gwii\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpwqb7gwii'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 126), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1788730741984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788730746912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788730752544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788730750080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788730749024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788730855440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788730854560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "✅ Conversión a TFLite exitosa!\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CONTRUIR\n",
      "La seña es:  CONTRUIR\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CONTRUIR\n",
      "La seña es:  CONTRUIR\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "\n",
      "Sistema de evaluación detenido\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Recolectando datos para el gesto 'CHAU'. Presiona 'ESC' para cancelar.\n",
      "Mantenga la seña frente a la cámara...\n",
      "Secuencias capturadas: 1/10\n",
      "Secuencias capturadas: 2/10\n",
      "Secuencias capturadas: 3/10\n",
      "Secuencias capturadas: 4/10\n",
      "Secuencias capturadas: 5/10\n",
      "Secuencias capturadas: 6/10\n",
      "Secuencias capturadas: 7/10\n",
      "Secuencias capturadas: 8/10\n",
      "Secuencias capturadas: 9/10\n",
      "Secuencias capturadas: 10/10\n",
      "\n",
      "Se recolectaron 10 secuencias para el gesto 'CHAU'\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Cargando datos y preparando el entrenamiento...\n",
      "\n",
      "Iniciando entrenamiento...\n",
      "Epoch 1/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 317ms/step - accuracy: 0.4368 - loss: 1.5166 - val_accuracy: 0.9091 - val_loss: 1.0274\n",
      "Epoch 2/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.5323 - loss: 1.4316 - val_accuracy: 0.9091 - val_loss: 1.0129\n",
      "Epoch 3/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6673 - loss: 1.3109 - val_accuracy: 0.9091 - val_loss: 0.9965\n",
      "Epoch 4/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7188 - loss: 1.2360 - val_accuracy: 0.9091 - val_loss: 0.9740\n",
      "Epoch 5/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7809 - loss: 1.1394 - val_accuracy: 0.9091 - val_loss: 0.9526\n",
      "Epoch 6/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7727 - loss: 1.1023 - val_accuracy: 0.9545 - val_loss: 0.9339\n",
      "Epoch 7/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8018 - loss: 1.0163 - val_accuracy: 0.9545 - val_loss: 0.9123\n",
      "Epoch 8/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8427 - loss: 0.9429 - val_accuracy: 0.9545 - val_loss: 0.8895\n",
      "Epoch 9/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8232 - loss: 0.8918 - val_accuracy: 0.9545 - val_loss: 0.8655\n",
      "Epoch 10/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8015 - loss: 0.8798 - val_accuracy: 0.9545 - val_loss: 0.8454\n",
      "Epoch 11/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8423 - loss: 0.8060 - val_accuracy: 0.9545 - val_loss: 0.8207\n",
      "Epoch 12/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8576 - loss: 0.7583 - val_accuracy: 0.9545 - val_loss: 0.7983\n",
      "Epoch 13/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8750 - loss: 0.7263 - val_accuracy: 0.9545 - val_loss: 0.7744\n",
      "Epoch 14/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8807 - loss: 0.6900 - val_accuracy: 0.9545 - val_loss: 0.7524\n",
      "Epoch 15/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8729 - loss: 0.6666 - val_accuracy: 0.9545 - val_loss: 0.7318\n",
      "Epoch 16/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8768 - loss: 0.6145 - val_accuracy: 0.9545 - val_loss: 0.7131\n",
      "Epoch 17/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8924 - loss: 0.5726 - val_accuracy: 0.9545 - val_loss: 0.6923\n",
      "Epoch 18/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9155 - loss: 0.5318 - val_accuracy: 0.9545 - val_loss: 0.6721\n",
      "Epoch 19/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9173 - loss: 0.5358 - val_accuracy: 1.0000 - val_loss: 0.6510\n",
      "Epoch 20/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9347 - loss: 0.4901 - val_accuracy: 1.0000 - val_loss: 0.6313\n",
      "Epoch 21/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9347 - loss: 0.4578 - val_accuracy: 1.0000 - val_loss: 0.6121\n",
      "Epoch 22/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9403 - loss: 0.4621 - val_accuracy: 1.0000 - val_loss: 0.5945\n",
      "Epoch 23/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9577 - loss: 0.4407 - val_accuracy: 1.0000 - val_loss: 0.5747\n",
      "Epoch 24/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9886 - loss: 0.3871 - val_accuracy: 1.0000 - val_loss: 0.5567\n",
      "Epoch 25/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9808 - loss: 0.3698 - val_accuracy: 1.0000 - val_loss: 0.5402\n",
      "Epoch 26/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.3563 - val_accuracy: 1.0000 - val_loss: 0.5252\n",
      "Epoch 27/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.3210 - val_accuracy: 1.0000 - val_loss: 0.5108\n",
      "Epoch 28/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.3224 - val_accuracy: 1.0000 - val_loss: 0.4968\n",
      "Epoch 29/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.2951 - val_accuracy: 1.0000 - val_loss: 0.4845\n",
      "Epoch 30/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.2779 - val_accuracy: 1.0000 - val_loss: 0.4697\n",
      "Epoch 31/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.2453 - val_accuracy: 1.0000 - val_loss: 0.4542\n",
      "Epoch 32/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.2555 - val_accuracy: 1.0000 - val_loss: 0.4425\n",
      "Epoch 33/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.2363 - val_accuracy: 1.0000 - val_loss: 0.4291\n",
      "Epoch 34/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.2172 - val_accuracy: 1.0000 - val_loss: 0.4156\n",
      "Epoch 35/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.2252 - val_accuracy: 1.0000 - val_loss: 0.4015\n",
      "Epoch 36/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.2073 - val_accuracy: 1.0000 - val_loss: 0.3871\n",
      "Epoch 37/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.1923 - val_accuracy: 1.0000 - val_loss: 0.3750\n",
      "Epoch 38/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1822 - val_accuracy: 1.0000 - val_loss: 0.3626\n",
      "Epoch 39/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.1672 - val_accuracy: 1.0000 - val_loss: 0.3517\n",
      "Epoch 40/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.1517 - val_accuracy: 1.0000 - val_loss: 0.3417\n",
      "Epoch 41/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.1570 - val_accuracy: 1.0000 - val_loss: 0.3310\n",
      "Epoch 42/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.1466 - val_accuracy: 1.0000 - val_loss: 0.3203\n",
      "Epoch 43/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1445 - val_accuracy: 1.0000 - val_loss: 0.3095\n",
      "Epoch 44/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.1318 - val_accuracy: 1.0000 - val_loss: 0.2995\n",
      "Epoch 45/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 0.1230 - val_accuracy: 1.0000 - val_loss: 0.2899\n",
      "Epoch 46/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.1210 - val_accuracy: 1.0000 - val_loss: 0.2806\n",
      "Epoch 47/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.1158 - val_accuracy: 1.0000 - val_loss: 0.2724\n",
      "Epoch 48/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.1114 - val_accuracy: 1.0000 - val_loss: 0.2641\n",
      "Epoch 49/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.1073 - val_accuracy: 1.0000 - val_loss: 0.2555\n",
      "Epoch 50/50\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.1024 - val_accuracy: 1.0000 - val_loss: 0.2463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo guardado en gesture_model_me_10.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpw3p3awpp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpw3p3awpp\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpw3p3awpp'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 126), dtype=tf.float32, name='keras_tensor_13')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1788731421472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788731420768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788731422704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788731487008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788731489120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788731410912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788731476272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "Modelo TFLite exportado exitosamente\n",
      "Precisión de validación final: 100.00%\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpd6vgeq5_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpd6vgeq5_\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpd6vgeq5_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 30, 126), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1788786754832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788786759760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788786769264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788786766976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788786762928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788786765744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1788786761872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "✅ Conversión a TFLite exitosa!\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: OK\n",
      "La seña es:  OK\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CONTRUIR\n",
      "La seña es:  CONTRUIR\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CONTRUIR\n",
      "La seña es:  CONTRUIR\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "Forma de entrada: (1, 30, 126)\n",
      "\n",
      "Sistema de evaluación detenido\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Opción inválida. Por favor, intente de nuevo.\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
