{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import mediapipe as mp \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_camara=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización de MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    max_num_hands=2,\n",
    "    model_complexity=0  # Complejidad mínima\n",
    ")\n",
    "\n",
    "# Variables globales\n",
    "data_dir = \"sign_language_data_JUPYTER_hands\"\n",
    "data_dir_video = \"sign_language_data_JUPYTER_videos\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(data_dir_video, exist_ok=True)\n",
    "\n",
    "sequence_length = 30  # Frames por secuencia\n",
    "n_hand_landmarks = 21 * 3\n",
    "total_landmarks = (n_hand_landmarks * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    \"\"\"Procesa un frame y retorna los resultados de pose y manos\"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    hands_results = hands.process(rgb_frame)\n",
    "    return hands_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(pose_results, hands_results):\n",
    "    \"\"\"Extrae y normaliza los landmarks de pose y manos.\"\"\"\n",
    "    landmarks = []\n",
    "\n",
    "    hand_landmarks_list = []\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks[:2]:  # Máximo 2 manos\n",
    "            hand_points = [[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]\n",
    "            hand_landmarks_list.extend(np.array(hand_points).flatten())\n",
    "\n",
    "    # Rellenar con ceros si no se detectan ambas manos\n",
    "    while len(hand_landmarks_list) < n_hand_landmarks * 2:\n",
    "        hand_landmarks_list.extend([0] * n_hand_landmarks)\n",
    "\n",
    "    landmarks.extend(hand_landmarks_list)\n",
    "    landmarks = np.array(landmarks)\n",
    "\n",
    "    # Normalización\n",
    "    if np.any(landmarks):\n",
    "        landmarks = (landmarks - np.mean(landmarks)) / np.std(landmarks)\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(data_dir_video, data_dir, sign_name, sequence_length):\n",
    "    \"\"\"Recolecta secuencias de movimiento para una seña específica y guarda el video de los landmarks\"\"\"\n",
    "    sign_dir = os.path.join(data_dir, sign_name)\n",
    "    sign_dir_video = os.path.join(data_dir_video, f\"{sign_name}\")\n",
    "    os.makedirs(sign_dir, exist_ok=True)\n",
    "    os.makedirs(sign_dir_video, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(num_camara)  # Cambiar índice si usas DroidCam u otra cámara\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir la cámara\")\n",
    "        return\n",
    "\n",
    "    total_sequences = int(input(\"Número de secuencias a recolectar (recomendado: 20-30): \"))\n",
    "\n",
    "    print(\"\\nInstrucciones:\")\n",
    "    print(f\"1. Cada secuencia grabará {sequence_length} frames de movimiento\")\n",
    "    print(\"2. Presiona ESPACIO para iniciar cada secuencia\")\n",
    "    print(\"3. Realiza el movimiento completo de la seña\")\n",
    "    print(\"4. La grabación se detendrá automáticamente\")\n",
    "    print(\"5. Presiona ESC para cancelar\")\n",
    "\n",
    "    sequence_count = 0\n",
    "    frame_count = 0\n",
    "    is_recording = False\n",
    "    current_sequence = []\n",
    "\n",
    "    while sequence_count < total_sequences:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        blank_frame = np.zeros_like(frame)\n",
    "        hands_results = process_frame(frame)\n",
    "\n",
    "        # Dibujar landmarks\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(blank_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Mostrar mensajes\n",
    "        if is_recording:\n",
    "            cv2.putText(frame, f\"Grabando secuencia {sequence_count + 1}...\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            landmarks = extract_landmarks(hands_results)\n",
    "            current_sequence.append(landmarks)\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count >= sequence_length:\n",
    "                # Guardar la secuencia\n",
    "                sequence_data = np.array(current_sequence)\n",
    "                np.save(os.path.join(sign_dir, f\"sequence_{sequence_count}.npy\"), sequence_data)\n",
    "                print(f\"Secuencia {sequence_count + 1}/{total_sequences} guardada\")\n",
    "                sequence_count += 1\n",
    "                frame_count = 0\n",
    "                is_recording = False\n",
    "                current_sequence = []\n",
    "\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)\n",
    "        cv2.imshow(\"Landmarks\", blank_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 32 and not is_recording:  # Espacio para iniciar grabación\n",
    "            is_recording = True\n",
    "            current_sequence = []\n",
    "            frame_count = 0\n",
    "        elif key == 27:  # ESC para salir\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_dir, sequence_length, total_landmarks, model_file):\n",
    "    \"\"\"Entrena el modelo utilizando CNN, LSTM y Transformers.\"\"\"\n",
    "    \n",
    "    if not os.listdir(data_dir):\n",
    "        print(\"No hay datos para entrenar\")\n",
    "        return\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "    print(\"Cargando secuencias...\")\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        samples = [f for f in os.listdir(class_dir) if f.startswith('sequence_')]\n",
    "        print(f\"Clase {class_name}: {len(samples)} secuencias\")\n",
    "\n",
    "        for sample_file in samples:\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sequence = np.load(sample_path)\n",
    "            X.append(sequence)\n",
    "            y.append(class_idx)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    # Normalización de los datos\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "    # Definir dimensiones del modelo\n",
    "    input_shape = (sequence_length, total_landmarks)\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Arquitectura del modelo\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))\n",
    "    x = tf.keras.layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Entrenamiento\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.save(model_file)\n",
    "    print(f\"\\nModelo guardado en {model_file}\")\n",
    "\n",
    "    # Visualización del entrenamiento\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Precisión de entrenamiento')\n",
    "    plt.plot(history.history['val_accuracy'], label='Precisión de validación')\n",
    "    plt.title('Precisión del Modelo')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "    plt.title('Pérdida del Modelo')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_spoken_word = None  # Variable global para almacenar la última palabra pronunciada\n",
    "\n",
    "def load_model(model_file):\n",
    "    \"\"\"Carga el modelo previamente entrenado.\"\"\"\n",
    "    if not os.path.exists(model_file):\n",
    "        print(\"No se encontró el modelo entrenado\")\n",
    "        return None\n",
    "    #Cargar el modelo entrenado\n",
    "    return tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(data_dir, sequence_length):\n",
    "    \"\"\"Carga y prepara los datos de prueba.\"\"\"\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "    # Crear conjunto de prueba\n",
    "    print(\"Cargando datos de prueba...\")\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        samples = [f for f in os.listdir(class_dir) if f.startswith('sequence_')]\n",
    "\n",
    "        # Seleccionar el 20% de las secuencias como prueba\n",
    "        test_samples = samples[:int(0.2 * len(samples))]\n",
    "        for sample_file in test_samples:\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sequence = np.load(sample_path)\n",
    "            X_test.append(sequence)\n",
    "            y_test.append(class_idx)\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    return X_test, y_test, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evalúa el modelo en los datos de prueba.\"\"\"\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(f\"\\nPérdida en prueba: {loss:.4f}\")\n",
    "    print(f\"Precisión en prueba: {accuracy:.2%}\")\n",
    "\n",
    "    # Mostrar métricas globales\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar([\"Pérdida\", \"Precisión\"], [loss, accuracy], color=['blue', 'green'])\n",
    "    plt.title(\"Métricas globales en prueba\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sequence(sequence):\n",
    "    \"\"\"Normaliza una secuencia de landmarks con manejo de errores.\"\"\"\n",
    "    sequence = np.array(sequence)\n",
    "    if sequence.size == 0:\n",
    "        return np.zeros_like(sequence)  # Evitar errores\n",
    "    \n",
    "    mean = np.nanmean(sequence, axis=0)  # Ignorar NaNs\n",
    "    std = np.nanstd(sequence, axis=0)\n",
    "    std[std == 0] = 1  # Evitar divisiones por cero\n",
    "    \n",
    "    return (sequence - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_realtime(model_file, data_dir, sequence_length, mp_draw, mp_pose, mp_hands, num_camara=0):\n",
    "    \"\"\"Realiza la evaluación en tiempo real usando una cámara.\"\"\"\n",
    "    model = load_model(model_file)\n",
    "    if model is None:\n",
    "        return\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "    cap = cv2.VideoCapture(num_camara)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir la cámara.\")\n",
    "        return\n",
    "\n",
    "    current_sequence = []\n",
    "    predictions_buffer = []\n",
    "\n",
    "    prediction_confidences = []\n",
    "    prediction_stabilities = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        pose_results, hands_results = process_frame(frame)\n",
    "\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            landmarks = extract_landmarks(pose_results, hands_results)\n",
    "            current_sequence.append(landmarks)\n",
    "\n",
    "            if len(current_sequence) > sequence_length:\n",
    "                current_sequence.pop(0)\n",
    "\n",
    "            if len(current_sequence) == sequence_length:\n",
    "                sequence_data = np.array([preprocess_sequence(current_sequence)])\n",
    "                prediction = model.predict(sequence_data, verbose=0)\n",
    "                predicted_class = class_names[np.argmax(prediction[0])]\n",
    "                confidence = np.max(prediction[0])\n",
    "\n",
    "                predictions_buffer.append(predicted_class)\n",
    "                if len(predictions_buffer) > 10:\n",
    "                    predictions_buffer.pop(0)\n",
    "\n",
    "                most_common = Counter(predictions_buffer).most_common(1)[0]\n",
    "                stable_prediction = most_common[0]\n",
    "                stability = most_common[1] / len(predictions_buffer)\n",
    "\n",
    "                if stability > 0.5 and confidence > 0.7:\n",
    "                    cv2.putText(frame, f\"Seña: {stable_prediction}\", (10, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"Confianza: {confidence:.2%}\", (10, 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"Evaluación\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if prediction_confidences:\n",
    "        avg_confidence = np.mean(prediction_confidences)\n",
    "        avg_stability = np.mean(prediction_stabilities)\n",
    "\n",
    "        print(f\"\\nPromedio de confianza: {avg_confidence:.2%}\")\n",
    "        print(f\"Promedio de estabilidad: {avg_stability:.2%}\")\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(prediction_confidences, label=\"Confianza\")\n",
    "        plt.title(\"Confianza de Predicciones en Tiempo Real\")\n",
    "        plt.xlabel(\"Frame\")\n",
    "        plt.ylabel(\"Confianza\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(prediction_stabilities, label=\"Estabilidad\")\n",
    "        plt.title(\"Estabilidad de Predicciones en Tiempo Real\")\n",
    "        plt.xlabel(\"Frame\")\n",
    "        plt.ylabel(\"Estabilidad\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #data_dir = \"data\"  # Directorio para los datos recolectados\n",
    "    data_dir = \"sign_language_data_JUPYTER\"\n",
    "    model_file = \"sign_language_model.h5\"\n",
    "    sequence_length = 90  # Longitud de la secuencia para cada muestra\n",
    "    num_camara = 0  # Índice de la cámara\n",
    "\n",
    "    # Crear el directorio si no existe\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Ver detección de pose y manos\")\n",
    "        print(\"2. Recolectar datos de señas\")\n",
    "        print(\"3. Entrenar modelo\")\n",
    "        print(\"4. Evaluar en tiempo real\")\n",
    "        print(\"5. Salir\")\n",
    "        \n",
    "        option = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if option == \"1\":\n",
    "            cap = cv2.VideoCapture(num_camara)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                pose_results, hands_results = process_frame(frame)\n",
    "                # Dibuja los landmarks si se detectan\n",
    "                if hands_results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                        mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                cv2.imshow(\"Detección de Pose y Manos\", frame)\n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == 27:  # ESC para salir\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        elif option == \"2\":\n",
    "            sign_name = input(\"Nombre de la seña a recolectar: \")\n",
    "            collect_data(data_dir_video, data_dir, sign_name, sequence_length)\n",
    "        \n",
    "        elif option == \"3\":\n",
    "            train_model(data_dir, sequence_length, total_landmarks, model_file)\n",
    "        \n",
    "        elif option == \"4\":\n",
    "            evaluate_realtime(model_file, data_dir, sequence_length, mp_draw, mp_hands, num_camara)\n",
    "        \n",
    "        elif option == \"5\":\n",
    "            print(\"¡Hasta luego!\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Opción no válida.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Ver detección de pose y manos\n",
      "2. Recolectar datos de señas\n",
      "3. Entrenar modelo\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "\n",
      "Instrucciones:\n",
      "1. Cada secuencia grabará 90 frames de movimiento\n",
      "2. Presiona ESPACIO para iniciar cada secuencia\n",
      "3. Realiza el movimiento completo de la seña\n",
      "4. La grabación se detendrá automáticamente\n",
      "5. Presiona ESC para cancelar\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "extract_landmarks() missing 1 required positional argument: 'hands_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[69], line 49\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     48\u001b[0m     sign_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNombre de la seña a recolectar: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msign_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     52\u001b[0m     train_model(data_dir, sequence_length, total_landmarks, model_file)\n",
      "Cell \u001b[1;32mIn[62], line 45\u001b[0m, in \u001b[0;36mcollect_data\u001b[1;34m(data_dir_video, data_dir, sign_name, sequence_length)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_recording:\n\u001b[0;32m     43\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrabando secuencia \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_count\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m30\u001b[39m),\n\u001b[0;32m     44\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.7\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m \u001b[43mextract_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhands_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     current_sequence\u001b[38;5;241m.\u001b[39mappend(landmarks)\n\u001b[0;32m     47\u001b[0m     frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: extract_landmarks() missing 1 required positional argument: 'hands_results'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
