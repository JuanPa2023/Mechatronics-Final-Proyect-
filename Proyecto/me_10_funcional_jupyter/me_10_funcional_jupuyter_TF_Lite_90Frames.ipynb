{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.saving.saving_api import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from collections import deque  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial global\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Optimizar MediaPipe\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.45,  # Reducir confianza\n",
    "    min_tracking_confidence=0.45,\n",
    "    model_complexity=0  # Menor complejidad\n",
    ")\n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "dataset_dir = \"dataset_11_90\"\n",
    "model_path = \"gesture_model_me_10_90.h5\"\n",
    "sequence_length = 90\n",
    "total_landmarks = 126\n",
    "gestures = []\n",
    "X_mean = None\n",
    "X_std = None\n",
    "\n",
    "num_camara = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FUNCIONES PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def init_system():\n",
    "    global gestures\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    gestures = get_existing_gestures()\n",
    "    \n",
    "def get_existing_gestures():\n",
    "    return [d for d in os.listdir(dataset_dir) \n",
    "           if os.path.isdir(os.path.join(dataset_dir, d))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DETECCION DE MANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hands():\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow(\"Detección de Manos\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    global gestures\n",
    "    gesture = input(\"\\nIngrese la palabra o letra para la cual desea recolectar datos: \").upper()\n",
    "    num_sequences = int(input(\"Ingrese el número de secuencias a capturar (recomendado: 50): \"))\n",
    "    \n",
    "    save_dir = os.path.join(dataset_dir, gesture)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nRecolectando datos para el gesto '{gesture}'. Presiona 'ESC' para cancelar.\")\n",
    "    print(\"Mantenga la seña frente a la cámara...\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    sequence = []\n",
    "    counter = 0\n",
    "\n",
    "    #NUEVO Configurar ventana de landmarks\n",
    "    landmark_window_name = \"Landmarks en Tiempo Real\"\n",
    "    cv2.namedWindow(landmark_window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(landmark_window_name, 640, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        #NUEVO Crear canvas para landmarks\n",
    "        landmark_canvas = np.zeros((480, 640, 3), dtype=np.uint8)  # Canvas negro 640x480\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            all_landmarks = []\n",
    "\n",
    "            #NUEVO Dibujar landmarks en el canvas\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Dibujar en el canvas negro\n",
    "                mp_draw.draw_landmarks(\n",
    "                    landmark_canvas,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                    mp_draw.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "                )\n",
    "            \n",
    "            # Extraer coordenadas para el dataset\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    all_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar si solo hay una mano\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                all_landmarks += [0.0] * 63\n",
    "            \n",
    "            sequence.append(all_landmarks)\n",
    "\n",
    "            # Dibujar en el frame original\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            np.save(os.path.join(save_dir, f\"secuencia_{counter}.npy\"), sequence)\n",
    "            counter += 1\n",
    "            sequence = []\n",
    "            print(f\"Secuencias capturadas: {counter}/{num_sequences}\")\n",
    "\n",
    "        #NUEVO Mostrar información en ambas ventanas\n",
    "        info_text = f\"Secuencias: {counter}/{num_sequences}\"\n",
    "        cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(landmark_canvas, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)\n",
    "        #NEUVO\n",
    "        cv2.imshow(landmark_window_name, landmark_canvas)\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or counter >= num_sequences:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    gestures = get_existing_gestures()\n",
    "    print(f\"\\nSe recolectaron {counter} secuencias para el gesto '{gesture}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for label_idx, gesture in enumerate(gestures):\n",
    "        gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "        sequences = [f for f in os.listdir(gesture_dir) if f.endswith('.npy')]\n",
    "        \n",
    "        for seq_file in sequences:\n",
    "            seq_path = os.path.join(gesture_dir, seq_file)\n",
    "            sequence = np.load(seq_path)\n",
    "            \n",
    "            if sequence.shape == (sequence_length, total_landmarks):\n",
    "                X.append(sequence)\n",
    "                y.append(label_idx)\n",
    "    \n",
    "    return np.array(X), np.array(y), gestures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global X_mean, X_std, gestures\n",
    "    \n",
    "    # 1. Verificar datos de entrenamiento\n",
    "    gestures = get_existing_gestures()\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay datos recolectados. Primero recolecte datos de gestos.\")\n",
    "        return\n",
    "\n",
    "    # 2. Cargar y preparar datos\n",
    "    print(\"\\nCargando datos y preparando el entrenamiento...\")\n",
    "    X, y, gestures = load_data()\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    # 3. Calcular parámetros de normalización\n",
    "    X_mean = np.mean(X, axis=(0, 1))\n",
    "    X_std = np.std(X, axis=(0, 1))\n",
    "    X = (X - X_mean) / X_std  # Normalización\n",
    "\n",
    "    # 4. Guardar parámetros de normalización\n",
    "    np.savez('normalization_params_90.npz', mean=X_mean, std=X_std)\n",
    "    \n",
    "    # 5. Arquitectura optimizada del modelo\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "    x = tf.keras.layers.LSTM(64, return_sequences=False)(x)\n",
    "    outputs = tf.keras.layers.Dense(len(gestures), activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # 6. Compilación y entrenamiento\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        #callbacks=[tf.keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 7. Guardar modelo y resultados\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nModelo guardado en {model_path}\")\n",
    "    \n",
    "    # 8. Conversión a TFLite con configuraciones especiales\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    \n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        with open('model_quantized_90.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(\"\\nModelo TFLite exportado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError en conversión TFLite: {str(e)}\")\n",
    "    \n",
    "    # Mostrar métricas finales\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Precisión de validación final: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite():\n",
    "    try:\n",
    "        # Cargar el modelo entrenado\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Configurar el conversor con parámetros especiales\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        \n",
    "        # Añadir estas 3 líneas clave para compatibilidad con LSTM\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        converter._experimental_lower_tensor_list_ops = False\n",
    "        converter.allow_custom_ops = True  # Permitir operaciones personalizadas\n",
    "        \n",
    "        # Realizar la conversión\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo cuantizado\n",
    "        with open('model_quantized_90.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "            \n",
    "        print(\"\\n✅ Conversión a TFLite exitosa!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error en conversión: {str(e)}\")\n",
    "        print(\"Posibles soluciones:\")\n",
    "        print(\"1. Verifique que el modelo .h5 existe\")\n",
    "        print(\"2. Actualice TensorFlow: pip install --upgrade tensorflow\")\n",
    "        print(\"3. Reinicie el runtime/kernel\")\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    # Generador de datos de ejemplo para calibración\n",
    "    for _ in range(100):\n",
    "        yield [np.random.randn(1, sequence_length, total_landmarks).astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "def evaluate():\n",
    "    if not os.path.exists(\"model_quantized_90.tflite\"):\n",
    "        print(\"\\n¡Primero debe entrenar y convertir el modelo!\")\n",
    "        return\n",
    "    \n",
    "    # 1. Cargar parámetros y modelo\n",
    "    try:\n",
    "        with np.load('normalization_params_90.npz') as data:\n",
    "            X_mean = data['mean']\n",
    "            X_std = data['std']\n",
    "            \n",
    "        interpreter = tf.lite.Interpreter(model_path=\"model_quantized_90.tflite\")\n",
    "        interpreter.allocate_tensors()\n",
    "        input_details = interpreter.get_input_details()[0]  # Mantener [0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError crítico: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 2. Configuración de cámara\n",
    "    cap = cv2.VideoCapture(0)  # Forzar backend DirectShow\n",
    "    #import time\n",
    "    #time.sleep(2)  # Permitir inicialización de cámara\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    #cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"\\n¡No se puede acceder a la cámara!\")\n",
    "        return\n",
    "\n",
    "    # 3. Variables de estado\n",
    "    current_gesture = None  # Variable para almacenar la última seña detectada\n",
    "    current_confidence = 0.0\n",
    "    sequence = deque(maxlen=sequence_length)  # Usar deque\n",
    "    #last_gesture = None\n",
    "    frame_counter = 0\n",
    "    #prediction_active = False\n",
    "\n",
    "    # 4. Bucle principal\n",
    "    while True:\n",
    "        # 4.1 Capturar frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"\\nError en captura de frame\")\n",
    "            break\n",
    "            \n",
    "        frame_counter += 1\n",
    "        \n",
    "        # 4.2 Procesamiento cada 2 frames\n",
    "        if frame_counter % 2 != 0:\n",
    "            continue\n",
    "            \n",
    "        # 4.3 Detección de landmarks\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            # 4.4 Extraer landmarks\n",
    "            landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar si solo hay una mano\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                landmarks += [0.0] * (total_landmarks - len(landmarks))\n",
    "                \n",
    "            # El deque ya maneja automáticamente el tamaño máximo gracias a maxlen\n",
    "            sequence.append(landmarks)  # Esto mantendrá solo los últimos 'sequence_length' elementos\n",
    "            \n",
    "            \n",
    "            # 4.5 Dibujar landmarks\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "\n",
    "                # Añadir verificación de dimensión de entrada\n",
    "            if input_details['shape'][1] != sequence_length:\n",
    "                print(\"\\nError: Dimensiones del modelo no coinciden con la configuración\")\n",
    "                return\n",
    "            \n",
    "            # 4.6 Realizar predicción cada 1 segundo (30 frames)\n",
    "            if len(sequence) == sequence_length and frame_counter % 15 == 0:\n",
    "                try:\n",
    "                    # Preprocesamiento CORREGIDO\n",
    "                    seq_array = np.array(sequence)\n",
    "                    seq_array = (seq_array - X_mean) / (X_std + 1e-7)  # +epsilon para seguridad\n",
    "                    \n",
    "                    # Asegurar 3 dimensiones: [batch_size=1, sequence_length, features]\n",
    "                    input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "                    \n",
    "                    # Debug: Verificar forma\n",
    "                    \n",
    "                    print(\"Forma de entrada:\", input_data.shape)  # Debe mostrar (1, 30, 126)\n",
    "\n",
    "                    # Inferencia\n",
    "                    interpreter.set_tensor(input_details['index'], input_data)\n",
    "                    interpreter.invoke()\n",
    "                    prediction = interpreter.get_tensor(output_details['index'])[0]\n",
    "                    \n",
    "                    # Procesar resultados\n",
    "                    predicted_idx = np.argmax(prediction)\n",
    "                    confidence = prediction[predicted_idx]\n",
    "                    \n",
    "                    if confidence > 0.65:\n",
    "                        current_gesture = gestures[predicted_idx]  # Actualizar solo si hay alta confianza\n",
    "                        current_confidence = confidence\n",
    "                        print(\"Nueva seña detectada:\", current_gesture)\n",
    "                        \n",
    "                        \n",
    "                        cv2.putText(frame, f\"{current_gesture} ({confidence:.2%})\", \n",
    "                            (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2) #el 0,255,0 es verde\n",
    "                        \n",
    "                        print(\"La seña es: \", current_gesture)\n",
    "\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error en predicción: {str(e)}\")\n",
    "                    current_gesture = None  # Resetear si hay error\n",
    "                    \n",
    "        if current_gesture:\n",
    "            cv2.putText(frame, \n",
    "              f\"{current_gesture} ({current_confidence:.2%})\", \n",
    "              (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "        # Mostrar frame reducido\n",
    "        cv2.imshow(\"Predicciones\", cv2.resize(frame, (640, 480)))\n",
    "        \n",
    "        \n",
    "        # 4.8 Salir con ESC\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # 5. Limpieza\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nSistema de evaluación detenido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. REENTRENAR GESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_gesture():\n",
    "    global gestures\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay gestos para reentrenar. Primero recolecte datos.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nGestos disponibles para reentrenar:\")\n",
    "    for i, gesture in enumerate(gestures):\n",
    "        print(f\"{i+1}. {gesture}\")\n",
    "\n",
    "    try:\n",
    "        choice = int(input(\"\\nSeleccione el número del gesto a reentrenar: \")) - 1\n",
    "        if 0 <= choice < len(gestures):\n",
    "            gesture = gestures[choice]\n",
    "            gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "            \n",
    "            for file in os.listdir(gesture_dir):\n",
    "                os.remove(os.path.join(gesture_dir, file))\n",
    "            \n",
    "            print(f\"\\nDatos anteriores de '{gesture}' eliminados.\")\n",
    "            collect_data()\n",
    "            train_model()\n",
    "        else:\n",
    "            print(\"\\nSelección inválida.\")\n",
    "    except ValueError:\n",
    "        print(\"\\nPor favor, ingrese un número válido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menú principal\n",
    "def main():\n",
    "    init_system()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Detectar Manos\")\n",
    "        print(\"2. Recolectar Datos\")\n",
    "        print(\"3. Entrenar Modelo, y despues ir a convertir a TFlite\")\n",
    "        print(\"4. Evaluar\")\n",
    "        print(\"5. Reentrenar Gesto\")\n",
    "        print(\"6. Convertir a TFLite\")  # Nueva opción\n",
    "        print(\"7. Salir\")\n",
    "        \n",
    "        choice = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            detect_hands()\n",
    "        elif choice == '2':\n",
    "            collect_data()\n",
    "        elif choice == '3':\n",
    "            train_model()\n",
    "        elif choice == '4':\n",
    "            evaluate()\n",
    "        elif choice == '5':\n",
    "            retrain_gesture()\n",
    "        elif choice == '6':  # Nueva opción de conversión\n",
    "            convert_to_tflite()\n",
    "        elif choice == '7':\n",
    "            print(\"\\n¡Hasta luego!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nOpción inválida. Por favor, intente de nuevo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Cargando datos y preparando el entrenamiento...\n",
      "\n",
      "Iniciando entrenamiento...\n",
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 210ms/step - accuracy: 0.0185 - loss: 2.0176 - val_accuracy: 0.3889 - val_loss: 1.5170\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.1830 - loss: 1.8154 - val_accuracy: 0.5556 - val_loss: 1.4378\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.4481 - loss: 1.6314 - val_accuracy: 0.6389 - val_loss: 1.3892\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.6332 - loss: 1.4889 - val_accuracy: 0.5833 - val_loss: 1.3826\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7837 - loss: 1.3048 - val_accuracy: 0.2222 - val_loss: 1.3992\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8472 - loss: 1.1829 - val_accuracy: 0.1389 - val_loss: 1.4260\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.8809 - loss: 1.0758 - val_accuracy: 0.1389 - val_loss: 1.4686\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8945 - loss: 0.9625 - val_accuracy: 0.1389 - val_loss: 1.5348\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8426 - loss: 0.9460 - val_accuracy: 0.1389 - val_loss: 1.6077\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.9197 - loss: 0.8440 - val_accuracy: 0.1389 - val_loss: 1.6799\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9544 - loss: 0.7289 - val_accuracy: 0.1667 - val_loss: 1.7495\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9486 - loss: 0.6955 - val_accuracy: 0.1667 - val_loss: 1.8195\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9569 - loss: 0.6143 - val_accuracy: 0.1667 - val_loss: 1.8953\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9771 - loss: 0.5612 - val_accuracy: 0.1667 - val_loss: 1.9665\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9624 - loss: 0.5321 - val_accuracy: 0.1667 - val_loss: 2.0434\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9754 - loss: 0.4890 - val_accuracy: 0.1667 - val_loss: 2.1194\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9708 - loss: 0.4486 - val_accuracy: 0.1667 - val_loss: 2.1927\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9725 - loss: 0.4270 - val_accuracy: 0.1667 - val_loss: 2.2501\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9790 - loss: 0.3709 - val_accuracy: 0.1667 - val_loss: 2.3047\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9552 - loss: 0.3706 - val_accuracy: 0.1667 - val_loss: 2.3525\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9682 - loss: 0.3333 - val_accuracy: 0.1667 - val_loss: 2.3978\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9848 - loss: 0.2874 - val_accuracy: 0.1667 - val_loss: 2.4389\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9887 - loss: 0.2581 - val_accuracy: 0.1667 - val_loss: 2.4727\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9753 - loss: 0.2525 - val_accuracy: 0.1667 - val_loss: 2.5048\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9874 - loss: 0.2145 - val_accuracy: 0.1667 - val_loss: 2.5374\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9844 - loss: 0.2018 - val_accuracy: 0.1667 - val_loss: 2.5651\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9761 - loss: 0.1927 - val_accuracy: 0.1667 - val_loss: 2.5861\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9831 - loss: 0.1788 - val_accuracy: 0.1667 - val_loss: 2.6042\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9605 - loss: 0.1916 - val_accuracy: 0.1667 - val_loss: 2.6254\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9910 - loss: 0.1466 - val_accuracy: 0.1667 - val_loss: 2.6538\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9868 - loss: 0.1465 - val_accuracy: 0.1667 - val_loss: 2.6805\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9868 - loss: 0.1332 - val_accuracy: 0.1667 - val_loss: 2.7047\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9920 - loss: 0.1194 - val_accuracy: 0.1667 - val_loss: 2.7218\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9920 - loss: 0.1196 - val_accuracy: 0.1667 - val_loss: 2.7358\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9946 - loss: 0.1044 - val_accuracy: 0.1667 - val_loss: 2.7544\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9920 - loss: 0.1048 - val_accuracy: 0.1667 - val_loss: 2.7722\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0874 - val_accuracy: 0.1667 - val_loss: 2.7934\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0957 - val_accuracy: 0.1667 - val_loss: 2.8120\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0833 - val_accuracy: 0.1667 - val_loss: 2.8323\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0787 - val_accuracy: 0.1667 - val_loss: 2.8496\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0765 - val_accuracy: 0.1667 - val_loss: 2.8678\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0807 - val_accuracy: 0.1667 - val_loss: 2.8859\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0721 - val_accuracy: 0.1667 - val_loss: 2.9046\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0722 - val_accuracy: 0.1667 - val_loss: 2.9247\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.0628 - val_accuracy: 0.1667 - val_loss: 2.9442\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0590 - val_accuracy: 0.1667 - val_loss: 2.9663\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0574 - val_accuracy: 0.1667 - val_loss: 2.9886\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0534 - val_accuracy: 0.1667 - val_loss: 3.0089\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0521 - val_accuracy: 0.1667 - val_loss: 3.0286\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.0449 - val_accuracy: 0.1667 - val_loss: 3.0481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo guardado en gesture_model_me_10_90.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp2us2097f\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp2us2097f\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp2us2097f'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 90, 126), dtype=tf.float32, name='keras_tensor_10')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2818254434752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818254428768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818254425600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818254436864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818102042944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818254424192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818112547904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "Modelo TFLite exportado exitosamente\n",
      "Precisión de validación final: 16.67%\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Recolectando datos para el gesto 'BUENOS DIAS'. Presiona 'ESC' para cancelar.\n",
      "Mantenga la seña frente a la cámara...\n",
      "Secuencias capturadas: 1/30\n",
      "Secuencias capturadas: 2/30\n",
      "Secuencias capturadas: 3/30\n",
      "Secuencias capturadas: 4/30\n",
      "Secuencias capturadas: 5/30\n",
      "Secuencias capturadas: 6/30\n",
      "Secuencias capturadas: 7/30\n",
      "Secuencias capturadas: 8/30\n",
      "Secuencias capturadas: 9/30\n",
      "Secuencias capturadas: 10/30\n",
      "Secuencias capturadas: 11/30\n",
      "Secuencias capturadas: 12/30\n",
      "Secuencias capturadas: 13/30\n",
      "Secuencias capturadas: 14/30\n",
      "Secuencias capturadas: 15/30\n",
      "Secuencias capturadas: 16/30\n",
      "Secuencias capturadas: 17/30\n",
      "Secuencias capturadas: 18/30\n",
      "Secuencias capturadas: 19/30\n",
      "Secuencias capturadas: 20/30\n",
      "Secuencias capturadas: 21/30\n",
      "Secuencias capturadas: 22/30\n",
      "Secuencias capturadas: 23/30\n",
      "Secuencias capturadas: 24/30\n",
      "Secuencias capturadas: 25/30\n",
      "Secuencias capturadas: 26/30\n",
      "Secuencias capturadas: 27/30\n",
      "Secuencias capturadas: 28/30\n",
      "Secuencias capturadas: 29/30\n",
      "Secuencias capturadas: 30/30\n",
      "\n",
      "Se recolectaron 30 secuencias para el gesto 'BUENOS DIAS'\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Cargando datos y preparando el entrenamiento...\n",
      "\n",
      "Iniciando entrenamiento...\n",
      "Epoch 1/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 129ms/step - accuracy: 0.2260 - loss: 1.9805 - val_accuracy: 0.2857 - val_loss: 1.7260\n",
      "Epoch 2/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.3455 - loss: 1.7715 - val_accuracy: 0.4524 - val_loss: 1.7111\n",
      "Epoch 3/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.3816 - loss: 1.6255 - val_accuracy: 0.2857 - val_loss: 1.7232\n",
      "Epoch 4/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4454 - loss: 1.4971 - val_accuracy: 0.2619 - val_loss: 1.7515\n",
      "Epoch 5/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.5491 - loss: 1.3559 - val_accuracy: 0.2619 - val_loss: 1.7787\n",
      "Epoch 6/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6468 - loss: 1.2559 - val_accuracy: 0.2619 - val_loss: 1.8008\n",
      "Epoch 7/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6984 - loss: 1.1623 - val_accuracy: 0.2381 - val_loss: 1.8328\n",
      "Epoch 8/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7996 - loss: 1.0759 - val_accuracy: 0.2619 - val_loss: 1.8529\n",
      "Epoch 9/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.8620 - loss: 1.0365 - val_accuracy: 0.2619 - val_loss: 1.8847\n",
      "Epoch 10/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9259 - loss: 0.9345 - val_accuracy: 0.2857 - val_loss: 1.9188\n",
      "Epoch 11/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9179 - loss: 0.8814 - val_accuracy: 0.2619 - val_loss: 1.9753\n",
      "Epoch 12/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9237 - loss: 0.8196 - val_accuracy: 0.2619 - val_loss: 2.0392\n",
      "Epoch 13/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9358 - loss: 0.7322 - val_accuracy: 0.2857 - val_loss: 2.0991\n",
      "Epoch 14/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9473 - loss: 0.7012 - val_accuracy: 0.2857 - val_loss: 2.1598\n",
      "Epoch 15/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9513 - loss: 0.6449 - val_accuracy: 0.2857 - val_loss: 2.2274\n",
      "Epoch 16/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9747 - loss: 0.5926 - val_accuracy: 0.2857 - val_loss: 2.2962\n",
      "Epoch 17/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9808 - loss: 0.5410 - val_accuracy: 0.2857 - val_loss: 2.3623\n",
      "Epoch 18/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9826 - loss: 0.4985 - val_accuracy: 0.2857 - val_loss: 2.4231\n",
      "Epoch 19/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9785 - loss: 0.4602 - val_accuracy: 0.2857 - val_loss: 2.4916\n",
      "Epoch 20/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9792 - loss: 0.4031 - val_accuracy: 0.2857 - val_loss: 2.5627\n",
      "Epoch 21/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9781 - loss: 0.3705 - val_accuracy: 0.2857 - val_loss: 2.6251\n",
      "Epoch 22/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9837 - loss: 0.3428 - val_accuracy: 0.2857 - val_loss: 2.6797\n",
      "Epoch 23/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9781 - loss: 0.3344 - val_accuracy: 0.2857 - val_loss: 2.7272\n",
      "Epoch 24/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9848 - loss: 0.2880 - val_accuracy: 0.2857 - val_loss: 2.7726\n",
      "Epoch 25/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9710 - loss: 0.2669 - val_accuracy: 0.2857 - val_loss: 2.8303\n",
      "Epoch 26/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9803 - loss: 0.2419 - val_accuracy: 0.2857 - val_loss: 2.8875\n",
      "Epoch 27/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9781 - loss: 0.2273 - val_accuracy: 0.2857 - val_loss: 2.9218\n",
      "Epoch 28/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9963 - loss: 0.2051 - val_accuracy: 0.2857 - val_loss: 2.9541\n",
      "Epoch 29/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9881 - loss: 0.1841 - val_accuracy: 0.2857 - val_loss: 2.9899\n",
      "Epoch 30/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.1646 - val_accuracy: 0.2857 - val_loss: 3.0202\n",
      "Epoch 31/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.1547 - val_accuracy: 0.2857 - val_loss: 3.0514\n",
      "Epoch 32/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.1382 - val_accuracy: 0.2857 - val_loss: 3.0810\n",
      "Epoch 33/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 0.1278 - val_accuracy: 0.2857 - val_loss: 3.1140\n",
      "Epoch 34/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.1224 - val_accuracy: 0.2857 - val_loss: 3.1421\n",
      "Epoch 35/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.1100 - val_accuracy: 0.2857 - val_loss: 3.1665\n",
      "Epoch 36/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 1.0000 - loss: 0.1039 - val_accuracy: 0.2857 - val_loss: 3.1910\n",
      "Epoch 37/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0955 - val_accuracy: 0.2857 - val_loss: 3.2180\n",
      "Epoch 38/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0889 - val_accuracy: 0.2857 - val_loss: 3.2494\n",
      "Epoch 39/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0822 - val_accuracy: 0.2857 - val_loss: 3.2749\n",
      "Epoch 40/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0787 - val_accuracy: 0.2857 - val_loss: 3.2987\n",
      "Epoch 41/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0729 - val_accuracy: 0.2857 - val_loss: 3.3277\n",
      "Epoch 42/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0678 - val_accuracy: 0.2857 - val_loss: 3.3550\n",
      "Epoch 43/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0681 - val_accuracy: 0.2857 - val_loss: 3.3819\n",
      "Epoch 44/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0599 - val_accuracy: 0.2857 - val_loss: 3.4084\n",
      "Epoch 45/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 0.0616 - val_accuracy: 0.2857 - val_loss: 3.4362\n",
      "Epoch 46/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0565 - val_accuracy: 0.2857 - val_loss: 3.4672\n",
      "Epoch 47/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.0539 - val_accuracy: 0.2857 - val_loss: 3.4966\n",
      "Epoch 48/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0525 - val_accuracy: 0.2857 - val_loss: 3.5231\n",
      "Epoch 49/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0483 - val_accuracy: 0.2857 - val_loss: 3.5546\n",
      "Epoch 50/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0483 - val_accuracy: 0.2857 - val_loss: 3.5837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo guardado en gesture_model_me_10_90.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmphkf20uid\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmphkf20uid\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\juanp\\AppData\\Local\\Temp\\tmphkf20uid'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 90, 126), dtype=tf.float32, name='keras_tensor_15')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2818349556208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818349548464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818338888816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818349556736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818349557088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818348702128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818348701424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "Modelo TFLite exportado exitosamente\n",
      "Precisión de validación final: 28.57%\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Sistema de evaluación detenido\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp76ogp3vr\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp76ogp3vr\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\juanp\\AppData\\Local\\Temp\\tmp76ogp3vr'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 90, 126), dtype=tf.float32, name='input_layer_3')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2818360106624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818360111552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818360112608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818360119488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818360121776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818360115264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818360119664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "✅ Conversión a TFLite exitosa!\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: BUENOS DIAS\n",
      "La seña es:  BUENOS DIAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: BUENOS DIAS\n",
      "La seña es:  BUENOS DIAS\n",
      "\n",
      "Sistema de evaluación detenido\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: 2\n",
      "La seña es:  2\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CHAU\n",
      "La seña es:  CHAU\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: HOLA\n",
      "La seña es:  HOLA\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: CONSTRUIR\n",
      "La seña es:  CONSTRUIR\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "Forma de entrada: (1, 90, 126)\n",
      "Nueva seña detectada: COMO ESTAS\n",
      "La seña es:  COMO ESTAS\n",
      "\n",
      "Sistema de evaluación detenido\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Cargando datos y preparando el entrenamiento...\n",
      "\n",
      "Iniciando entrenamiento...\n",
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - accuracy: 0.2025 - loss: 1.8696 - val_accuracy: 0.5000 - val_loss: 1.6317\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.2488 - loss: 1.7182 - val_accuracy: 0.1667 - val_loss: 1.6354\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4511 - loss: 1.5413 - val_accuracy: 0.0278 - val_loss: 1.6457\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5809 - loss: 1.3874 - val_accuracy: 0.0000e+00 - val_loss: 1.6583\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6508 - loss: 1.3209 - val_accuracy: 0.0278 - val_loss: 1.6658\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.6907 - loss: 1.2052 - val_accuracy: 0.0833 - val_loss: 1.6755\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8163 - loss: 1.0864 - val_accuracy: 0.1667 - val_loss: 1.6928\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9090 - loss: 1.0047 - val_accuracy: 0.1667 - val_loss: 1.7250\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8903 - loss: 0.9580 - val_accuracy: 0.1667 - val_loss: 1.7589\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9478 - loss: 0.8602 - val_accuracy: 0.1667 - val_loss: 1.7956\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9434 - loss: 0.8215 - val_accuracy: 0.1667 - val_loss: 1.8335\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9446 - loss: 0.7957 - val_accuracy: 0.1667 - val_loss: 1.8714\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9682 - loss: 0.7059 - val_accuracy: 0.1667 - val_loss: 1.9099\n",
      "Epoch 14/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9725 - loss: 0.6613 - val_accuracy: 0.1667 - val_loss: 1.9554\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9808 - loss: 0.6362 - val_accuracy: 0.1667 - val_loss: 1.9942\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9751 - loss: 0.5941 - val_accuracy: 0.1667 - val_loss: 2.0300\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9690 - loss: 0.5834 - val_accuracy: 0.1667 - val_loss: 2.0605\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9630 - loss: 0.5280 - val_accuracy: 0.1667 - val_loss: 2.1040\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9647 - loss: 0.5037 - val_accuracy: 0.1667 - val_loss: 2.1451\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9905 - loss: 0.4691 - val_accuracy: 0.1667 - val_loss: 2.1795\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9657 - loss: 0.4639 - val_accuracy: 0.1667 - val_loss: 2.2224\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9884 - loss: 0.4340 - val_accuracy: 0.1667 - val_loss: 2.2581\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9832 - loss: 0.3707 - val_accuracy: 0.1667 - val_loss: 2.3024\n",
      "Epoch 24/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9920 - loss: 0.3746 - val_accuracy: 0.1667 - val_loss: 2.3444\n",
      "Epoch 25/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9920 - loss: 0.3475 - val_accuracy: 0.1667 - val_loss: 2.3849\n",
      "Epoch 26/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9946 - loss: 0.3051 - val_accuracy: 0.1667 - val_loss: 2.4259\n",
      "Epoch 27/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9977 - loss: 0.2643 - val_accuracy: 0.1667 - val_loss: 2.4668\n",
      "Epoch 28/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9977 - loss: 0.2414 - val_accuracy: 0.1667 - val_loss: 2.5086\n",
      "Epoch 29/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9868 - loss: 0.2406 - val_accuracy: 0.1667 - val_loss: 2.5444\n",
      "Epoch 30/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9946 - loss: 0.2105 - val_accuracy: 0.1667 - val_loss: 2.5758\n",
      "Epoch 31/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9964 - loss: 0.1840 - val_accuracy: 0.1667 - val_loss: 2.6062\n",
      "Epoch 32/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9964 - loss: 0.1695 - val_accuracy: 0.1667 - val_loss: 2.6374\n",
      "Epoch 33/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9920 - loss: 0.1631 - val_accuracy: 0.1667 - val_loss: 2.6676\n",
      "Epoch 34/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9868 - loss: 0.1547 - val_accuracy: 0.1667 - val_loss: 2.6937\n",
      "Epoch 35/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9920 - loss: 0.1379 - val_accuracy: 0.1667 - val_loss: 2.7250\n",
      "Epoch 36/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.9920 - loss: 0.1272 - val_accuracy: 0.1667 - val_loss: 2.7561\n",
      "Epoch 37/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9977 - loss: 0.1088 - val_accuracy: 0.1667 - val_loss: 2.7877\n",
      "Epoch 38/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9946 - loss: 0.1069 - val_accuracy: 0.1667 - val_loss: 2.8184\n",
      "Epoch 39/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.1035 - val_accuracy: 0.1667 - val_loss: 2.8476\n",
      "Epoch 40/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0970 - val_accuracy: 0.1667 - val_loss: 2.8767\n",
      "Epoch 41/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0903 - val_accuracy: 0.1667 - val_loss: 2.9075\n",
      "Epoch 42/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0844 - val_accuracy: 0.1667 - val_loss: 2.9382\n",
      "Epoch 43/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0835 - val_accuracy: 0.1667 - val_loss: 2.9657\n",
      "Epoch 44/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 1.0000 - loss: 0.0838 - val_accuracy: 0.1667 - val_loss: 2.9940\n",
      "Epoch 45/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0793 - val_accuracy: 0.1667 - val_loss: 3.0208\n",
      "Epoch 46/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0685 - val_accuracy: 0.1667 - val_loss: 3.0460\n",
      "Epoch 47/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 0.0662 - val_accuracy: 0.1667 - val_loss: 3.0690\n",
      "Epoch 48/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0623 - val_accuracy: 0.1667 - val_loss: 3.0931\n",
      "Epoch 49/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.0637 - val_accuracy: 0.1667 - val_loss: 3.1162\n",
      "Epoch 50/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.0570 - val_accuracy: 0.1667 - val_loss: 3.1381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo guardado en gesture_model_me_10_90.h5\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpqtu21uvk\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpqtu21uvk\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\juanp\\AppData\\Local\\Temp\\tmpqtu21uvk'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 90, 126), dtype=tf.float32, name='keras_tensor_28')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2817739850848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818404809040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818404810096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818404845168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818404855904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818404858016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2818404856080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "\n",
      "Modelo TFLite exportado exitosamente\n",
      "Precisión de validación final: 16.67%\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
