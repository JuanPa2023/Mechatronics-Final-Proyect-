{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.saving.saving_api import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from collections import deque  \n",
    "\n",
    "#graficos\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import io\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#voz\n",
    "import pyttsx3\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOTOR TEXTO A VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el motor de texto-a-voz de pyttsx3\n",
    "tts_engine = pyttsx3.init()  # Crea una instancia del motor TTS\n",
    "\n",
    "# Crea un objeto de bloqueo para sincronización de hilos\n",
    "tts_lock = threading.Lock()  # Previene acceso concurrente al motor TTS\n",
    "\n",
    "# Variable global para almacenar la última seña vocalizada\n",
    "last_spoken_gesture = None  # Guarda el texto del último gesto reproducido\n",
    "\n",
    "def speak_text(text):\n",
    "    global last_spoken_gesture  # Accede a la variable global\n",
    "    \n",
    "    # Bloquea el acceso concurrente usando with para manejo seguro del recurso\n",
    "    with tts_lock:  # Asegura que solo un hilo use el motor TTS a la vez\n",
    "        \n",
    "        # Verifica si el texto es diferente al último reproducido\n",
    "        if text != last_spoken_gesture:  # Evita repeticiones consecutivas\n",
    "            \n",
    "            # Actualiza el registro del último gesto vocalizado\n",
    "            last_spoken_gesture = text  # Almacena el nuevo texto\n",
    "            \n",
    "            # Añade el texto a la cola de reproducción\n",
    "            tts_engine.say(text)  # Programa la reproducción del texto\n",
    "            \n",
    "            # Ejecuta la reproducción y espera a que termine\n",
    "            tts_engine.runAndWait()  # Bloquea hasta terminar la reproducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial global\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Optimizar MediaPipe\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.45,  # Reducir confianza\n",
    "    min_tracking_confidence=0.45,\n",
    "    model_complexity=0  # Menor complejidad\n",
    ")\n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "dataset_dir = \"dataset_11_90\"\n",
    "dataset_dir_video = \"dataset_11_90_videos\"\n",
    "model_path = \"gesture_model_me_10_90.h5\"\n",
    "sequence_length = 90\n",
    "total_landmarks = 126\n",
    "gestures = []\n",
    "X_mean = None\n",
    "X_std = None\n",
    "\n",
    "num_camara = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FUNCIONES PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def init_system():\n",
    "    global gestures\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    gestures = get_existing_gestures()\n",
    "    \n",
    "def get_existing_gestures():\n",
    "    return [d for d in os.listdir(dataset_dir) \n",
    "           if os.path.isdir(os.path.join(dataset_dir, d))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DETECCION DE MANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hands():\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow(\"Detección de Manos\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    global gestures\n",
    "    gesture = input(\"\\nIngrese la palabra o letra para la cual desea recolectar datos: \").upper()\n",
    "    num_sequences = int(input(\"Ingrese el número de secuencias a capturar (recomendado: 50): \"))\n",
    "    \n",
    "    save_dir = os.path.join(dataset_dir, gesture)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_dir_video = os.path.join(dataset_dir_video, gesture)\n",
    "    os.makedirs(save_dir_video, exist_ok=True)\n",
    "\n",
    "    # Parámetros para la creación del video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Códec para el video\n",
    "    fps = 20.0                               # Velocidad de fotogramas\n",
    "    frame_size = (640, 480)                  # Tamaño de cada fotograma (ancho, alto)\n",
    "\n",
    "    print(f\"\\nRecolectando datos para el gesto '{gesture}'. Presiona 'ESC' para cancelar.\")\n",
    "    print(\"Mantenga la seña frente a la cámara...\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    sequence = []    # Lista para almacenar los landmarks de la secuencia actual\n",
    "    counter = 0      # Contador de secuencias capturadas\n",
    "\n",
    "    # Inicializamos la variable para el VideoWriter (para el video de la secuencia actual)\n",
    "    video_writer = None\n",
    "\n",
    "    # Configurar ventana para visualizar los landmarks en tiempo real\n",
    "    landmark_window_name = \"Landmarks en Tiempo Real\"\n",
    "    cv2.namedWindow(landmark_window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(landmark_window_name, 640, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Crear un canvas negro para dibujar los landmarks\n",
    "        landmark_canvas = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Procesar la imagen para detectar manos (se asume que 'hands' está previamente configurado)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            all_landmarks = []\n",
    "            \n",
    "            # Dibujar landmarks en el canvas negro y extraer las coordenadas para el dataset\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(\n",
    "                    landmark_canvas,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                    mp_draw.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "                )\n",
    "                \n",
    "                # Extraer las coordenadas (x, y, z) de cada punto\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    all_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Si se detecta una sola mano, se rellena con ceros para que cada secuencia tenga la misma cantidad de datos\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                all_landmarks += [0.0] * 63  # 21 puntos x 3 coordenadas = 63 valores\n",
    "            \n",
    "            sequence.append(all_landmarks)\n",
    "            \n",
    "            # También se dibujan los landmarks sobre el frame original para la visualización\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Si se inicia una nueva secuencia, se crea un VideoWriter para grabar este bloque de video\n",
    "        if video_writer is None: #if len(sequence) == 0:\n",
    "            video_filename_seq = os.path.join(save_dir_video, f\"{gesture}_landmarks_seq_{counter}.avi\")\n",
    "            video_writer = cv2.VideoWriter(video_filename_seq, fourcc, fps, frame_size)\n",
    "        \n",
    "        # Escribir el canvas actual en el video de la secuencia\n",
    "        video_writer.write(landmark_canvas)\n",
    "        \n",
    "        # Mostrar información de cuántas secuencias se han capturado\n",
    "        info_text = f\"Secuencias: {counter}/{num_sequences}\"\n",
    "        cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(landmark_canvas, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Mostrar las ventanas\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)\n",
    "        cv2.imshow(landmark_window_name, landmark_canvas)\n",
    "        \n",
    "        # Cuando se alcanza la longitud deseada de la secuencia, se guarda el array y se cierra el video actual\n",
    "        if len(sequence) == sequence_length:\n",
    "            np.save(os.path.join(save_dir, f\"secuencia_{counter}.npy\"), sequence)\n",
    "            counter += 1\n",
    "            sequence = []\n",
    "            if video_writer is not None:\n",
    "                video_writer.release()\n",
    "                video_writer = None\n",
    "            print(f\"Secuencias capturadas: {counter}/{num_sequences}\")\n",
    "\n",
    "        # Salir si se presiona 'ESC' o se han capturado el número deseado de secuencias\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or counter >= num_sequences:\n",
    "            break\n",
    "\n",
    "    # Liberar recursos\n",
    "    cap.release()\n",
    "    if video_writer is not None:\n",
    "        video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    gestures = get_existing_gestures()\n",
    "    print(f\"\\nSe recolectaron {counter} secuencias para el gesto '{gesture}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for label_idx, gesture in enumerate(gestures):\n",
    "        gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "        sequences = [f for f in os.listdir(gesture_dir) if f.endswith('.npy')]\n",
    "        \n",
    "        for seq_file in sequences:\n",
    "            seq_path = os.path.join(gesture_dir, seq_file)\n",
    "            sequence = np.load(seq_path)\n",
    "            \n",
    "            if sequence.shape == (sequence_length, total_landmarks):\n",
    "                X.append(sequence)\n",
    "                y.append(label_idx)\n",
    "    \n",
    "    return np.array(X), np.array(y), gestures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global X_mean, X_std, gestures\n",
    "    \n",
    "    # 1. Verificar datos de entrenamiento\n",
    "    gestures = get_existing_gestures()\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay datos recolectados. Primero recolecte datos de gestos.\")\n",
    "        return\n",
    "\n",
    "    # 2. Cargar y preparar datos\n",
    "    print(\"\\nCargando datos y preparando el entrenamiento...\")\n",
    "    X, y, gestures = load_data()\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    # 3. Calcular parámetros de normalización\n",
    "    X_mean = np.mean(X, axis=(0, 1))\n",
    "    X_std = np.std(X, axis=(0, 1))\n",
    "    X = (X - X_mean) / X_std  # Normalización\n",
    "\n",
    "    # 4. Guardar parámetros de normalización\n",
    "    np.savez('normalization_params_90.npz', mean=X_mean, std=X_std)\n",
    "    \n",
    "    # 5. Arquitectura optimizada del modelo\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "    x = tf.keras.layers.LSTM(64, return_sequences=False)(x)\n",
    "    outputs = tf.keras.layers.Dense(len(gestures), activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # 6. Compilación y entrenamiento\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Calcular pesos para clases desbalanceadas\n",
    "    y_true = np.argmax(y, axis=1)\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_true), y=y_true)\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "     # Función para graficar métricas de entrenamiento\n",
    "    def generate_training_plots(history):\n",
    "        # Crear figura de 12x5 pulgadas (ancho x alto)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Primer subgráfico (1 fila, 2 columnas, posición 1)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        # Graficar precisión de entrenamiento (datos del historial)\n",
    "        plt.plot(history.history['accuracy'], label='Precisión Entrenamiento')\n",
    "        # Graficar precisión de validación (línea punteada)\n",
    "        plt.plot(history.history['val_accuracy'], '--', label='Precisión Validación')\n",
    "        plt.title('Evolución de la Precisión')\n",
    "        plt.ylabel('Precisión')  # Rango 0-1 (0% a 100%)\n",
    "        plt.xlabel('Época')  # Iteración de entrenamiento\n",
    "        plt.legend()  # Mostrar etiquetas\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)  # Cuadrícula suave\n",
    "\n",
    "        # Segundo subgráfico (posición 2)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Pérdida Entrenamiento')\n",
    "        plt.plot(history.history['val_loss'], '--', label='Pérdida Validación')\n",
    "        plt.title('Evolución de la Pérdida')\n",
    "        plt.ylabel('Pérdida')  # Valor de la función de costo\n",
    "        plt.xlabel('Época')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "        # Ajustar espacios entre subgráficos\n",
    "        plt.tight_layout()\n",
    "        # Guardar como imagen PNG (calidad vectorial)\n",
    "        plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()  # Mostrar en pantalla\n",
    "\n",
    "        # Función para matriz de confusión\n",
    "    def generate_confusion_matrix(model, X, y):\n",
    "        # Obtener etiquetas verdaderas (índices de mayor probabilidad)\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        # Predecir y obtener índices de clases\n",
    "        y_pred = np.argmax(model.predict(X), axis=1)\n",
    "        \n",
    "        # Calcular matriz de confusión (formato numérico)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "        # Crear figura para el heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        # Heatmap con anotaciones numéricas\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=gestures,  # Nombres clases en X\n",
    "                    yticklabels=gestures)  # Nombres clases en Y\n",
    "        plt.title('Matriz de Confusión', pad=20)\n",
    "        plt.xlabel('Predicciones del Modelo')\n",
    "        plt.ylabel('Etiquetas Reales')\n",
    "        # Rotar etiquetas X 45 grados para mejor legibilidad\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        # Ajustar márgenes\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        # Reporte detallado de métricas\n",
    "        print(\"\\nReporte de Clasificación:\")\n",
    "        print(classification_report(y_true, y_pred, \n",
    "                                target_names=gestures,\n",
    "                                digits=3))  # 3 decimales\n",
    "\n",
    "    # 7. Guardar modelo y resultados\n",
    "    model.save(model_path)\n",
    "    generate_training_plots(history)\n",
    "    generate_confusion_matrix(model, X, y)\n",
    "    print(f\"\\nModelo guardado en {model_path}\")\n",
    "    \n",
    "    # 8. Conversión a TFLite con configuraciones especiales\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    \n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        with open('model_quantized.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(\"\\nModelo TFLite exportado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError en conversión TFLite: {str(e)}\")\n",
    "\n",
    "    \n",
    "    # Mostrar métricas finales\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Precisión de validación final: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite():\n",
    "    try:\n",
    "        # Cargar el modelo entrenado\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Configurar el conversor con parámetros especiales\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        \n",
    "        # Añadir estas 3 líneas clave para compatibilidad con LSTM\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        converter._experimental_lower_tensor_list_ops = False\n",
    "        converter.allow_custom_ops = True  # Permitir operaciones personalizadas\n",
    "        \n",
    "        # Realizar la conversión\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo cuantizado\n",
    "        with open('model_quantized_90.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "            \n",
    "        print(\"\\n✅ Conversión a TFLite exitosa!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error en conversión: {str(e)}\")\n",
    "        print(\"Posibles soluciones:\")\n",
    "        print(\"1. Verifique que el modelo .h5 existe\")\n",
    "        print(\"2. Actualice TensorFlow: pip install --upgrade tensorflow\")\n",
    "        print(\"3. Reinicie el runtime/kernel\")\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    # Generador de datos de ejemplo para calibración\n",
    "    for _ in range(100):\n",
    "        yield [np.random.randn(1, sequence_length, total_landmarks).astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "def evaluate():\n",
    "    if not os.path.exists(\"model_quantized_90.tflite\"):\n",
    "        print(\"\\n¡Primero debe entrenar y convertir el modelo!\")\n",
    "        return\n",
    "    \n",
    "    # 1. Cargar parámetros y modelo\n",
    "    try:\n",
    "        with np.load('normalization_params_90.npz') as data:\n",
    "            X_mean = data['mean']\n",
    "            X_std = data['std']\n",
    "            \n",
    "        interpreter = tf.lite.Interpreter(model_path=\"model_quantized_90.tflite\")\n",
    "        interpreter.allocate_tensors()\n",
    "        input_details = interpreter.get_input_details()[0]  # Mantener [0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError crítico: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 2. Configuración de cámara\n",
    "    cap = cv2.VideoCapture(0)  # Forzar backend DirectShow\n",
    "    #import time\n",
    "    #time.sleep(2)  # Permitir inicialización de cámara\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    #cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"\\n¡No se puede acceder a la cámara!\")\n",
    "        return\n",
    "\n",
    "    # 3. Variables de estado\n",
    "    current_gesture = None  # Variable para almacenar la última seña detectada\n",
    "    current_confidence = 0.0\n",
    "    sequence = deque(maxlen=sequence_length)  # Usar deque\n",
    "    #last_gesture = None\n",
    "    frame_counter = 0\n",
    "    #prediction_active = False\n",
    "\n",
    "    # 4. Bucle principal\n",
    "    while True:\n",
    "        # 4.1 Capturar frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"\\nError en captura de frame\")\n",
    "            break\n",
    "            \n",
    "        frame_counter += 1\n",
    "        \n",
    "        # 4.2 Procesamiento cada 2 frames\n",
    "        if frame_counter % 2 != 0:\n",
    "            continue\n",
    "            \n",
    "        # 4.3 Detección de landmarks\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            # 4.4 Extraer landmarks\n",
    "            landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar si solo hay una mano\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                landmarks += [0.0] * (total_landmarks - len(landmarks))\n",
    "                \n",
    "            # El deque ya maneja automáticamente el tamaño máximo gracias a maxlen\n",
    "            sequence.append(landmarks)  # Esto mantendrá solo los últimos 'sequence_length' elementos\n",
    "            \n",
    "            \n",
    "            # 4.5 Dibujar landmarks\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "\n",
    "                # Añadir verificación de dimensión de entrada\n",
    "            if input_details['shape'][1] != sequence_length:\n",
    "                print(\"\\nError: Dimensiones del modelo no coinciden con la configuración\")\n",
    "                return\n",
    "            \n",
    "            # 4.6 Realizar predicción cada 1 segundo (30 frames)\n",
    "            if len(sequence) == sequence_length and frame_counter % 15 == 0:\n",
    "                try:\n",
    "                    # Preprocesamiento CORREGIDO\n",
    "                    seq_array = np.array(sequence)\n",
    "                    seq_array = (seq_array - X_mean) / (X_std + 1e-7)  # +epsilon para seguridad\n",
    "                    \n",
    "                    # Asegurar 3 dimensiones: [batch_size=1, sequence_length, features]\n",
    "                    input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "                    \n",
    "                    # Debug: Verificar forma\n",
    "                    \n",
    "                    print(\"Forma de entrada:\", input_data.shape)  # Debe mostrar (1, 30, 126)\n",
    "\n",
    "                    # Inferencia\n",
    "                    interpreter.set_tensor(input_details['index'], input_data)\n",
    "                    interpreter.invoke()\n",
    "                    prediction = interpreter.get_tensor(output_details['index'])[0]\n",
    "                    \n",
    "                    # Procesar resultados\n",
    "                    predicted_idx = np.argmax(prediction)\n",
    "                    confidence = prediction[predicted_idx]\n",
    "                    \n",
    "                    if confidence > 0.75:\n",
    "                        current_gesture = gestures[predicted_idx]  # Actualizar solo si hay alta confianza\n",
    "                        current_confidence = confidence\n",
    "                        print(\"Nueva seña detectada:\", current_gesture)\n",
    "                        \n",
    "                        \n",
    "                        cv2.putText(frame, f\"{current_gesture} ({confidence:.2%})\", \n",
    "                            (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2) #el 0,255,0 es verde\n",
    "                        \n",
    "                        print(\"La seña es: \", current_gesture)\n",
    "\n",
    "                        # Iniciar la reproducción en un hilo separado\n",
    "                        threading.Thread(target=speak_text, args=(current_gesture,)).start()\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error en predicción: {str(e)}\")\n",
    "                    current_gesture = None  # Resetear si hay error\n",
    "                    \n",
    "        if current_gesture:\n",
    "            cv2.putText(frame, \n",
    "              f\"{current_gesture} ({current_confidence:.2%})\", \n",
    "              (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)\n",
    "\n",
    "        # Mostrar frame reducido\n",
    "        cv2.imshow(\"Predicciones\", cv2.resize(frame, (640, 480)))\n",
    "        \n",
    "        \n",
    "        # 4.8 Salir con ESC\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # 5. Limpieza\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nSistema de evaluación detenido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAFICOS DE EVALUACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_plots():\n",
    "    # Verificar si el modelo existe\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"\\nPrimero debe entrenar el modelo.\")\n",
    "        return\n",
    "    \n",
    "    # Validar parámetros de normalización\n",
    "    if X_mean is None or X_std is None:\n",
    "        print(\"\\nERROR: Debe entrenar el modelo primero para obtener los parámetros de normalización\")\n",
    "        return\n",
    "\n",
    "    # --- CARGAR MODELO Y DATOS ---\n",
    "    model = load_model(model_path)  # Cargar modelo entrenado desde archivo .h5\n",
    "    X, y, _ = load_data()  # Obtener datos de entrenamiento/prueba\n",
    "    y = tf.keras.utils.to_categorical(y)  # Convertir etiquetas a one-hot encoding\n",
    "    \n",
    "    # Normalizar datos (igual que durante el entrenamiento)\n",
    "    X = (X - X_mean) / X_std\n",
    "    \n",
    "    # Generar predicciones del modelo\n",
    "    y_pred = model.predict(X)  # Obtener probabilidades para cada clase\n",
    "    y_classes = np.argmax(y_pred, axis=1)  # Convertir a clases (índice de mayor probabilidad)\n",
    "    y_true = np.argmax(y, axis=1)  # Convertir one-hot encoding a índices\n",
    "\n",
    "    # --- 1. MATRIZ DE CONFUSIÓN ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_classes)  # Calcular matriz numérica\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=gestures, yticklabels=gestures)  # Heatmap con etiquetas\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.xlabel('Predicciones')\n",
    "    plt.ylabel('Valores Verdaderos')\n",
    "    plt.xticks(rotation=45)  # Rotar etiquetas para mejor legibilidad\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 2. REPORTE DE CLASIFICACIÓN ---\n",
    "    print(\"\\nReporte de Clasificación:\")\n",
    "    # Imprimir métricas detalladas por clase\n",
    "    print(classification_report(y_true, y_classes, target_names=gestures))\n",
    "\n",
    "    # --- 3. CURVAS ROC POR CLASE ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(len(gestures)):\n",
    "        # Calcular curva ROC para cada clase\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred[:,i], pos_label=i)\n",
    "        roc_auc = auc(fpr, tpr)  # Calcular área bajo la curva\n",
    "        plt.plot(fpr, tpr, label=f'{gestures[i]} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Línea de referencia para aleatoriedad\n",
    "    plt.xlabel('Tasa de Falsos Positivos')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "    plt.title('Curvas ROC por Clase')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- 4. PRECISIÓN vs RECALL ---\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(len(gestures)):\n",
    "        # Calcular curva Precisión-Recall para cada clase\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_pred[:,i], pos_label=i)\n",
    "        plt.plot(recall, precision, label=gestures[i])\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.title('Curvas Precisión-Recall')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- 5. DISTRIBUCIÓN DE PROBABILIDADES ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(len(gestures)):\n",
    "        # Graficar distribución de probabilidades para predicciones correctas\n",
    "        sns.kdeplot(y_pred[y_true == i][:,i], label=gestures[i])\n",
    "    \n",
    "    plt.xlabel('Probabilidad Predicha')\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.title('Distribución de Probabilidades por Clase')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. REENTRENAR GESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_gesture():\n",
    "    global gestures\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay gestos para reentrenar. Primero recolecte datos.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nGestos disponibles para reentrenar:\")\n",
    "    for i, gesture in enumerate(gestures):\n",
    "        print(f\"{i+1}. {gesture}\")\n",
    "\n",
    "    try:\n",
    "        choice = int(input(\"\\nSeleccione el número del gesto a reentrenar: \")) - 1\n",
    "        if 0 <= choice < len(gestures):\n",
    "            gesture = gestures[choice]\n",
    "            gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "            \n",
    "            for file in os.listdir(gesture_dir):\n",
    "                os.remove(os.path.join(gesture_dir, file))\n",
    "            \n",
    "            print(f\"\\nDatos anteriores de '{gesture}' eliminados.\")\n",
    "            collect_data()\n",
    "            train_model()\n",
    "        else:\n",
    "            print(\"\\nSelección inválida.\")\n",
    "    except ValueError:\n",
    "        print(\"\\nPor favor, ingrese un número válido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menú principal\n",
    "def main():\n",
    "    init_system()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Detectar Manos\")\n",
    "        print(\"2. Recolectar Datos\")\n",
    "        print(\"3. Entrenar Modelo, y despues ir a convertir a TFlite\")\n",
    "        print(\"4. Convertir a TFLite\")  # Nueva opción\n",
    "        print(\"5. Evaluar\")\n",
    "        print(\"6. Evaluar con Graficos\")\n",
    "        print(\"7. Reentrenar Gesto\")\n",
    "        print(\"8. Salir\")\n",
    "        \n",
    "        choice = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            detect_hands()\n",
    "        elif choice == '2':\n",
    "            collect_data()\n",
    "        elif choice == '3':\n",
    "            train_model()\n",
    "        elif choice == '4':  # Nueva opción de conversión\n",
    "            convert_to_tflite()\n",
    "        elif choice == '5':\n",
    "            evaluate()\n",
    "        elif choice == '6':\n",
    "            evaluate_with_plots()\n",
    "        elif choice == '7':\n",
    "            retrain_gesture()\n",
    "        elif choice == '8':\n",
    "            print(\"\\n¡Hasta luego!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nOpción inválida. Por favor, intente de nuevo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Convertir a TFLite\n",
      "5. Evaluar\n",
      "6. Evaluar con Graficos\n",
      "7. Reentrenar Gesto\n",
      "8. Salir\n",
      "\n",
      "Recolectando datos para el gesto '4'. Presiona 'ESC' para cancelar.\n",
      "Mantenga la seña frente a la cámara...\n",
      "Secuencias capturadas: 1/3\n",
      "Secuencias capturadas: 2/3\n",
      "Secuencias capturadas: 3/3\n",
      "\n",
      "Se recolectaron 3 secuencias para el gesto '4'\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Convertir a TFLite\n",
      "5. Evaluar\n",
      "6. Evaluar con Graficos\n",
      "7. Reentrenar Gesto\n",
      "8. Salir\n",
      "\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
