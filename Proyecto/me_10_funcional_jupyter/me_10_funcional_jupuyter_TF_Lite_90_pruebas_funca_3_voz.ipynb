{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.saving.saving_api import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from collections import deque  \n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#voz\n",
    "import pyttsx3\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOTOR TEXTO A VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el motor de texto-a-voz de pyttsx3\n",
    "tts_engine = pyttsx3.init()  # Crea una instancia del motor TTS\n",
    "\n",
    "# Crea un objeto de bloqueo para sincronización de hilos\n",
    "tts_lock = threading.Lock()  # Previene acceso concurrente al motor TTS\n",
    "\n",
    "# Variable global para almacenar la última seña vocalizada\n",
    "last_spoken_gesture = None  # Guarda el texto del último gesto reproducido\n",
    "\n",
    "def speak_text(text):\n",
    "    global last_spoken_gesture  # Accede a la variable global\n",
    "    \n",
    "    # Bloquea el acceso concurrente usando with para manejo seguro del recurso\n",
    "    with tts_lock:  # Asegura que solo un hilo use el motor TTS a la vez\n",
    "        \n",
    "        # Verifica si el texto es diferente al último reproducido\n",
    "        if text != last_spoken_gesture:  # Evita repeticiones consecutivas\n",
    "            \n",
    "            # Actualiza el registro del último gesto vocalizado\n",
    "            last_spoken_gesture = text  # Almacena el nuevo texto\n",
    "            \n",
    "            # Añade el texto a la cola de reproducción\n",
    "            tts_engine.say(text)  # Programa la reproducción del texto\n",
    "            \n",
    "            # Ejecuta la reproducción y espera a que termine\n",
    "            tts_engine.runAndWait()  # Bloquea hasta terminar la reproducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial global\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Optimizar MediaPipe\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.45,  # Reducir confianza\n",
    "    min_tracking_confidence=0.45,\n",
    "    model_complexity=0  # Menor complejidad\n",
    ")\n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "#VARIABLES GLOBALES \n",
    "MODEL_PATH = \"model_quantized_90_pruebas_3.tflite\"\n",
    "NORMALIZATION_PARAMS_PATH = 'normalization_params_90_pruebas_3.npz'\n",
    "dataset_dir = \"dataset_11_90\"\n",
    "model_path = \"gesture_model_me_10_90_pruebas_3.h5\"\n",
    "num_camara = 0\n",
    "\n",
    "sequence_length = 90\n",
    "total_landmarks = 126\n",
    "gestures = []\n",
    "X_mean = None\n",
    "X_std = None\n",
    "\n",
    "num_camara = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FUNCIONES PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def init_system():\n",
    "    global gestures\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    gestures = get_existing_gestures()\n",
    "    \n",
    "def get_existing_gestures():\n",
    "    return sorted([d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DETECCION DE MANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hands():\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")\n",
    "    global num_camara\n",
    "    cap = cv2.VideoCapture(num_camara)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow(\"Detección de Manos\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    global gestures\n",
    "    global num_camara\n",
    "    gesture = input(\"\\nIngrese la palabra o letra para la cual desea recolectar datos: \").upper()\n",
    "    num_sequences = int(input(\"Ingrese el número de secuencias a capturar (recomendado: 50): \"))\n",
    "    \n",
    "    save_dir = os.path.join(dataset_dir, gesture)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nRecolectando datos para el gesto '{gesture}'. Presiona 'ESC' para cancelar.\")\n",
    "    print(\"Mantenga la seña frente a la cámara...\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(num_camara)\n",
    "    sequence = []\n",
    "    counter = 0\n",
    "\n",
    "    #NUEVO Configurar ventana de landmarks\n",
    "    landmark_window_name = \"Landmarks en Tiempo Real\"\n",
    "    cv2.namedWindow(landmark_window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(landmark_window_name, 640, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        #NUEVO Crear canvas para landmarks\n",
    "        landmark_canvas = np.zeros((480, 640, 3), dtype=np.uint8)  # Canvas negro 640x480\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            all_landmarks = []\n",
    "\n",
    "            #NUEVO Dibujar landmarks en el canvas\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Dibujar en el canvas negro\n",
    "                mp_draw.draw_landmarks(\n",
    "                    landmark_canvas,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                    mp_draw.DrawingSpec(color=(0, 0, 255), thickness=2)\n",
    "                )\n",
    "            \n",
    "            # Extraer coordenadas para el dataset\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    all_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar si solo hay una mano\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                all_landmarks += [0.0] * 63\n",
    "            \n",
    "            sequence.append(all_landmarks)\n",
    "\n",
    "            # Dibujar en el frame original\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            np.save(os.path.join(save_dir, f\"secuencia_{counter}.npy\"), sequence)\n",
    "            counter += 1\n",
    "            sequence = []\n",
    "            print(f\"Secuencias capturadas: {counter}/{num_sequences}\")\n",
    "\n",
    "        #NUEVO Mostrar información en ambas ventanas\n",
    "        info_text = f\"Secuencias: {counter}/{num_sequences}\"\n",
    "        cv2.putText(frame, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(landmark_canvas, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)\n",
    "        #NEUVO\n",
    "        cv2.imshow(landmark_window_name, landmark_canvas)\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or counter >= num_sequences:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    gestures = get_existing_gestures()\n",
    "    print(f\"\\nSe recolectaron {counter} secuencias para el gesto '{gesture}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_augmentation(sequence):\n",
    "    \"\"\"Aumentación 100% en TensorFlow\"\"\"\n",
    "    # 1. Ruido Gaussiano\n",
    "    noise = tf.random.normal(tf.shape(sequence), mean=0.0, stddev=0.01)\n",
    "\n",
    "    # Convertir explícitamente a float32\n",
    "    sequence = tf.cast(sequence, tf.float32)\n",
    "    # 1. Ruido Gaussiano\n",
    "    noise = tf.random.normal(tf.shape(sequence), mean=0.0, stddev=0.01)\n",
    "    sequence = tf.add(sequence, noise)\n",
    "    \n",
    "    # 2. Escalado aleatorio\n",
    "    scale_factor = tf.random.uniform([], 0.95, 1.05)\n",
    "    sequence = tf.multiply(sequence, scale_factor)\n",
    "    \n",
    "    # 3. Rotación 2D (versión TensorFlow)\n",
    "    angle = tf.random.uniform([], -15.0, 15.0)  # Grados\n",
    "    angle_rad = tf.math.divide(angle * math.pi, 180.0)\n",
    "    \n",
    "    # Crear matriz de rotación como tensor\n",
    "    rot_matrix = tf.stack([\n",
    "        [tf.cos(angle_rad), -tf.sin(angle_rad), 0.0],\n",
    "        [tf.sin(angle_rad), tf.cos(angle_rad), 0.0],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ])\n",
    "    \n",
    "    # Aplicar rotación a cada landmark\n",
    "    original_shape = tf.shape(sequence)\n",
    "    sequence = tf.reshape(sequence, [-1, 3])  # [secuencia_length*42, 3]\n",
    "    sequence = tf.matmul(sequence, rot_matrix)\n",
    "    sequence = tf.reshape(sequence, original_shape)\n",
    "    \n",
    "    # 4. Desplazamiento temporal (versión TensorFlow)\n",
    "    shift = tf.random.uniform([], -5, 5, dtype=tf.int32)\n",
    "    sequence = tf.cond(\n",
    "        tf.random.uniform([]) > 0.5,\n",
    "        lambda: tf.roll(sequence, shift=shift, axis=0),\n",
    "        lambda: sequence\n",
    "    )\n",
    "\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "# Modificar la función create_dataset\n",
    "def create_dataset(X_data, y_data, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (custom_augmentation(x), y),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "        dataset = dataset.shuffle(1000)\n",
    "    \n",
    "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(augment=True):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for label_idx, gesture in enumerate(gestures):\n",
    "        gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "        sequences = [f for f in os.listdir(gesture_dir) if f.endswith('.npy')]\n",
    "        print(f\"Gesto '{gesture}' - secuencias encontradas: {len(sequences)}\")\n",
    "        \n",
    "        for seq_file in sequences:\n",
    "            seq_path = os.path.join(gesture_dir, seq_file)\n",
    "            sequence = np.load(seq_path)\n",
    "            \n",
    "            if sequence.shape == (sequence_length, total_landmarks):\n",
    "                X.append(sequence)\n",
    "                y.append(label_idx)\n",
    "            else:\n",
    "                print(f\"Secuencia {seq_file} con forma {sequence.shape} ignorada.\")\n",
    "    \n",
    "    return np.array(X, dtype=np.float32), np.array(y), gestures  # Asegurar tipo float32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global X_mean, X_std, gestures\n",
    "    global NORMALIZATION_PARAMS_PATH, MODEL_PATH\n",
    "    \n",
    "    # 1. Verificar datos de entrenamiento\n",
    "    gestures = get_existing_gestures()\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay datos recolectados. Primero recolecte datos de gestos.\")\n",
    "        return\n",
    "\n",
    "    # 2. Cargar y preparar datos\n",
    "    print(\"\\nCargando datos y preparando el entrenamiento...\")\n",
    "    X, y, gestures = load_data(augment=False)  # Cargar sin aumentación inicial\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    # 3. Dividir datos antes de crear el Dataset\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    # 4. Calcular parámetros de normalización\n",
    "    #X_mean = np.mean(X_train, axis=(0, 1)).astype(np.float32)\n",
    "    #X_std = np.std(X_train, axis=(0, 1)).astype(np.float32)\n",
    "    X_mean = np.mean(X_train, axis=(0)).astype(np.float32)\n",
    "    X_std = np.std(X_train, axis=(0)).astype(np.float32)\n",
    "    X_train = (X_train - X_mean) / X_std\n",
    "    X_val = (X_val - X_mean) / X_std  # Aplicar misma normalización a validación\n",
    "\n",
    "    train_dataset = create_dataset(X_train, y_train, augment=True)\n",
    "    val_dataset = create_dataset(X_val, y_val, augment=False)\n",
    "    \n",
    "\n",
    "    # 4. Guardar parámetros de normalización\n",
    "    np.savez(NORMALIZATION_PARAMS_PATH, mean=X_mean, std=X_std)\n",
    "    \n",
    "    # 5. Arquitectura optimizada del modelo\n",
    "    # Modelo CNN + LSTM Bidireccional\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))\n",
    "\n",
    "    # Primera etapa CNN\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)  # Reduce secuencia a la mitad\n",
    "\n",
    "    # Primera LSTM (retorna secuencia completa)\n",
    "    x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n",
    "\n",
    "    # Segunda etapa CNN\n",
    "    x = tf.keras.layers.Conv1D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)  # Reduce nuevamente\n",
    "\n",
    "    # Segunda LSTM (retorna secuencia) + Tercera LSTM (solo último paso)\n",
    "    x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.LSTM(64, return_sequences=False)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(len(gestures), activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    \"\"\"Explicación:\n",
    "    CNN Inicial: Extrae patrones espaciales entre los landmarks en cada paso temporal.\n",
    "    LSTM Intermedia: Procesa las características extraídas a lo largo del tiempo.\n",
    "    CNN Profunda: Captura relaciones más complejas en las características temporales.\n",
    "    LSTM Finales: Modelan dependencias temporales jerárquicas.\n",
    "    Ventajas vs Opción LSTM Inicial:\n",
    "    Las CNN actúan como filtros espaciales antes de las LSTM, reduciendo ruido y mejorando la eficiencia.\n",
    "    Intercalar CNN entre LSTM permite capturar patrones locales en diferentes niveles de abstracción temporal.\n",
    "    Evita el problema de que las LSTM iniciales procesen datos crudos sin características relevantes.\n",
    "    Consideraciones:\n",
    "    Usar return_sequences=True en LSTM intermedias para mantener la secuencia.\n",
    "    Ajustar padding y MaxPooling para controlar la longitud de la secuencia.\n",
    "    Si la secuencia es muy corta, eliminar MaxPooling en etapas posteriores.\n",
    "    \n",
    "    probar con \n",
    "    # Nueva arquitectura:\n",
    "    inputs = Input(shape=(sequence_length, total_landmarks))\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
    "    x = Attention()(x)  # Capa de atención para features clave\n",
    "    x = Bidirectional(LSTM(64))(x)\n",
    "    outputs = Dense(len(gestures), activation='softmax')(x)\n",
    "    mejor \"\"\"\n",
    "\n",
    "    # 6. Compilación y entrenamiento\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,  # Usar dataset de validación explícito\n",
    "        epochs=50,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # 7. Guardar modelo y resultados\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nModelo guardado en {model_path}\")\n",
    "    \n",
    "    # 8. Conversión a TFLite con configuraciones especiales\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    \n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        with open(MODEL_PATH, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(\"\\nModelo TFLite exportado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError en conversión TFLite: {str(e)}\")\n",
    "    \n",
    "    # Mostrar métricas finales\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Precisión de validación final: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite():\n",
    "    global MODEL_PATH\n",
    "    try:\n",
    "        # Cargar el modelo entrenado\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Configurar el conversor con parámetros especiales\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        \n",
    "        # Añadir estas 3 líneas clave para compatibilidad con LSTM\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        converter._experimental_lower_tensor_list_ops = False\n",
    "        converter.allow_custom_ops = True  # Permitir operaciones personalizadas\n",
    "        \n",
    "        # Realizar la conversión\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo cuantizado\n",
    "        with open(MODEL_PATH, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "            \n",
    "        print(\"\\n✅ Conversión a TFLite exitosa!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error en conversión: {str(e)}\")\n",
    "        print(\"Posibles soluciones:\")\n",
    "        print(\"1. Verifique que el modelo .h5 existe\")\n",
    "        print(\"2. Actualice TensorFlow: pip install --upgrade tensorflow\")\n",
    "        print(\"3. Reinicie el runtime/kernel\")\n",
    "\n",
    "    global gestures\n",
    "    gestures = get_existing_gestures()\n",
    "    print(\"Gestos cargados para evaluación:\", gestures)\n",
    "\n",
    "    print(\"Salida del modelo:\", model.output_shape)\n",
    "\n",
    "\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    # Generador de datos de ejemplo para calibración\n",
    "    for _ in range(100):\n",
    "        yield [np.random.randn(1, sequence_length, total_landmarks).astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "def evaluate():\n",
    "    global MODEL_PATH, NORMALIZATION_PARAMS_PATH, num_camara\n",
    "    global gestures\n",
    "    gestures = get_existing_gestures()\n",
    "\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(\"\\n¡Primero debe entrenar y convertir el modelo!\")\n",
    "        return\n",
    "    \n",
    "    # 1. Cargar parámetros y modelo\n",
    "    try:\n",
    "        with np.load(NORMALIZATION_PARAMS_PATH) as data:\n",
    "            X_mean = data['mean']\n",
    "            X_std = data['std']\n",
    "            \n",
    "        interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)\n",
    "        interpreter.allocate_tensors()\n",
    "        input_details = interpreter.get_input_details()[0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "        print(\"Output details shape:\", output_details['shape'])\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError crítico: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    # 2. Configuración de cámara\n",
    "    cap = cv2.VideoCapture(num_camara)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "    #https://www.toolify.ai/es/ai-news-es/cmo-cambiar-la-resolucin-de-la-cmara-en-opencv-1149753 \n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"\\n¡No se puede acceder a la cámara!\")\n",
    "        return\n",
    "\n",
    "    # 3. Variables de estado mejoradas\n",
    "    sequence = deque(maxlen=sequence_length)\n",
    "    prediction_history = deque(maxlen=15)  # Suavizado de predicciones\n",
    "    current_gesture = \"Esperando...\"\n",
    "    current_confidence = 0.0\n",
    "\n",
    "    # 4. Bucle principal optimizado\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        #NUEVO Redimensionar el frame para la ventana (ejemplo: 640x360)\n",
    "        frame_resized = cv2.resize(frame, (640,390))\n",
    "\n",
    "\n",
    "        \n",
    "        # Siempre procesar landmarks (manos detectadas o no)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        landmarks = []\n",
    "        #NUEVO no esta en v2. Solo agregar landmarks si hay manos detectadas\n",
    "        #hand_detected = False  # Variable para saber si hay manos detectadas\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            #NUEVO no esta en v2. Solo agregar landmarks si hay manos detectadas\n",
    "            #hand_detected = True  # Se detectaron manos\n",
    "            # Extraer landmarks para ambas manos\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar con ceros si es necesario\n",
    "            if len(landmarks) < total_landmarks:\n",
    "                landmarks += [0.0] * (total_landmarks - len(landmarks))\n",
    "\n",
    "        #si se descomenta el if del hand_detect, comentar este else\n",
    "        else:\n",
    "            # Si no hay manos, usar ceros\n",
    "            landmarks = [0.0] * total_landmarks\n",
    "\n",
    "        sequence.append(landmarks)\n",
    "        #NUEVO no esta en v2. Solo agregar landmarks si hay manos detectadas\n",
    "        \"\"\"if hand_detected:\n",
    "            sequence.append(landmarks)\n",
    "            # Si se detectan manos nuevamente, borrar predicción anterior\n",
    "            if current_gesture != \"Esperando...\":\n",
    "                current_gesture = \"Esperando...\"\n",
    "                current_confidence = 0.0\n",
    "                prediction_history.clear()  # Vaciar historial de predicciones para evitar residuos\"\"\"\n",
    "        \n",
    "        # Realizar predicción cuando la secuencia esté completa\n",
    "        if len(sequence) == sequence_length:\n",
    "            try:\n",
    "                # Preprocesamiento y normalización\n",
    "                seq_array = np.array(sequence)\n",
    "                seq_array = (seq_array - X_mean) / (X_std + 1e-7)\n",
    "                input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "\n",
    "                \n",
    "                # Durante la evaluación, agregar testeo de forma\n",
    "                if input_data.shape != tuple(input_details['shape']):\n",
    "                    print(f\"Error: Forma esperada {input_details['shape']}, obtenida {input_data.shape}\")\n",
    "                    return\n",
    "                \n",
    "                # Inferencia\n",
    "                interpreter.set_tensor(input_details['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                prediction = interpreter.get_tensor(output_details['index'])[0]\n",
    "                \n",
    "                # Procesar resultados con suavizado\n",
    "                predicted_idx = np.argmax(prediction)\n",
    "                confidence = prediction[predicted_idx]\n",
    "                prediction_history.append((predicted_idx, confidence))\n",
    "                \n",
    "                # Calcular moda de las últimas predicciones\n",
    "                most_common = max(prediction_history, key=lambda x: list(prediction_history).count(x))\n",
    "                final_idx, final_confidence = most_common\n",
    "                \n",
    "                if final_confidence > 0.9:\n",
    "                #NUEVO Validación de índice dentro de rangos\n",
    "                #if 0 <= final_idx < len(gestures) and final_confidence > 0.9:\n",
    "                    current_gesture = gestures[final_idx]\n",
    "                    current_confidence = final_confidence\n",
    "\n",
    "                    # Iniciar la reproducción en un hilo separado\n",
    "                    threading.Thread(target=speak_text, args=(current_gesture,)).start()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error en predicción: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "        #NUEVO no esta en v2 Si no se detectan manos, mantener el último gesto válido\n",
    "        #if not hand_detected:\n",
    "        #    current_gesture = \"Esperando...\"  # O dejar el último gesto válido sin cambiarlo\n",
    "        \n",
    "        # Visualización mejorada\n",
    "        cv2.putText(frame_resized, f\"Prediccion: {current_gesture}\", (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 0), 2)\n",
    "        cv2.putText(frame_resized, f\"Confianza: {current_confidence:.2%}\", (10, 70),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "        \n",
    "        cv2.imshow(\"Predicciones en Tiempo Real\", frame_resized)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break \n",
    "    \n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menú principal\n",
    "def main():\n",
    "    init_system()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Detectar Manos\")\n",
    "        print(\"2. Recolectar Datos\")\n",
    "        print(\"3. Entrenar Modelo, y despues ir a convertir a TFlite\")\n",
    "        print(\"4. Evaluar\")\n",
    "        print(\"5. Graficos de evaluacion\")\n",
    "        print(\"6. Convertir a TFLite\")  # Nueva opción\n",
    "        print(\"7. Salir\")\n",
    "        \n",
    "        choice = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            detect_hands()\n",
    "        elif choice == '2':\n",
    "            collect_data()\n",
    "        elif choice == '3':\n",
    "            train_model()\n",
    "        elif choice == '4':\n",
    "            evaluate()\n",
    "        elif choice == '5':\n",
    "            \"evaluate_with_plots()\"\n",
    "        elif choice == '6':  # Nueva opción de conversión\n",
    "            convert_to_tflite()\n",
    "        elif choice == '7':\n",
    "            print(\"\\n¡Hasta luego!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nOpción inválida. Por favor, intente de nuevo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Graficos de evaluacion\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "Output details shape: [1 7]\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Graficos de evaluacion\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
