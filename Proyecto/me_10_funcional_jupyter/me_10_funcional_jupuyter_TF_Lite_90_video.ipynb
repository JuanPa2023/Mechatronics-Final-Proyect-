{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.saving.saving_api import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from collections import deque  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial global\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Optimizar MediaPipe\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.45,  # Reducir confianza\n",
    "    min_tracking_confidence=0.45,\n",
    "    model_complexity=0  # Menor complejidad\n",
    ")\n",
    "\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "dataset_dir = \"dataset_11_90\"\n",
    "data_dir_video  = \"dataset_11_90_video\"\n",
    "model_path = \"gesture_model_me_10_90.h5\"\n",
    "\n",
    "sequence_length = 90\n",
    "total_landmarks = 126\n",
    "gestures = []\n",
    "X_mean = None\n",
    "X_std = None\n",
    "\n",
    "num_camara = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FUNCIONES PRINCIPALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def init_system():\n",
    "    global gestures\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    gestures = get_existing_gestures()\n",
    "    \n",
    "def get_existing_gestures():\n",
    "    valid_gestures = []\n",
    "    for d in os.listdir(dataset_dir):\n",
    "        full_path = os.path.join(dataset_dir, d)\n",
    "        if os.path.isdir(full_path):\n",
    "            if any(f.endswith('.npy') for f in os.listdir(full_path)):\n",
    "                valid_gestures.append(d)\n",
    "            else:\n",
    "                print(f\"¡Directorio vacío: {d}!\")\n",
    "    return valid_gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares\n",
    "def process_frame(frame):\n",
    "    \"\"\"Procesa el frame para detectar manos con MediaPipe.\"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    return results\n",
    "\n",
    "def extract_landmarks(hands_results, max_hands=2, landmarks_per_hand=21):\n",
    "    \"\"\"Extrae los landmarks de las manos detectadas y rellena con ceros si no se detectan manos.\"\"\"\n",
    "    landmarks = []\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks[:max_hands]:  # Limitar al número máximo de manos\n",
    "            landmarks.extend([(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark])\n",
    "    \n",
    "    # Rellenar con ceros si no se detectan suficientes manos\n",
    "    expected_length = max_hands * landmarks_per_hand * 3  # 3 valores (x, y, z) por landmark\n",
    "    if len(landmarks) < expected_length:\n",
    "        landmarks.extend([(0.0, 0.0, 0.0)] * (expected_length - len(landmarks)))\n",
    "    \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DETECCION DE MANO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def detect_hands():\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        #results = hands.process(rgb_frame)\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        results = process_frame(frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow(\"Detección de Manos\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "def process_frame(frame):\n",
    "    \"\"\"Procesa el frame para detectar manos con MediaPipe.\"\"\"\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    return results\n",
    "\n",
    "def extract_landmarks(hands_results, max_hands=2, landmarks_per_hand=21):\n",
    "    \"\"\"Extrae los landmarks de las manos detectadas y rellena con ceros si no se detectan manos.\"\"\"\n",
    "    landmarks = []\n",
    "    if hands_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hands_results.multi_hand_landmarks[:max_hands]:  # Limitar al número máximo de manos\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])  # Añadir cada coordenada como elemento individual\n",
    "    \n",
    "    # Rellenar con ceros si no se detectan suficientes manos\n",
    "    expected_length = max_hands * landmarks_per_hand * 3  # 3 valores (x, y, z) por landmark\n",
    "    if len(landmarks) < expected_length:\n",
    "        landmarks.extend([0.0] * (expected_length - len(landmarks)))  # Rellenar con 0.0s\n",
    "    \n",
    "    return landmarks\n",
    "\n",
    "# Función para detectar manos en tiempo real\n",
    "def detect_hands():\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        results = process_frame(frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow(\"Detección de Manos\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # Presiona ESC para salir\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para recolectar datos de gestos\n",
    "def collect_data(data_dir_video, dataset_dir, sign_name, sequence_length):\n",
    "    \"\"\"Recolecta secuencias de movimiento para una seña específica y guarda el video de los landmarks.\"\"\"\n",
    "    sign_dir = os.path.join(dataset_dir, sign_name)\n",
    "    sign_dir_video = os.path.join(data_dir_video, f\"{sign_name}\")\n",
    "    os.makedirs(sign_dir, exist_ok=True)\n",
    "    os.makedirs(sign_dir_video, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Cambiar índice si usas DroidCam u otra cámara\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: No se pudo abrir la cámara\")\n",
    "        return\n",
    "\n",
    "    total_sequences = int(input(\"Número de secuencias a recolectar (recomendado: 20-30): \"))\n",
    "\n",
    "    print(\"\\nInstrucciones:\")\n",
    "    print(f\"1. Cada secuencia grabará {sequence_length} frames de movimiento\")\n",
    "    print(\"2. Presiona ESPACIO para iniciar cada secuencia\")\n",
    "    print(\"3. Realiza el movimiento completo de la seña\")\n",
    "    print(\"4. La grabación se detendrá automáticamente\")\n",
    "    print(\"5. Presiona ESC para cancelar\")\n",
    "\n",
    "    sequence_count = 0\n",
    "    frame_count = 0\n",
    "    is_recording = False\n",
    "    current_sequence = []\n",
    "\n",
    "    # Configuración para guardar el video\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = 30  # Puedes ajustar los FPS según sea necesario\n",
    "    video_writer = None\n",
    "\n",
    "    while sequence_count < total_sequences:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        blank_frame = np.zeros_like(frame)\n",
    "        hands_results = process_frame(frame)\n",
    "\n",
    "        # Dibujar landmarks de las manos\n",
    "        if hands_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hands_results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(blank_frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Mostrar mensajes\n",
    "        if is_recording:\n",
    "            cv2.putText(blank_frame, f\"Grabando secuencia {sequence_count + 1}...\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "            landmarks = extract_landmarks(hands_results)\n",
    "            current_sequence.append(landmarks)\n",
    "            frame_count += 1\n",
    "\n",
    "            if frame_count >= sequence_length:\n",
    "                # Guardar la secuencia de landmarks\n",
    "                sequence_data = np.array(current_sequence)\n",
    "                np.save(os.path.join(sign_dir, f\"sequence_{sequence_count}.npy\"), sequence_data)\n",
    "\n",
    "                # Guardar el video\n",
    "                video_writer.release()\n",
    "                print(f\"Secuencia {sequence_count + 1}/{total_sequences} guardada\")\n",
    "                sequence_count += 1\n",
    "                frame_count = 0\n",
    "                is_recording = False\n",
    "                current_sequence = []\n",
    "\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)  # Mostrar el video original\n",
    "        cv2.imshow(\"Landmarks\", blank_frame)  # Mostrar los landmarks\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 32 and not is_recording:  # Espacio para iniciar grabación\n",
    "            is_recording = True\n",
    "            current_sequence = []\n",
    "            frame_count = 0\n",
    "            # Inicializar el VideoWriter para guardar el video\n",
    "            video_path = os.path.join(sign_dir_video, f\"sequence_{sequence_count}.avi\")\n",
    "            video_writer = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n",
    "        elif key == 27:  # ESC para salir\n",
    "            break\n",
    "\n",
    "        # Escribir el frame en el video si está grabando\n",
    "        if is_recording:\n",
    "            video_writer.write(blank_frame)  # Guardar solo los landmarks\n",
    "\n",
    "    cap.release()\n",
    "    if video_writer is not None:\n",
    "        video_writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X, y = [], []\n",
    "    for label_idx, gesture in enumerate(gestures):\n",
    "        gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "        if not os.path.exists(gesture_dir):\n",
    "            print(f\"¡Error! Directorio no encontrado: {gesture}\")\n",
    "            continue\n",
    "            \n",
    "        sequences = [f for f in os.listdir(gesture_dir) if f.endswith('.npy')]\n",
    "        if not sequences:\n",
    "            print(f\"¡No hay archivos .npy en {gesture}!\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        for seq_file in sequences:\n",
    "            seq_path = os.path.join(gesture_dir, seq_file)\n",
    "            try:\n",
    "                sequence = np.load(seq_path)\n",
    "                if sequence.shape == (sequence_length, total_landmarks):\n",
    "                    X.append(sequence)\n",
    "                    y.append(label_idx)\n",
    "                else:\n",
    "                    print(f\"¡Secuencia inválida en {gesture}: {seq_file}!\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error cargando {seq_path}: {str(e)}\")\n",
    "    \n",
    "    if len(X) == 0 or len(y) == 0:\n",
    "        print(\"\\nERROR CRÍTICO: Dataset vacío o corrupto.\")\n",
    "        print(\"Posibles causas:\")\n",
    "        print(\"- Directorios de gestos vacíos\")\n",
    "        print(\"- Archivos .npy con formato incorrecto\")\n",
    "        print(\"- Secuencias con dimensiones diferentes a (sequence_length, total_landmarks)\")\n",
    "        return np.array([]), np.array([]), gestures\n",
    "    \n",
    "    return np.array(X), np.array(y), gestures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global X_mean, X_std, gestures\n",
    "    \n",
    "    # 1. Verificar datos de entrenamiento\n",
    "    #gestures = get_existing_gestures()\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay datos recolectados. Primero recolecte datos de gestos.\")\n",
    "        return\n",
    "\n",
    "    # 2. Cargar y preparar datos\n",
    "    print(\"\\nCargando datos y preparando el entrenamiento...\")\n",
    "    X, y, gestures = load_data()\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    # 3. Calcular parámetros de normalización\n",
    "    X_mean = np.mean(X, axis=(0, 1))\n",
    "    X_std = np.std(X, axis=(0, 1))\n",
    "    X = (X - X_mean) / X_std  # Normalización\n",
    "\n",
    "    # 4. Guardar parámetros de normalización\n",
    "    np.savez('normalization_params_90.npz', mean=X_mean, std=X_std)\n",
    "    \n",
    "    # 5. Arquitectura optimizada del modelo\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))\n",
    "    x = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "    x = tf.keras.layers.LSTM(64, return_sequences=False)(x)\n",
    "    outputs = tf.keras.layers.Dense(len(gestures), activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # 6. Compilación y entrenamiento\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 7. Guardar modelo y resultados\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nModelo guardado en {model_path}\")\n",
    "    \n",
    "    # 8. Conversión a TFLite con configuraciones especiales\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.target_spec.supported_ops = [\n",
    "        tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "        tf.lite.OpsSet.SELECT_TF_OPS\n",
    "    ]\n",
    "    converter._experimental_lower_tensor_list_ops = False\n",
    "    \n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        with open('model_quantized_90.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        print(\"\\nModelo TFLite exportado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError en conversión TFLite: {str(e)}\")\n",
    "    \n",
    "    # Mostrar métricas finales\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Precisión de validación final: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite():\n",
    "    try:\n",
    "        # Cargar el modelo entrenado\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Configurar el conversor con parámetros especiales\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        \n",
    "        # Añadir estas 3 líneas clave para compatibilidad con LSTM\n",
    "        converter.target_spec.supported_ops = [\n",
    "            tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "            tf.lite.OpsSet.SELECT_TF_OPS\n",
    "        ]\n",
    "        converter._experimental_lower_tensor_list_ops = False\n",
    "        converter.allow_custom_ops = True  # Permitir operaciones personalizadas\n",
    "        \n",
    "        # Realizar la conversión\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo cuantizado\n",
    "        with open('model_quantized.tflite', 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "            \n",
    "        print(\"\\n✅ Conversión a TFLite exitosa!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error en conversión: {str(e)}\")\n",
    "        print(\"Posibles soluciones:\")\n",
    "        print(\"1. Verifique que el modelo .h5 existe\")\n",
    "        print(\"2. Actualice TensorFlow: pip install --upgrade tensorflow\")\n",
    "        print(\"3. Reinicie el runtime/kernel\")\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    # Generador de datos de ejemplo para calibración\n",
    "    for _ in range(100):\n",
    "        yield [np.random.randn(1, sequence_length, total_landmarks).astype(np.float32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ IMPORTS ADICIONALES NECESARIOS ------------------\n",
    "from collections import deque\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "# ------------------ SECCIÓN DE CONFIGURACIÓN ------------------\n",
    "# Añadir al inicio con las demás configuraciones\n",
    "frame_queue = Queue(maxsize=30)\n",
    "landmark_queue = Queue(maxsize=20)\n",
    "prediction_queue = Queue(maxsize=10)\n",
    "\n",
    "# ------------------ FUNCIÓN CAMERA_CAPTURE ------------------\n",
    "def camera_capture(cap):\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            resized_frame = cv2.resize(frame, (320, 240))\n",
    "            frame_queue.put(resized_frame)\n",
    "\n",
    "# ------------------ FUNCIÓN LANDMARK_PROCESSING ------------------\n",
    "def landmark_processing():\n",
    "    while True:\n",
    "        frame = frame_queue.get()\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        all_landmarks = np.zeros(total_landmarks, dtype=np.float32)\n",
    "        if results.multi_hand_landmarks:\n",
    "            idx = 0\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    all_landmarks[idx:idx+3] = [lm.x, lm.y, lm.z]\n",
    "                    idx += 3\n",
    "        landmark_queue.put(all_landmarks)\n",
    "\n",
    "# ------------------ FUNCIÓN MODEL_INFERENCE ------------------\n",
    "def model_inference(interpreter, input_details, output_details):\n",
    "    while True:\n",
    "        data = landmark_queue.get()\n",
    "        if data is not None:\n",
    "            sequence.append(data)\n",
    "            sequence = sequence[-sequence_length:]\n",
    "            \n",
    "            if len(sequence) == sequence_length:\n",
    "                seq_array = (np.array(sequence) - X_mean) / X_std\n",
    "                input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "                \n",
    "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                interpreter.invoke()\n",
    "                prediction = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "                \n",
    "                predicted_class = np.argmax(prediction)\n",
    "                confidence = np.max(prediction)\n",
    "                prediction_queue.put((gestures[predicted_class], confidence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "def evaluate():\n",
    "    if not os.path.exists(\"model_quantized_90.tflite\"):\n",
    "        print(\"\\n¡Primero debe entrenar y convertir el modelo!\")\n",
    "        return\n",
    "    \n",
    "    # 1. Cargar parámetros y modelo\n",
    "    try:\n",
    "        with np.load('normalization_params_90.npz') as data:\n",
    "            X_mean = data['mean']\n",
    "            X_std = data['std']\n",
    "            \n",
    "        interpreter = tf.lite.Interpreter(model_path=\"model_quantized_90.tflite\")\n",
    "        interpreter.allocate_tensors()\n",
    "        input_details = interpreter.get_input_details()[0]  # Mantener [0]\n",
    "        output_details = interpreter.get_output_details()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError crítico: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # 2. Configuración de cámara\n",
    "    cap = cv2.VideoCapture(0)  # Forzar backend DirectShow\n",
    "    #import time\n",
    "    #time.sleep(2)  # Permitir inicialización de cámara\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    #cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"\\n¡No se puede acceder a la cámara!\")\n",
    "        return\n",
    "\n",
    "    # 3. Variables de estado\n",
    "    current_gesture = None  # Variable para almacenar la última seña detectada\n",
    "    current_confidence = 0.0\n",
    "    sequence = deque(maxlen=sequence_length)  # Usar deque\n",
    "    #last_gesture = None\n",
    "    frame_counter = 0\n",
    "    #prediction_active = False\n",
    "\n",
    "    # 4. Bucle principal\n",
    "    while True:\n",
    "        # 4.1 Capturar frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"\\nError en captura de frame\")\n",
    "            break\n",
    "            \n",
    "        frame_counter += 1\n",
    "        \n",
    "        # 4.2 Procesamiento cada 2 frames\n",
    "        if frame_counter % 2 != 0:\n",
    "            continue\n",
    "            \n",
    "        # 4.3 Detección de landmarks\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            # 4.4 Extraer landmarks\n",
    "            landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Rellenar si solo hay una mano\n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                landmarks += [0.0] * (total_landmarks - len(landmarks))\n",
    "                \n",
    "            # El deque ya maneja automáticamente el tamaño máximo gracias a maxlen\n",
    "            sequence.append(landmarks)  # Esto mantendrá solo los últimos 'sequence_length' elementos\n",
    "            \n",
    "            \n",
    "            # 4.5 Dibujar landmarks\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "\n",
    "                # Añadir verificación de dimensión de entrada\n",
    "            if input_details['shape'][1] != sequence_length:\n",
    "                print(\"\\nError: Dimensiones del modelo no coinciden con la configuración\")\n",
    "                return\n",
    "            \n",
    "            # 4.6 Realizar predicción cada 1 segundo (90 frames)\n",
    "            if len(sequence) == sequence_length and frame_counter % 90 == 0:\n",
    "                try:\n",
    "                    # Preprocesamiento CORREGIDO\n",
    "                    seq_array = np.array(sequence)\n",
    "                    seq_array = (seq_array - X_mean) / (X_std + 1e-7)  # +epsilon para seguridad\n",
    "                    \n",
    "                    # Asegurar 3 dimensiones: [batch_size=1, sequence_length, features]\n",
    "                    input_data = seq_array.reshape(1, sequence_length, total_landmarks).astype(np.float32)\n",
    "                    \n",
    "                    # Debug: Verificar forma\n",
    "                    \n",
    "                    print(\"Forma de entrada:\", input_data.shape)  # Debe mostrar (1, 30, 126)\n",
    "\n",
    "                    # Inferencia\n",
    "                    interpreter.set_tensor(input_details['index'], input_data)\n",
    "                    interpreter.invoke()\n",
    "                    prediction = interpreter.get_tensor(output_details['index'])[0]\n",
    "                    \n",
    "                    # Procesar resultados\n",
    "                    predicted_idx = np.argmax(prediction)\n",
    "                    confidence = prediction[predicted_idx]\n",
    "                    \n",
    "                    if confidence > 0.35:\n",
    "                        current_gesture = gestures[predicted_idx]  # Actualizar solo si hay alta confianza\n",
    "                        current_confidence = confidence\n",
    "                        print(\"Nueva seña detectada:\", current_gesture)\n",
    "                        \n",
    "                        # Actualizar UI\n",
    "                        #cv2.putText(frame, f\"{current_gesture} ({confidence:.2%})\", \n",
    "                        #          (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                        \n",
    "                        cv2.putText(frame, f\"{current_gesture} ({confidence:.2%})\", \n",
    "                            (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "                        \n",
    "                        print(\"La seña es: \", current_gesture)\n",
    "\n",
    "                        \"\"\"# Reproducir voz\n",
    "                        if current_gesture != last_gesture:\n",
    "                            threading.Thread(target=voice_system.speak, \n",
    "                                           args=(current_gesture,)).start()\n",
    "                            last_gesture = current_gesture\"\"\"\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error en predicción: {str(e)}\")\n",
    "                    current_gesture = None  # Resetear si hay error\n",
    "                    \n",
    "        if current_gesture:\n",
    "            cv2.putText(frame, \n",
    "              f\"{current_gesture} ({current_confidence:.2%})\", \n",
    "              (10, 50), \n",
    "              cv2.FONT_HERSHEY_SIMPLEX, \n",
    "              1, \n",
    "              (0, 255, 0), \n",
    "              2)\n",
    "\n",
    "        \"\"\"# 4.7 Mostrar frame (CORREGIDO)\n",
    "        # Redimensionar y centrar la ventana\n",
    "        display_frame = cv2.resize(frame, (640, 480))  # Ajustar a resolución HD\n",
    "        \n",
    "        # Obtener dimensiones de la pantalla\n",
    "        screen_width = 1920  # Ajustar según la resolución del monitor del usuario\n",
    "        screen_height = 1080\n",
    "        \n",
    "        # Calcular posición centrada\n",
    "        x_pos = (screen_width - 1500) // 2\n",
    "        y_pos = (screen_height - 1000) // 2\n",
    "        \n",
    "        # Crear ventana y posicionar\n",
    "        cv2.namedWindow(\"Reconocimiento de Señas\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"Reconocimiento de Señas\", 640, 480)\n",
    "        cv2.moveWindow(\"Reconocimiento de Señas\", x_pos, y_pos)\n",
    "        \n",
    "        cv2.imshow(\"Reconocimiento de Señas\", display_frame)\"\"\"\n",
    "\n",
    "        # Mostrar frame reducido\n",
    "        cv2.imshow(\"Predicciones\", cv2.resize(frame, (640, 480)))\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == 27: break\n",
    "        \n",
    "        # 4.8 Salir con ESC\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # 5. Limpieza\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\nSistema de evaluación detenido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. REENTRENAR GESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_gesture():\n",
    "    global gestures\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay gestos para reentrenar. Primero recolecte datos.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nGestos disponibles para reentrenar:\")\n",
    "    for i, gesture in enumerate(gestures):\n",
    "        print(f\"{i+1}. {gesture}\")\n",
    "\n",
    "    try:\n",
    "        choice = int(input(\"\\nSeleccione el número del gesto a reentrenar: \")) - 1\n",
    "        if 0 <= choice < len(gestures):\n",
    "            gesture = gestures[choice]\n",
    "            gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "            \n",
    "            for file in os.listdir(gesture_dir):\n",
    "                os.remove(os.path.join(gesture_dir, file))\n",
    "            \n",
    "            print(f\"\\nDatos anteriores de '{gesture}' eliminados.\")\n",
    "            collect_data()\n",
    "            train_model()\n",
    "        else:\n",
    "            print(\"\\nSelección inválida.\")\n",
    "    except ValueError:\n",
    "        print(\"\\nPor favor, ingrese un número válido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menú principal\n",
    "def main():\n",
    "    init_system()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Detectar Manos\")\n",
    "        print(\"2. Recolectar Datos\")\n",
    "        print(\"3. Entrenar Modelo, y despues ir a convertir a TFlite\")\n",
    "        print(\"4. Evaluar\")\n",
    "        print(\"5. Reentrenar Gesto\")\n",
    "        print(\"6. Convertir a TFLite\")  # Nueva opción\n",
    "        print(\"7. Salir\")\n",
    "        \n",
    "        choice = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            detect_hands()\n",
    "        elif choice == '2':\n",
    "            sign_name = input(\"Nombre de la seña a recolectar: \")\n",
    "            collect_data(data_dir_video, dataset_dir, sign_name, sequence_length)\n",
    "        elif choice == '3':\n",
    "            train_model()\n",
    "        elif choice == '4':\n",
    "            evaluate()\n",
    "        elif choice == '5':\n",
    "            retrain_gesture()\n",
    "        elif choice == '6':  # Nueva opción de conversión\n",
    "            convert_to_tflite()\n",
    "        elif choice == '7':\n",
    "            print(\"\\n¡Hasta luego!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nOpción inválida. Por favor, intente de nuevo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo, y despues ir a convertir a TFlite\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Convertir a TFLite\n",
      "7. Salir\n",
      "\n",
      "Cargando datos y preparando el entrenamiento...\n",
      "¡Secuencia inválida en Hola: sequence_0.npy!\n",
      "¡Secuencia inválida en Hola: sequence_1.npy!\n",
      "¡Secuencia inválida en Hola: sequence_10.npy!\n",
      "¡Secuencia inválida en Hola: sequence_11.npy!\n",
      "¡Secuencia inválida en Hola: sequence_12.npy!\n",
      "¡Secuencia inválida en Hola: sequence_13.npy!\n",
      "¡Secuencia inválida en Hola: sequence_14.npy!\n",
      "¡Secuencia inválida en Hola: sequence_15.npy!\n",
      "¡Secuencia inválida en Hola: sequence_16.npy!\n",
      "¡Secuencia inválida en Hola: sequence_17.npy!\n",
      "¡Secuencia inválida en Hola: sequence_18.npy!\n",
      "¡Secuencia inválida en Hola: sequence_19.npy!\n",
      "¡Secuencia inválida en Hola: sequence_2.npy!\n",
      "¡Secuencia inválida en Hola: sequence_3.npy!\n",
      "¡Secuencia inválida en Hola: sequence_4.npy!\n",
      "¡Secuencia inválida en Hola: sequence_5.npy!\n",
      "¡Secuencia inválida en Hola: sequence_6.npy!\n",
      "¡Secuencia inválida en Hola: sequence_7.npy!\n",
      "¡Secuencia inválida en Hola: sequence_8.npy!\n",
      "¡Secuencia inválida en Hola: sequence_9.npy!\n",
      "\n",
      "ERROR CRÍTICO: Dataset vacío o corrupto.\n",
      "Posibles causas:\n",
      "- Directorios de gestos vacíos\n",
      "- Archivos .npy con formato incorrecto\n",
      "- Secuencias con dimensiones diferentes a (sequence_length, total_landmarks)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 23\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     collect_data(data_dir_video, dataset_dir, sign_name, sequence_length)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     25\u001b[0m     evaluate()\n",
      "Cell \u001b[1;32mIn[50], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCargando datos y preparando el entrenamiento...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m X, y, gestures \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m---> 13\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 3. Calcular parámetros de normalización\u001b[39;00m\n\u001b[0;32m     16\u001b[0m X_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\numerical_utils.py:96\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(x, num_classes)\u001b[0m\n\u001b[0;32m     94\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m num_classes:\n\u001b[1;32m---> 96\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     97\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     98\u001b[0m categorical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, num_classes))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
