{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comunicacion y camara\n",
    "import socket\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voz\n",
    "import pyttsx3 \n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración UDP\n",
    "UDP_IP_PI = \"192.168.7.2\"  # IP de la Raspberry Pi\n",
    "UDP_OPEN = '0.0.0.0' # Direccion que escucha en todos los canales\n",
    "UDP_PORT = 5003\n",
    "UDP_PORT_SERVO = 5001  # Puerto para enviar comandos\n",
    "UDP_PORT_CAM = 5002  # Puerto para recibir video\n",
    "MAX_PACKET_SIZE = 1400  # Tamaño máximo del paquete UDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el socket UDP (compartido para todos los hilos)\n",
    "udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "# Inicializa el motor TTS\n",
    "tts_engine = pyttsx3.init()\n",
    "tts_engine.setProperty('rate', 150)\n",
    "\n",
    "# Objeto de bloqueo para sincronización\n",
    "tts_lock = threading.Lock()\n",
    "\n",
    "# Variable global para última reproducción\n",
    "last_spoken_gesture = None\n",
    "\n",
    "def speak_text(text):\n",
    "    global last_spoken_gesture, udp_socket\n",
    "    \n",
    "    with tts_lock:\n",
    "        if text != last_spoken_gesture:\n",
    "            last_spoken_gesture = text\n",
    "            \n",
    "            try:\n",
    "                # Generar archivo de audio temporal\n",
    "                temp_file = \"temp_audio.wav\"\n",
    "                tts_engine.save_to_file(text, temp_file)\n",
    "                tts_engine.runAndWait()  # Esperar generación del archivo\n",
    "                \n",
    "                # Leer y enviar audio\n",
    "                with open(temp_file, 'rb') as f:\n",
    "                    audio_data = f.read()\n",
    "                    \n",
    "                    # Fragmentar y enviar\n",
    "                    total_chunks = (len(audio_data) + MAX_PACKET_SIZE - 1) // MAX_PACKET_SIZE\n",
    "                    for i in range(total_chunks):\n",
    "                        start = i * MAX_PACKET_SIZE\n",
    "                        end = start + MAX_PACKET_SIZE\n",
    "                        chunk = audio_data[start:end]\n",
    "                        udp_socket.sendto(chunk, (UDP_IP_PI, UDP_PORT))\n",
    "                        time.sleep(0.001)  # Pequeña pausa entre paquetes\n",
    "                \n",
    "                print(f\"Audio enviado: {text}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en generación/envío de audio: {str(e)}\")\n",
    "            finally:\n",
    "                # Limpiar archivo temporal\n",
    "                if os.path.exists(temp_file):\n",
    "                    os.remove(temp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5, #probar con 0.4\n",
    "    min_tracking_confidence=0.5 #probar con 0.4\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class UDPCamera:\n",
    "    def __init__(self):\n",
    "        self.host = UDP_OPEN\n",
    "        self.port = UDP_PORT_CAM\n",
    "        self.buffer_size = 65536\n",
    "        self.mtu = 1400\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.settimeout(2)\n",
    "        self.frame = None\n",
    "        self.fragments = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Configuración INTRÍNSECA de MediaPipe para el seguimiento\n",
    "        self.hands_tracker = mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.6,\n",
    "            min_tracking_confidence=0.6\n",
    "        )\n",
    "        \n",
    "        # Socket para enviar datos del servo\n",
    "        self.send_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        \n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.sock.bind((self.host, self.port))\n",
    "            self.thread = threading.Thread(target=self._receive_frames, daemon=True)\n",
    "            self.thread.start()\n",
    "\n",
    "    def _process_hand(self, frame):\n",
    "        # Procesamiento específico de la muñeca\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands_tracker.process(frame_rgb)\n",
    "        \n",
    "        if results.multi_hand_landmarks and results.multi_handedness:\n",
    "            for hand_handedness in results.multi_handedness:\n",
    "                if hand_handedness.classification[0].label == 'Left':\n",
    "                    wrist = results.multi_hand_landmarks[0].landmark[mp_hands.HandLandmark.WRIST]\n",
    "                    # Mapear coordenadas de la palma a un rango de -7.5 a 7.5\n",
    "                    x_normalized = int((wrist.x - 0.5) * 15) \n",
    "                    \n",
    "                    # Envío UDP automático\n",
    "                    self.send_sock.sendto(\n",
    "                        str(x_normalized).encode(), \n",
    "                        (UDP_IP_PI, UDP_PORT_SERVO)\n",
    "                    )\n",
    "                    \n",
    "                    # Dibujar punto (opcional)\n",
    "                    wrist_pixel = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                        wrist.x, wrist.y, frame.shape[1], frame.shape[0]\n",
    "                    )\n",
    "                    if wrist_pixel:\n",
    "                        cv2.circle(frame, wrist_pixel, 10, (0, 255, 0), -1)\n",
    "                    \n",
    "                    return x_normalized\n",
    "        return None\n",
    "\n",
    "    def _receive_frames(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                fragment, _ = self.sock.recvfrom(self.buffer_size)\n",
    "                with self.lock:\n",
    "                    self.fragments.append(fragment)\n",
    "                    if len(fragment) < self.mtu:\n",
    "                        frame_bytes = b''.join(self.fragments)\n",
    "                        self.fragments = []\n",
    "                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)\n",
    "                        frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)\n",
    "                        \n",
    "                        # Procesamiento AUTOMÁTICO de la mano\n",
    "                        if frame is not None:\n",
    "                            self._process_hand(frame)\n",
    "                            self.frame = frame  # Almacenar frame procesado\n",
    "                            \n",
    "            except socket.timeout:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def read(self):\n",
    "        with self.lock:\n",
    "            if self.frame is not None:\n",
    "                return True, self.frame.copy()\n",
    "            return False, None\n",
    "\n",
    "    def release(self):\n",
    "        self.running = False\n",
    "        self.hands_tracker.close()\n",
    "        with self.lock:\n",
    "            self.fragments = []\n",
    "            self.frame = None\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        self.sock.close()\n",
    "        self.send_sock.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel:\n",
    "    def __init__(self, model_path):\n",
    "        # Cargar el modelo TFLite\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        \n",
    "        # Obtener detalles de entrada y salida\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        # Asegurar el tipo de dato correcto y agregar dimensión batch si es necesario\n",
    "        input_data = np.array(input_data, dtype=self.input_details[0]['dtype'])\n",
    "        if len(input_data.shape) == len(self.input_details[0]['shape']) - 1:\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        # Establecer la entrada y ejecutar la inferencia\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.interpreter.invoke()\n",
    "        \n",
    "        # Obtener la salida\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de TensorFlow para rendimiento\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Configuración de TensorFlow para rendimiento en CPU\n",
    "    try:\n",
    "        # Verificar si hay GPU disponible (para futuras expansiones)\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        if physical_devices:\n",
    "            # Configuración para GPU (no se ejecutará en tu caso)\n",
    "            for device in physical_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(\"GPU disponible para aceleración\")\n",
    "        else:\n",
    "            # Optimización para CPU\n",
    "            tf.config.threading.set_intra_op_parallelism_threads(4)  # Aprovecha núcleos físicos\n",
    "            tf.config.threading.set_inter_op_parallelism_threads(2)  # Paralelismo entre operaciones\n",
    "            print(\"Modo CPU activado: Configuración optimizada para Intel Core i7-7500U\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error de configuración: {str(e)}\")\n",
    "        print(\"Usando configuración por defecto de CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "data_dir = \"hand_gestures_data_v15\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Modelo y datos de entrenamiento\n",
    "model = None\n",
    "# Inicializar scaler y label encoder\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "model_file = \"hand_gesture_nn_model_v15.h5\"\n",
    "scaler_file = \"hand_gesture_scaler_v15.pkl\"\n",
    "encoder_file = \"hand_gesture_encoder_v15.pkl\"\n",
    "gesture_data = \"gesture_data_v15.pkl\" \n",
    "model_tflite = \"modelo_optimizadotl_v15.tflite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí tienes la explicación detestructura de los archivos clave del sistema:\n",
    "\n",
    "### 1. **hand_gestures_data_4_3/** (Directorio)\n",
    "- **Qué es**: Carpeta que almacena datos crudos de entrenamiento.\n",
    "- **Contenido**:\n",
    "  - Archivos `.pkl` con pares de:\n",
    "  - `features`: Lista de landmarks (126 valores numéricos por muestra).\n",
    "  - `labels`: Nombres de gestos asociados (ej: \"Hola\", \"Chau\").\n",
    "- **Función**: Base de datos para entrenar/reentrenar el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **hand_gesture_nn_model_4_3.h5** (Archivo de modelo)\n",
    "- **Qué es**: Modelo de red neuronal entrenado en formato HDF5.\n",
    "- **Contenido**:\n",
    "  - Arquitectura de la red (capas y conexiones).\n",
    "  - Pesos aprendidos durante el entrenamiento.\n",
    "  - Hiperparámetros de configuración.\n",
    "- **Función**: Realiza predicciones usando los landmarks detectados.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **hand_gesture_scaler_4_3.pkl** (Archivo de preprocesamiento)\n",
    "- **Qué es**: Objeto `StandardScaler` serializado.\n",
    "- **Contenido**:\n",
    "  - Media y desviación estándar de cada característica.\n",
    "  ```python\n",
    "  Ejemplo de datos almacenados:\n",
    "  mean = [0.12, 0.45, ..., 0.78]  # 126 valores\n",
    "  std = [0.02, 0.15, ..., 0.23]    # 126 valores\n",
    "  ```\n",
    "- **Función**: Normaliza nuevos datos usando los mismos parámetros del entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **hand_gesture_encoder_4_3.pkl** (Archivo de codificación)\n",
    "- **Qué es**: Objeto `LabelEncoder` serializado.\n",
    "- **Contenido**:\n",
    "  - Mapeo entre nombres de gestos y números:\n",
    "  ```python\n",
    "  Ejemplo:\n",
    "  {\"Hola\": 0, \"Chau\": 1, \"Buenos días\": 2}\n",
    "  ```\n",
    "- **Función**: Convierte:  \n",
    "  `Predicción numérica → Nombre de gesto legible`.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **gesture_data_4_3.pkl** (Datos procesados)\n",
    "- **Qué es**: Versión serializada de los datos listos para entrenamiento.\n",
    "- **Contenido**:\n",
    "  - Datos ya normalizados (`X_scaled`).\n",
    "  - Etiquetas codificadas numéricamente (`y_encoded`).\n",
    "- **Función**: Acelera el reentrenamiento al evitar reprocesar datos crudos.\n",
    "\n",
    "---\n",
    "\n",
    "### Flujo de trabajo:\n",
    "1. **Recolección**:  \n",
    "   Landmarks crudos → Se guardan en `hand_gestures_data_4_3/`.\n",
    "\n",
    "2. **Entrenamiento**:  \n",
    "   Usa `scaler` para normalizar y `encoder` para codificar → Genera el modelo `.h5`.\n",
    "\n",
    "3. **Predicción**:  \n",
    "   Nuevos landmarks → Se normalizan con `scaler.pkl` → Modelo `.h5` predice → `encoder.pkl` traduce a texto.\n",
    "\n",
    "Estos archivos forman un ecosistema completo que permite actualizar gestos, reentrenar el modelo, y mantener consistencia en las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para estado\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Estado del sistema\n",
    "is_trained = False\n",
    "is_collecting = False\n",
    "current_gesture = \"\"\n",
    "samples_collected = 0\n",
    "max_samples = 5000 #estaba en 100 antes\n",
    "\n",
    "# Control de tiempo para la recolección continua\n",
    "last_sample_time = 0\n",
    "sample_delay = 0.05  # 50ms entre muestras\n",
    "\n",
    "# Temporizador para mostrar mensajes\n",
    "message = \"\"\n",
    "message_until = 0\n",
    "\n",
    "# Para evaluación del modelo\n",
    "metrics = {\n",
    "    'accuracy': 0,\n",
    "    'val_accuracy': 0,\n",
    "    'training_time': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extrae los landmarks de las manos desde un frame de video.\n",
    "\n",
    "    Args:\n",
    "        frame: Imagen capturada por la cámara (en formato BGR).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Lista de landmarks normalizados (126 elementos) y booleano indicando si se detectaron manos.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    landmarks_data = []\n",
    "    hands_detected = False\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hands_detected = True\n",
    "        # Extraer landmarks de hasta dos manos\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Dibujar landmarks en el frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = []\n",
    "\n",
    "            # Extraer coordenadas (x,y,z) de los 21 landmarks\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            landmarks_data.extend(landmarks)\n",
    "    \n",
    "    # Normalizar para 2 manos (si solo hay una o ninguna, rellenar con ceros)\n",
    "    while len(landmarks_data) < 21 * 3 * 2:  # 21 puntos * 3 coordenadas * 2 manos\n",
    "        landmarks_data.append(0.0)\n",
    "    \n",
    "    # Limitar a exactamente 126 características (21 puntos * 3 coordenadas * 2 manos)\n",
    "    landmarks_data = landmarks_data[:21 * 3 * 2]\n",
    "    \n",
    "    return landmarks_data, hands_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_message(message_text, duration=2):\n",
    "    \"\"\"\n",
    "    Establece un mensaje para mostrar en pantalla por una duración determinada.\n",
    "\n",
    "    Args:\n",
    "        message_text: Mensaje a mostrar (str).\n",
    "        duration: Duración en segundos (int o float, por defecto 2).\n",
    "    \"\"\"\n",
    "    global message, message_until\n",
    "    message = message_text\n",
    "    message_until = time.time() + duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_collection(gesture_name):\n",
    "    \"\"\"\n",
    "    Inicia la recolección de datos para una seña específica.\n",
    "\n",
    "    Args:\n",
    "        gesture_name: Nombre de la seña a recolectar (str).\n",
    "    \"\"\"\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = True\n",
    "    current_gesture = gesture_name\n",
    "    samples_collected = 0\n",
    "    set_message(f\"Mantenga la seña frente a la cámara. Recolectando '{gesture_name}'...\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_collection():\n",
    "    \"\"\"\n",
    "    Detiene la recolección de datos.\n",
    "    \"\"\"\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = False\n",
    "    current_gesture = \"\"\n",
    "    samples_collected = 0\n",
    "    set_message(\"Recolección finalizada\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "    \"\"\"\n",
    "    Guarda los datos recolectados en disco.\n",
    "    \"\"\"\n",
    "    global data, labels, gesture_data\n",
    "    data_dict = {\n",
    "        \"features\": data, \n",
    "        \"labels\": labels\n",
    "        }\n",
    "    with open(f\"{data_dir}/{gesture_data}\", \"wb\") as f:\n",
    "        pickle.dump(data_dict, f)\n",
    "    set_message(f\"Datos guardados: {len(data)} muestras\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sample(landmarks):\n",
    "    \"\"\"\n",
    "    Recolecta una muestra de landmarks para la seña actual.\n",
    "\n",
    "    Args:\n",
    "        landmarks: Lista de landmarks extraídos (126 elementos).\n",
    "\n",
    "    Returns:\n",
    "        bool: True si se completó la recolección (max_samples alcanzado), False en caso contrario.\n",
    "    \"\"\"\n",
    "    global data, labels, samples_collected, last_sample_time, is_collecting\n",
    "\n",
    "    if not is_collecting:\n",
    "        return False\n",
    "    \n",
    "    current_time = time.time()\n",
    "    # Verificar si ha pasado suficiente tiempo desde la última muestra\n",
    "    if current_time - last_sample_time >= sample_delay:\n",
    "        data.append(landmarks)\n",
    "        labels.append(current_gesture)\n",
    "        samples_collected += 1\n",
    "        last_sample_time = current_time\n",
    "\n",
    "        # Guardar datos periódicamente\n",
    "        if samples_collected % 10 == 0:\n",
    "            save_data()\n",
    "            \n",
    "        # Verificar si se ha completado la recolección\n",
    "        if samples_collected >= max_samples:\n",
    "            stop_collection()\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Carga los datos recolectados desde disco.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si los datos se cargaron correctamente, False en caso contrario.\n",
    "    \"\"\"\n",
    "    global data, labels, gesture_data\n",
    "    try:\n",
    "        with open(f\"{data_dir}/{gesture_data}\", \"rb\") as f:\n",
    "            data_dict = pickle.load(f)\n",
    "            data = data_dict[\"features\"]\n",
    "            labels = data_dict[\"labels\"]\n",
    "        set_message(f\"Datos cargados: {len(data)} muestras\", 2)\n",
    "        return True\n",
    "    except:\n",
    "        set_message(\"No se encontraron datos previos\", 2)\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Crea una red neuronal liviana para reconocimiento de gestos.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Dimensión de las características de entrada (int, ej. 126).\n",
    "        num_classes: Número de clases a predecir (int).\n",
    "\n",
    "    Returns:\n",
    "        Modelo de red neuronal compilado (tensorflow.keras.Model).\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Capa de entrada con regularización para prevenir sobreajuste\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,), \n",
    "              kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3), # Dropout para mejorar generalización\n",
    "\n",
    "        # Capa oculta \n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        # Capa de salida\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compilar con optimizador Adam y tasa de aprendizaje reducida para estabilidad\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    \"\"\"\n",
    "    Entrena el modelo de red neuronal con los datos recolectados.\n",
    "\n",
    "    Returns:\n",
    "        float: Precisión del modelo en datos de prueba.\n",
    "    \"\"\"\n",
    "    global data, labels, scaler, label_encoder, is_trained, metrics\n",
    "    if len(data) < 10:\n",
    "        set_message(\"Se necesitan más datos para entrenar\", 2)\n",
    "        return 0\n",
    "    \n",
    "    set_message(\"Preparando datos para entrenamiento...\", 1)\n",
    "\n",
    "    # Convertir a arrays NumPy\n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Codificar etiquetas\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Dividir datos en conjuntos de entrenamiento y prueba\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Normalizar datos\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "\n",
    "    # Crear modelo\n",
    "    num_classes = len(set(y_encoded))\n",
    "    set_message(f\"Entrenando modelo con {num_classes} clases...\", 2)\n",
    "    \n",
    "    model = create_neural_network(X_train.shape[1], num_classes)\n",
    "    \n",
    "    # Callbacks para mejorar el entrenamiento\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True),\n",
    "\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=5, \n",
    "            min_lr=0.0001)\n",
    "    ]\n",
    "\n",
    "    # Medir tiempo de entrenamiento\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=50, \n",
    "        batch_size=32, \n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks, \n",
    "        verbose=1)\n",
    "    \n",
    "    # Calcular tiempo de entrenamiento\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    \n",
    "    # Guardar métricas\n",
    "    metrics.update({\n",
    "        'accuracy': test_accuracy,\n",
    "        'val_accuracy': max(history.history['val_accuracy']),\n",
    "        'training_time': training_time,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'report': classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "    })\n",
    "    \n",
    "    # Guardar el modelo y componentes\n",
    "    model.save(model_file)\n",
    "    with open(scaler_file, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(encoder_file, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    is_trained = True\n",
    "    set_message(f\"Modelo entrenado. Precisión: {test_accuracy:.2f}\", 3)\n",
    "    \n",
    "    # Imprimir reporte detallado\n",
    "    print(\"\\n--- Informe del Modelo ---\")\n",
    "    print(f\"Precisión en datos de prueba: {test_accuracy:.4f}\")\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f} segundos\")\n",
    "    print(\"\\nClasificación detallada:\")\n",
    "    print(metrics['report'])\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model():\n",
    "    \"\"\"\n",
    "    Carga un modelo previamente entrenado desde disco.\n",
    "\n",
    "    Returns:\n",
    "        Modelo cargado (tensorflow.keras.Model) o None si falla la carga.\n",
    "    \"\"\"\n",
    "    global scaler, label_encoder, is_trained\n",
    "    try:\n",
    "        # Cargar modelo, scaler y codificador de etiquetas\n",
    "        model = load_model(model_file)\n",
    "        #model = tf.keras.models.load_model(model_file)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(encoder_file, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        is_trained = True\n",
    "        set_message(\"Modelo de red neuronal cargado correctamente\", 2)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo: {str(e)}\")\n",
    "        set_message(\"No se encontró un modelo guardado\", 2)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Predice la seña a partir de los landmarks usando el modelo TFLite.\n",
    "\n",
    "    Args:\n",
    "        landmarks: Lista de landmarks extraídos (126 elementos).\n",
    "        tflite_model: Instancia de TFLiteModel.\n",
    "        scaler: Objeto StandardScaler ya entrenado.\n",
    "        label_encoder: Objeto LabelEncoder ya entrenado.\n",
    "        threshold: Valor de confianza mínimo para considerar la predicción.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Nombre de la seña predicha (str) y confianza (float).\n",
    "    \"\"\"\n",
    "    # Preprocesar datos\n",
    "    X = np.array([landmarks])\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Realizar la predicción con TFLite\n",
    "    prediction_probs = tflite_model.predict(X_scaled)[0]\n",
    "    prediction_idx = np.argmax(prediction_probs)\n",
    "    confidence = prediction_probs[prediction_idx]\n",
    "    \n",
    "    # Decodificar la clase\n",
    "    try:\n",
    "        prediction_label = label_encoder.inverse_transform([prediction_idx])[0]\n",
    "    except Exception as e:\n",
    "        prediction_label = \"Desconocido\"\n",
    "    \n",
    "    # Solo devolver predicción si la confianza es suficiente\n",
    "    if confidence >= threshold:\n",
    "        return prediction_label, confidence\n",
    "    return \"Desconocido\", confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_gestures():\n",
    "    \"\"\"\n",
    "    Obtiene la lista de gestos disponibles en el conjunto de datos.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de nombres de gestos únicos.\n",
    "    \"\"\"\n",
    "    return list(set(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUEVO MENSAJE QUE DICE SI EXISTE EL MDELO\n",
    "def check_model_exists():\n",
    "    \"\"\"Verifica si el archivo del modelo existe.\"\"\"\n",
    "    return os.path.exists(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model_file, model_tflite):\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(model_file):\n",
    "            raise FileNotFoundError(f\"El archivo {model_file} no existe.\")\n",
    "        \n",
    "        # Cargar el modelo entrenado\n",
    "        modelo = load_model(model_file)\n",
    "        \n",
    "        # Convertir a TensorFlow Lite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo convertido\n",
    "        with open(model_tflite, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(\"Modelo convertido a TensorFlow Lite.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al convertir el modelo a TFLite:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "global model_file, model_tflite\n",
    "\n",
    "try:\n",
    "    if not os.path.exists(model_file):\n",
    "        raise FileNotFoundError(f\"El archivo {model_file} no existe.\")\n",
    "\n",
    "    # Cargar el modelo entrenado\n",
    "    modelo = load_model(model_file)\n",
    "\n",
    "    # Convertir a TensorFlow Lite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n",
    "    tflite_model = converter.convert()\n",
    "\n",
    "    # Guardar el modelo convertido\n",
    "    with open(model_tflite, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "    print(\"Modelo convertido a TensorFlow Lite.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error al cargar o convertir el modelo:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"modelo_optimizadotl.tflite\"\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"El archivo {model_path} no existe. Asegúrate de haberlo generado correctamente.\")\n",
    "else:\n",
    "    tflite_model = TFLiteModel(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_menu():\n",
    "    print(\"\\n=== MENU PRINCIPAL ===\")\n",
    "    print(\"1. Recolectar nueva seña\")\n",
    "    print(\"2. Entrenar modelo\")\n",
    "    print(\"3. Listar señas cargadas\")\n",
    "    print(\"4. Evaluar en tiempo real\")\n",
    "    print(\"5. Salir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gestures():\n",
    "    # Asumiendo que 'labels' es la lista donde se guardan las señas\n",
    "    if not labels:\n",
    "        print(\"No hay señas guardadas.\")\n",
    "    else:\n",
    "        unique_gestures = list(set(labels))\n",
    "        print(\"\\n--- Señas Guardadas ---\")\n",
    "        for i, gesture in enumerate(unique_gestures, 1):\n",
    "            print(f\"{i}. {gesture}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_collection_mode():\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP iniciada para recolección.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "\n",
    "    while is_collecting:  # Asumiendo que 'is_collecting' se activa en start_collection()\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame)\n",
    "\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        # Mostrar información en pantalla durante la recolección\n",
    "        progress = int((samples_collected / max_samples) * frame_w)\n",
    "        cv2.rectangle(frame, (0, 0), (progress, 20), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, f\"Recolectando: {current_gesture} ({samples_collected}/{max_samples})\", \n",
    "                    (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        \"\"\"if not hands_detected:\n",
    "            cv2.putText(frame, \"¡Muestra las manos!\", (frame_w//2 - 100, frame_h//2), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\"\"\"\n",
    "        \n",
    "        if hands_detected:\n",
    "                    collect_sample(landmarks)\n",
    "        else:\n",
    "            cv2.putText(frame, \"¡Muestra las manos!\", (frame_w//2 - 100, frame_h//2), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "        if not is_collecting:  # Cuando termina la recolección\n",
    "                menu_active = True\n",
    "                save_data()\n",
    "        \n",
    "        cv2.imshow(\"Recolectar Señas\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        # Puedes agregar una tecla para finalizar la recolección, por ejemplo 'm' para volver al menú.\n",
    "        if key == ord('m'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    save_data()  # Guarda los datos recolectados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_mode():\n",
    "    global model_tflite\n",
    "    # Inicializa el modelo TFLite si aún no se ha cargado\n",
    "    if os.path.exists(model_tflite):\n",
    "        tflite_model = TFLiteModel(model_tflite)\n",
    "    else:\n",
    "        print(\"El modelo TFLite no existe. Conviértelo primero.\")\n",
    "        return\n",
    "\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP iniciada para evaluación en tiempo real.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame)\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        if hands_detected:\n",
    "            prediction, confidence = predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.9)\n",
    "            color = (0, 255, 0) if confidence > 0.9 else (0, 165, 255)\n",
    "            cv2.putText(frame, f\"Seña: {prediction}\", (10, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "            cv2.putText(frame, f\"Confianza: {confidence:.2%}\", (10, 90), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Extraer valor escalar en caso de que 'confidence' sea un array\n",
    "            confidence_value = np.max(confidence) if isinstance(confidence, np.ndarray) else confidence\n",
    "\n",
    "            if confidence_value > 0.99 and prediction != \"Desconocido\":\n",
    "                threading.Thread(target=speak_text, args=(prediction,), daemon=True).start()\n",
    "        else:\n",
    "            cv2.putText(frame, \"Acerca las manos a la cámara\", (frame_w//4, frame_h//2), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "        cv2.putText(frame, \"Presiona M para volver al menú\", (10, frame_h - 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.imshow(\"Evaluación en Tiempo Real\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        #if key == ord('m'):\n",
    "        #    break\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global model, is_trained, data, labels\n",
    "\n",
    "    # Inicialización del sistema\n",
    "    is_trained = False\n",
    "    model = None\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Cargar datos existentes\n",
    "    load_data()\n",
    "    \n",
    "    # Intentar cargar modelo si existe\n",
    "    if check_model_exists():\n",
    "        model = load_saved_model()\n",
    "        is_trained = True\n",
    "    else:\n",
    "        is_trained = False\n",
    "\n",
    "    # Mostrar el menú en la consola\n",
    "    print_menu()\n",
    "\n",
    "    # Bucle principal de selección en consola\n",
    "    while True:\n",
    "        opcion = input(\"\\nSelecciona una opción (Recolectar: 1, Entrenar: 2, Señas: 3, Evaluar: 4, Salir: 5): \").strip()\n",
    "        \n",
    "        if opcion == '1':\n",
    "            # Recolección de señas\n",
    "            gesture_name = input(\"Ingrese nombre de la seña (ej. 'Hola'): \")\n",
    "            if gesture_name:\n",
    "                start_collection(gesture_name)\n",
    "                # Iniciar la cámara para mostrar video durante la recolección\n",
    "                run_collection_mode()\n",
    "                \n",
    "        elif opcion == '2':\n",
    "            if len(data) > 10:\n",
    "                train_model()\n",
    "                model = load_saved_model() if check_model_exists() else None\n",
    "                is_trained = True\n",
    "                print(\"Entrenamiento completado. Modelo entrenado.\")\n",
    "                convert_to_tflite(model_file, model_tflite)\n",
    "                print(\"convertido a Tflite para evaluacion en tiemop real\")\n",
    "            else:\n",
    "                print(\"¡Necesitas al menos 10 muestras para entrenar!\")\n",
    "                \n",
    "        elif opcion == '3':\n",
    "            list_gestures()  # Lista las señas cargadas\n",
    "\n",
    "        elif opcion == '4':\n",
    "            if is_trained:\n",
    "                # Inicializar modo evaluación en tiempo real\n",
    "                print(\"Modo de evaluación activado.\")\n",
    "                run_evaluation_mode()\n",
    "            else:\n",
    "                print(\"¡Entrena el modelo primero (Opción 2)!\")\n",
    "                \n",
    "        elif opcion == '5':\n",
    "            print(\"Saliendo del programa...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Opción inválida, intenta nuevamente.\")\n",
    "        \n",
    "        # Mostrar nuevamente el menú luego de finalizar la opción seleccionada.\n",
    "        print_menu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "No hay señas guardadas.\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "No hay señas guardadas.\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8640 - loss: 0.4908 - val_accuracy: 0.9875 - val_loss: 0.1661 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9804 - loss: 0.1844 - val_accuracy: 0.9975 - val_loss: 0.1194 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.1444 - val_accuracy: 0.9979 - val_loss: 0.1036 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.1347 - val_accuracy: 0.9971 - val_loss: 0.0946 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.1240 - val_accuracy: 0.9967 - val_loss: 0.0855 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.1008 - val_accuracy: 0.9967 - val_loss: 0.0775 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9923 - loss: 0.0945 - val_accuracy: 0.9983 - val_loss: 0.0678 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0923 - val_accuracy: 0.9967 - val_loss: 0.0628 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9920 - loss: 0.0819 - val_accuracy: 0.9979 - val_loss: 0.0564 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0793 - val_accuracy: 0.9962 - val_loss: 0.0582 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0830 - val_accuracy: 0.9954 - val_loss: 0.0561 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0679 - val_accuracy: 0.9975 - val_loss: 0.0503 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0734 - val_accuracy: 0.9967 - val_loss: 0.0466 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9916 - loss: 0.0661 - val_accuracy: 0.9975 - val_loss: 0.0451 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0606 - val_accuracy: 0.9925 - val_loss: 0.0505 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0712 - val_accuracy: 0.9958 - val_loss: 0.0418 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0557 - val_accuracy: 0.9971 - val_loss: 0.0396 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9950 - loss: 0.0516 - val_accuracy: 0.9975 - val_loss: 0.0356 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0550 - val_accuracy: 0.9958 - val_loss: 0.0405 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9927 - loss: 0.0572 - val_accuracy: 0.9958 - val_loss: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0627 - val_accuracy: 0.9983 - val_loss: 0.0362 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9918 - loss: 0.0543 - val_accuracy: 0.9950 - val_loss: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9906 - loss: 0.0548 - val_accuracy: 0.9975 - val_loss: 0.0355 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0553 - val_accuracy: 0.9987 - val_loss: 0.0304 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9964 - loss: 0.0412 - val_accuracy: 0.9979 - val_loss: 0.0307 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0375 - val_accuracy: 0.9992 - val_loss: 0.0272 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0370 - val_accuracy: 0.9967 - val_loss: 0.0329 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9969 - loss: 0.0374 - val_accuracy: 0.9983 - val_loss: 0.0302 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9965 - loss: 0.0342 - val_accuracy: 0.9992 - val_loss: 0.0235 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9969 - loss: 0.0324 - val_accuracy: 0.9962 - val_loss: 0.0295 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9938 - loss: 0.0410 - val_accuracy: 0.9979 - val_loss: 0.0278 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0390 - val_accuracy: 0.9987 - val_loss: 0.0238 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0287 - val_accuracy: 0.9987 - val_loss: 0.0225 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0297 - val_accuracy: 0.9983 - val_loss: 0.0236 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0313 - val_accuracy: 0.9987 - val_loss: 0.0226 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0337 - val_accuracy: 0.9996 - val_loss: 0.0202 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0287 - val_accuracy: 0.9992 - val_loss: 0.0207 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9959 - loss: 0.0305 - val_accuracy: 0.9971 - val_loss: 0.0234 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0270 - val_accuracy: 0.9987 - val_loss: 0.0219 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9956 - loss: 0.0314 - val_accuracy: 0.9983 - val_loss: 0.0206 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0257 - val_accuracy: 0.9987 - val_loss: 0.0207 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0228 - val_accuracy: 0.9987 - val_loss: 0.0182 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9989 - loss: 0.0217 - val_accuracy: 0.9992 - val_loss: 0.0185 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9988 - loss: 0.0197 - val_accuracy: 0.9996 - val_loss: 0.0166 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9968 - loss: 0.0223 - val_accuracy: 0.9992 - val_loss: 0.0180 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0239 - val_accuracy: 0.9987 - val_loss: 0.0179 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0215 - val_accuracy: 0.9987 - val_loss: 0.0173 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0237 - val_accuracy: 0.9992 - val_loss: 0.0169 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0245 - val_accuracy: 0.9992 - val_loss: 0.0157 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0205 - val_accuracy: 0.9987 - val_loss: 0.0172 - learning_rate: 2.5000e-04\n",
      "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9997\n",
      "Tiempo de entrenamiento: 35.79 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Chau       1.00      1.00      1.00      1000\n",
      "        Hola       1.00      1.00      1.00      1000\n",
      "          Yo       1.00      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      3000\n",
      "   macro avg       1.00      1.00      1.00      3000\n",
      "weighted avg       1.00      1.00      1.00      3000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpoubvfa0r\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpoubvfa0r\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpoubvfa0r'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2482588526096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588532432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588723760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588725872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588719184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588721472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588819952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588818016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588825232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588825760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588823296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482588824528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482589016384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482589014448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7871 - loss: 0.7608 - val_accuracy: 0.9860 - val_loss: 0.1920 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9554 - loss: 0.2598 - val_accuracy: 0.9875 - val_loss: 0.1501 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9679 - loss: 0.2087 - val_accuracy: 0.9887 - val_loss: 0.1289 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9703 - loss: 0.1894 - val_accuracy: 0.9896 - val_loss: 0.1160 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9729 - loss: 0.1669 - val_accuracy: 0.9869 - val_loss: 0.1087 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9735 - loss: 0.1572 - val_accuracy: 0.9898 - val_loss: 0.0971 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.1544 - val_accuracy: 0.9902 - val_loss: 0.0904 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.1345 - val_accuracy: 0.9871 - val_loss: 0.0921 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.1259 - val_accuracy: 0.9877 - val_loss: 0.0890 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.1198 - val_accuracy: 0.9881 - val_loss: 0.0844 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.1263 - val_accuracy: 0.9921 - val_loss: 0.0789 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.1149 - val_accuracy: 0.9925 - val_loss: 0.0730 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9809 - loss: 0.1130 - val_accuracy: 0.9906 - val_loss: 0.0732 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9760 - loss: 0.1225 - val_accuracy: 0.9883 - val_loss: 0.0766 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9789 - loss: 0.1087 - val_accuracy: 0.9931 - val_loss: 0.0652 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.1115 - val_accuracy: 0.9883 - val_loss: 0.0751 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.1064 - val_accuracy: 0.9892 - val_loss: 0.0762 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.1073 - val_accuracy: 0.9912 - val_loss: 0.0675 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.1024 - val_accuracy: 0.9925 - val_loss: 0.0601 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.1004 - val_accuracy: 0.9929 - val_loss: 0.0602 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9789 - loss: 0.1043 - val_accuracy: 0.9917 - val_loss: 0.0632 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9805 - loss: 0.0985 - val_accuracy: 0.9925 - val_loss: 0.0610 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9813 - loss: 0.0961 - val_accuracy: 0.9927 - val_loss: 0.0636 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9839 - loss: 0.0876 - val_accuracy: 0.9923 - val_loss: 0.0617 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0878 - val_accuracy: 0.9929 - val_loss: 0.0572 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0770 - val_accuracy: 0.9937 - val_loss: 0.0528 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0769 - val_accuracy: 0.9927 - val_loss: 0.0524 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0746 - val_accuracy: 0.9931 - val_loss: 0.0535 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0839 - val_accuracy: 0.9940 - val_loss: 0.0480 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0777 - val_accuracy: 0.9929 - val_loss: 0.0490 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0740 - val_accuracy: 0.9935 - val_loss: 0.0488 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0692 - val_accuracy: 0.9933 - val_loss: 0.0489 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0778 - val_accuracy: 0.9940 - val_loss: 0.0465 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0757 - val_accuracy: 0.9923 - val_loss: 0.0449 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0685 - val_accuracy: 0.9942 - val_loss: 0.0424 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0687 - val_accuracy: 0.9933 - val_loss: 0.0462 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.0758 - val_accuracy: 0.9917 - val_loss: 0.0491 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0673 - val_accuracy: 0.9937 - val_loss: 0.0432 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0675 - val_accuracy: 0.9925 - val_loss: 0.0498 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0705 - val_accuracy: 0.9906 - val_loss: 0.0512 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0589 - val_accuracy: 0.9940 - val_loss: 0.0421 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9890 - loss: 0.0556 - val_accuracy: 0.9937 - val_loss: 0.0408 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0545 - val_accuracy: 0.9929 - val_loss: 0.0406 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0558 - val_accuracy: 0.9952 - val_loss: 0.0383 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9893 - loss: 0.0527 - val_accuracy: 0.9935 - val_loss: 0.0385 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0571 - val_accuracy: 0.9946 - val_loss: 0.0368 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0555 - val_accuracy: 0.9950 - val_loss: 0.0361 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0558 - val_accuracy: 0.9954 - val_loss: 0.0359 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0591 - val_accuracy: 0.9952 - val_loss: 0.0345 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0533 - val_accuracy: 0.9948 - val_loss: 0.0337 - learning_rate: 2.5000e-04\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9952\n",
      "Tiempo de entrenamiento: 63.41 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Chau       1.00      1.00      1.00      1000\n",
      "     De Nada       0.99      0.99      0.99      1000\n",
      "     Gracias       1.00      1.00      1.00      1000\n",
      "        Hola       1.00      1.00      1.00      1000\n",
      "   Te Quiero       0.99      0.99      0.99      1000\n",
      "          Yo       0.99      1.00      1.00      1000\n",
      "\n",
      "    accuracy                           1.00      6000\n",
      "   macro avg       1.00      1.00      1.00      6000\n",
      "weighted avg       1.00      1.00      1.00      6000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpeoel9b4e\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpeoel9b4e\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpeoel9b4e'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2482549725968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482549727904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542661120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542663232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482549734240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542658832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542673264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542669216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542778976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542779504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542777040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542778272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542787776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542781440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7619 - loss: 0.8567 - val_accuracy: 0.9723 - val_loss: 0.2158 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9488 - loss: 0.2885 - val_accuracy: 0.9825 - val_loss: 0.1666 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9614 - loss: 0.2281 - val_accuracy: 0.9889 - val_loss: 0.1368 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9690 - loss: 0.1926 - val_accuracy: 0.9891 - val_loss: 0.1249 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.1824 - val_accuracy: 0.9909 - val_loss: 0.1150 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9744 - loss: 0.1590 - val_accuracy: 0.9804 - val_loss: 0.1333 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9721 - loss: 0.1601 - val_accuracy: 0.9914 - val_loss: 0.0973 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.1458 - val_accuracy: 0.9893 - val_loss: 0.1055 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 0.1381 - val_accuracy: 0.9911 - val_loss: 0.0917 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.1316 - val_accuracy: 0.9887 - val_loss: 0.0999 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1291 - val_accuracy: 0.9911 - val_loss: 0.0876 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.1285 - val_accuracy: 0.9912 - val_loss: 0.0874 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9752 - loss: 0.1323 - val_accuracy: 0.9909 - val_loss: 0.0840 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.1188 - val_accuracy: 0.9900 - val_loss: 0.0864 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9805 - loss: 0.1139 - val_accuracy: 0.9912 - val_loss: 0.0829 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.1148 - val_accuracy: 0.9925 - val_loss: 0.0804 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.1121 - val_accuracy: 0.9930 - val_loss: 0.0772 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.1051 - val_accuracy: 0.9921 - val_loss: 0.0813 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.1053 - val_accuracy: 0.9921 - val_loss: 0.0771 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.1132 - val_accuracy: 0.9904 - val_loss: 0.0815 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9816 - loss: 0.1046 - val_accuracy: 0.9895 - val_loss: 0.0920 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.1078 - val_accuracy: 0.9923 - val_loss: 0.0778 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0916 - val_accuracy: 0.9939 - val_loss: 0.0702 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0855 - val_accuracy: 0.9941 - val_loss: 0.0674 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9864 - loss: 0.0814 - val_accuracy: 0.9937 - val_loss: 0.0680 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.0789 - val_accuracy: 0.9925 - val_loss: 0.0744 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0848 - val_accuracy: 0.9945 - val_loss: 0.0649 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.0776 - val_accuracy: 0.9932 - val_loss: 0.0646 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0771 - val_accuracy: 0.9941 - val_loss: 0.0621 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0713 - val_accuracy: 0.9948 - val_loss: 0.0585 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0765 - val_accuracy: 0.9932 - val_loss: 0.0575 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9860 - loss: 0.0743 - val_accuracy: 0.9934 - val_loss: 0.0595 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.0723 - val_accuracy: 0.9941 - val_loss: 0.0602 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0766 - val_accuracy: 0.9948 - val_loss: 0.0554 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0756 - val_accuracy: 0.9914 - val_loss: 0.0636 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0725 - val_accuracy: 0.9929 - val_loss: 0.0596 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9854 - loss: 0.0739 - val_accuracy: 0.9939 - val_loss: 0.0526 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0718 - val_accuracy: 0.9930 - val_loss: 0.0590 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9860 - loss: 0.0714 - val_accuracy: 0.9918 - val_loss: 0.0626 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0663 - val_accuracy: 0.9925 - val_loss: 0.0615 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.0706 - val_accuracy: 0.9911 - val_loss: 0.0627 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0703 - val_accuracy: 0.9911 - val_loss: 0.0605 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0691 - val_accuracy: 0.9945 - val_loss: 0.0512 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0563 - val_accuracy: 0.9950 - val_loss: 0.0542 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0602 - val_accuracy: 0.9954 - val_loss: 0.0537 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9906 - loss: 0.0540 - val_accuracy: 0.9948 - val_loss: 0.0480 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9890 - loss: 0.0623 - val_accuracy: 0.9955 - val_loss: 0.0460 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0582 - val_accuracy: 0.9948 - val_loss: 0.0493 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0594 - val_accuracy: 0.9948 - val_loss: 0.0496 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m700/700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0611 - val_accuracy: 0.9943 - val_loss: 0.0479 - learning_rate: 2.5000e-04\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9937\n",
      "Tiempo de entrenamiento: 73.65 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cero       1.00      0.99      1.00      1000\n",
      "        Chau       1.00      0.99      0.99      1000\n",
      "     De Nada       0.99      0.99      0.99      1000\n",
      "     Gracias       0.99      0.99      0.99      1000\n",
      "        Hola       1.00      0.99      0.99      1000\n",
      "   Te Quiero       0.99      1.00      0.99      1000\n",
      "          Yo       0.99      1.00      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      7000\n",
      "   macro avg       0.99      0.99      0.99      7000\n",
      "weighted avg       0.99      0.99      0.99      7000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmph_c9f4b8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmph_c9f4b8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmph_c9f4b8'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer_2')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2482543480320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482543485248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482757368320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482757367792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482757354592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482757357936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482549423296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482549409216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482549443920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482549444976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482549448144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482549454480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482548549664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482548552656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Chau\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: De Nada\n",
      "Audio enviado: Yo\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7854 - loss: 0.8282 - val_accuracy: 0.9862 - val_loss: 0.1666 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9585 - loss: 0.2575 - val_accuracy: 0.9861 - val_loss: 0.1456 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 0.1997 - val_accuracy: 0.9892 - val_loss: 0.1215 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9697 - loss: 0.1875 - val_accuracy: 0.9928 - val_loss: 0.1060 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.1667 - val_accuracy: 0.9893 - val_loss: 0.1063 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.1515 - val_accuracy: 0.9901 - val_loss: 0.1004 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.1429 - val_accuracy: 0.9894 - val_loss: 0.0948 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.1369 - val_accuracy: 0.9906 - val_loss: 0.0882 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.1377 - val_accuracy: 0.9940 - val_loss: 0.0828 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9792 - loss: 0.1306 - val_accuracy: 0.9925 - val_loss: 0.0789 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.1208 - val_accuracy: 0.9901 - val_loss: 0.0865 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1300 - val_accuracy: 0.9918 - val_loss: 0.0809 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9787 - loss: 0.1218 - val_accuracy: 0.9925 - val_loss: 0.0751 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9786 - loss: 0.1210 - val_accuracy: 0.9933 - val_loss: 0.0717 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.1137 - val_accuracy: 0.9926 - val_loss: 0.0692 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.1149 - val_accuracy: 0.9926 - val_loss: 0.0686 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.1105 - val_accuracy: 0.9932 - val_loss: 0.0731 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.1164 - val_accuracy: 0.9935 - val_loss: 0.0663 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.1264 - val_accuracy: 0.9912 - val_loss: 0.0761 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.1112 - val_accuracy: 0.9928 - val_loss: 0.0677 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.1111 - val_accuracy: 0.9892 - val_loss: 0.0775 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.1140 - val_accuracy: 0.9932 - val_loss: 0.0637 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1173 - val_accuracy: 0.9946 - val_loss: 0.0655 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.1017 - val_accuracy: 0.9924 - val_loss: 0.0702 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.1086 - val_accuracy: 0.9939 - val_loss: 0.0655 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9798 - loss: 0.1086 - val_accuracy: 0.9924 - val_loss: 0.0684 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.1093 - val_accuracy: 0.9936 - val_loss: 0.0650 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0981 - val_accuracy: 0.9954 - val_loss: 0.0565 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0825 - val_accuracy: 0.9949 - val_loss: 0.0577 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0824 - val_accuracy: 0.9950 - val_loss: 0.0530 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0787 - val_accuracy: 0.9950 - val_loss: 0.0525 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9873 - loss: 0.0744 - val_accuracy: 0.9953 - val_loss: 0.0487 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0791 - val_accuracy: 0.9962 - val_loss: 0.0482 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0753 - val_accuracy: 0.9935 - val_loss: 0.0536 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0754 - val_accuracy: 0.9967 - val_loss: 0.0454 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0755 - val_accuracy: 0.9949 - val_loss: 0.0472 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0742 - val_accuracy: 0.9943 - val_loss: 0.0477 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0745 - val_accuracy: 0.9940 - val_loss: 0.0499 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0767 - val_accuracy: 0.9931 - val_loss: 0.0494 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0703 - val_accuracy: 0.9942 - val_loss: 0.0472 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9890 - loss: 0.0655 - val_accuracy: 0.9961 - val_loss: 0.0425 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9897 - loss: 0.0599 - val_accuracy: 0.9962 - val_loss: 0.0400 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0670 - val_accuracy: 0.9969 - val_loss: 0.0413 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.0573 - val_accuracy: 0.9954 - val_loss: 0.0418 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0609 - val_accuracy: 0.9968 - val_loss: 0.0384 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9897 - loss: 0.0580 - val_accuracy: 0.9962 - val_loss: 0.0398 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9887 - loss: 0.0609 - val_accuracy: 0.9971 - val_loss: 0.0371 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9918 - loss: 0.0545 - val_accuracy: 0.9961 - val_loss: 0.0379 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9914 - loss: 0.0527 - val_accuracy: 0.9951 - val_loss: 0.0407 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m900/900\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0532 - val_accuracy: 0.9968 - val_loss: 0.0371 - learning_rate: 2.5000e-04\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9950\n",
      "Tiempo de entrenamiento: 92.17 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cero       1.00      1.00      1.00      1000\n",
      "        Chau       1.00      0.99      0.99      1000\n",
      "     De Nada       0.99      0.99      0.99      1000\n",
      "         Dos       1.00      1.00      1.00      1000\n",
      "     Gracias       1.00      1.00      1.00      1000\n",
      "        Hola       0.99      0.99      0.99      1000\n",
      "   Te Quiero       0.99      0.99      0.99      1000\n",
      "         Uno       1.00      1.00      1.00      1000\n",
      "          Yo       0.99      0.99      0.99      1000\n",
      "\n",
      "    accuracy                           0.99      9000\n",
      "   macro avg       1.00      0.99      1.00      9000\n",
      "weighted avg       1.00      0.99      1.00      9000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpl16ox7u0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpl16ox7u0\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpl16ox7u0'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer_3')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 9), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2482829910864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830396224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830407136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830409248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830402560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830404848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830503328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830501392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830502976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830501744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830506672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482829108224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830631936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830630000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Cero\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "\n",
      "--- Señas Guardadas ---\n",
      "1. Te Quiero\n",
      "2. Hola\n",
      "3. Cero\n",
      "4. Gracias\n",
      "5. Chau\n",
      "6. De Nada\n",
      "7. Dos\n",
      "8. Uno\n",
      "9. Yo\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "\n",
      "--- Señas Guardadas ---\n",
      "1. Te Quiero\n",
      "2. Hola\n",
      "3. Cero\n",
      "4. Gracias\n",
      "5. Chau\n",
      "6. De Nada\n",
      "7. Dos\n",
      "8. Uno\n",
      "9. Yo\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7227 - loss: 1.0492 - val_accuracy: 0.9724 - val_loss: 0.2111 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9445 - loss: 0.3117 - val_accuracy: 0.9861 - val_loss: 0.1573 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.2381 - val_accuracy: 0.9834 - val_loss: 0.1502 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9644 - loss: 0.2165 - val_accuracy: 0.9880 - val_loss: 0.1283 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.2058 - val_accuracy: 0.9865 - val_loss: 0.1232 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.1900 - val_accuracy: 0.9882 - val_loss: 0.1141 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1779 - val_accuracy: 0.9873 - val_loss: 0.1086 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9699 - loss: 0.1646 - val_accuracy: 0.9864 - val_loss: 0.1086 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.1655 - val_accuracy: 0.9902 - val_loss: 0.0995 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9721 - loss: 0.1541 - val_accuracy: 0.9853 - val_loss: 0.1107 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9716 - loss: 0.1546 - val_accuracy: 0.9881 - val_loss: 0.0985 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9732 - loss: 0.1476 - val_accuracy: 0.9906 - val_loss: 0.0882 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.1494 - val_accuracy: 0.9893 - val_loss: 0.0893 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9737 - loss: 0.1430 - val_accuracy: 0.9858 - val_loss: 0.1051 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.1490 - val_accuracy: 0.9892 - val_loss: 0.0922 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9719 - loss: 0.1478 - val_accuracy: 0.9905 - val_loss: 0.0879 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9729 - loss: 0.1431 - val_accuracy: 0.9912 - val_loss: 0.0832 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 0.1437 - val_accuracy: 0.9893 - val_loss: 0.0856 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9754 - loss: 0.1381 - val_accuracy: 0.9906 - val_loss: 0.0865 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9748 - loss: 0.1383 - val_accuracy: 0.9905 - val_loss: 0.0833 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.1388 - val_accuracy: 0.9900 - val_loss: 0.0884 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9724 - loss: 0.1429 - val_accuracy: 0.9905 - val_loss: 0.0835 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.1180 - val_accuracy: 0.9912 - val_loss: 0.0745 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.1090 - val_accuracy: 0.9914 - val_loss: 0.0758 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.1090 - val_accuracy: 0.9924 - val_loss: 0.0700 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.1008 - val_accuracy: 0.9916 - val_loss: 0.0684 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0977 - val_accuracy: 0.9911 - val_loss: 0.0688 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.1070 - val_accuracy: 0.9915 - val_loss: 0.0682 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9830 - loss: 0.0961 - val_accuracy: 0.9916 - val_loss: 0.0660 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.1018 - val_accuracy: 0.9914 - val_loss: 0.0666 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.0978 - val_accuracy: 0.9922 - val_loss: 0.0628 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.0991 - val_accuracy: 0.9920 - val_loss: 0.0620 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0972 - val_accuracy: 0.9927 - val_loss: 0.0614 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9812 - loss: 0.0989 - val_accuracy: 0.9928 - val_loss: 0.0580 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0935 - val_accuracy: 0.9931 - val_loss: 0.0597 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9810 - loss: 0.0994 - val_accuracy: 0.9917 - val_loss: 0.0609 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0999 - val_accuracy: 0.9923 - val_loss: 0.0587 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9827 - loss: 0.0930 - val_accuracy: 0.9924 - val_loss: 0.0581 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0993 - val_accuracy: 0.9920 - val_loss: 0.0611 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9855 - loss: 0.0831 - val_accuracy: 0.9931 - val_loss: 0.0533 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9845 - loss: 0.0812 - val_accuracy: 0.9936 - val_loss: 0.0545 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0788 - val_accuracy: 0.9937 - val_loss: 0.0519 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9848 - loss: 0.0835 - val_accuracy: 0.9936 - val_loss: 0.0517 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0758 - val_accuracy: 0.9935 - val_loss: 0.0532 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0738 - val_accuracy: 0.9944 - val_loss: 0.0491 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0723 - val_accuracy: 0.9910 - val_loss: 0.0568 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0720 - val_accuracy: 0.9925 - val_loss: 0.0532 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9864 - loss: 0.0733 - val_accuracy: 0.9936 - val_loss: 0.0482 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0741 - val_accuracy: 0.9940 - val_loss: 0.0490 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0714 - val_accuracy: 0.9941 - val_loss: 0.0461 - learning_rate: 2.5000e-04\n",
      "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9924\n",
      "Tiempo de entrenamiento: 112.34 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cero       1.00      1.00      1.00      1000\n",
      "        Chau       1.00      0.99      1.00      1000\n",
      "      Cuatro       0.98      0.99      0.98      1000\n",
      "     De Nada       0.99      0.98      0.98      1000\n",
      "         Dos       1.00      1.00      1.00      1000\n",
      "     Gracias       0.99      0.99      0.99      1000\n",
      "        Hola       0.99      1.00      0.99      1000\n",
      "   Te Quiero       0.98      1.00      0.99      1000\n",
      "        Tres       1.00      0.99      0.99      1000\n",
      "         Uno       1.00      1.00      1.00      1000\n",
      "          Yo       0.99      1.00      0.99      1000\n",
      "\n",
      "    accuracy                           0.99     11000\n",
      "   macro avg       0.99      0.99      0.99     11000\n",
      "weighted avg       0.99      0.99      0.99     11000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpmdooc1rz\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpmdooc1rz\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpmdooc1rz'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer_4')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 11), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2482830633520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482898383648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482793309008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482793898656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482898376784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482898376256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482793891264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482793893728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482705820208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482705819856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482705822848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482705816336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482543482256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482543491232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Chau\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7086 - loss: 1.1298 - val_accuracy: 0.9688 - val_loss: 0.2308 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9325 - loss: 0.3528 - val_accuracy: 0.9811 - val_loss: 0.1739 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9454 - loss: 0.2872 - val_accuracy: 0.9807 - val_loss: 0.1623 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9526 - loss: 0.2528 - val_accuracy: 0.9782 - val_loss: 0.1614 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9609 - loss: 0.2198 - val_accuracy: 0.9854 - val_loss: 0.1324 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9602 - loss: 0.2144 - val_accuracy: 0.9856 - val_loss: 0.1255 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.2034 - val_accuracy: 0.9818 - val_loss: 0.1346 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9633 - loss: 0.1988 - val_accuracy: 0.9832 - val_loss: 0.1236 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.2026 - val_accuracy: 0.9855 - val_loss: 0.1137 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9650 - loss: 0.1874 - val_accuracy: 0.9864 - val_loss: 0.1110 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1804 - val_accuracy: 0.9846 - val_loss: 0.1117 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9679 - loss: 0.1764 - val_accuracy: 0.9858 - val_loss: 0.1133 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1780 - val_accuracy: 0.9855 - val_loss: 0.1115 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9661 - loss: 0.1776 - val_accuracy: 0.9871 - val_loss: 0.1034 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1761 - val_accuracy: 0.9882 - val_loss: 0.1075 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9661 - loss: 0.1755 - val_accuracy: 0.9856 - val_loss: 0.1133 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.1765 - val_accuracy: 0.9883 - val_loss: 0.1018 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9692 - loss: 0.1648 - val_accuracy: 0.9848 - val_loss: 0.1085 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1714 - val_accuracy: 0.9866 - val_loss: 0.1048 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9702 - loss: 0.1632 - val_accuracy: 0.9868 - val_loss: 0.1018 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1720 - val_accuracy: 0.9839 - val_loss: 0.1120 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.1720 - val_accuracy: 0.9870 - val_loss: 0.1008 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1656 - val_accuracy: 0.9859 - val_loss: 0.1020 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9652 - loss: 0.1718 - val_accuracy: 0.9867 - val_loss: 0.1040 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9685 - loss: 0.1625 - val_accuracy: 0.9864 - val_loss: 0.0981 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1659 - val_accuracy: 0.9850 - val_loss: 0.1032 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.1619 - val_accuracy: 0.9866 - val_loss: 0.1016 - learning_rate: 0.0010\n",
      "Epoch 28/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.1618 - val_accuracy: 0.9875 - val_loss: 0.0994 - learning_rate: 0.0010\n",
      "Epoch 29/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9700 - loss: 0.1598 - val_accuracy: 0.9853 - val_loss: 0.1048 - learning_rate: 0.0010\n",
      "Epoch 30/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.1632 - val_accuracy: 0.9847 - val_loss: 0.1082 - learning_rate: 0.0010\n",
      "Epoch 31/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.1397 - val_accuracy: 0.9884 - val_loss: 0.0922 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.1322 - val_accuracy: 0.9904 - val_loss: 0.0800 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 0.1292 - val_accuracy: 0.9901 - val_loss: 0.0822 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.1178 - val_accuracy: 0.9899 - val_loss: 0.0801 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1228 - val_accuracy: 0.9911 - val_loss: 0.0761 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.1183 - val_accuracy: 0.9887 - val_loss: 0.0795 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.1212 - val_accuracy: 0.9888 - val_loss: 0.0797 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.1193 - val_accuracy: 0.9891 - val_loss: 0.0784 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9772 - loss: 0.1212 - val_accuracy: 0.9872 - val_loss: 0.0808 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1213 - val_accuracy: 0.9889 - val_loss: 0.0717 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.1132 - val_accuracy: 0.9897 - val_loss: 0.0719 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.1183 - val_accuracy: 0.9897 - val_loss: 0.0734 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1158 - val_accuracy: 0.9902 - val_loss: 0.0722 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 0.1214 - val_accuracy: 0.9912 - val_loss: 0.0686 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.1163 - val_accuracy: 0.9916 - val_loss: 0.0665 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.1182 - val_accuracy: 0.9894 - val_loss: 0.0719 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.1150 - val_accuracy: 0.9897 - val_loss: 0.0716 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.1160 - val_accuracy: 0.9905 - val_loss: 0.0722 - learning_rate: 5.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9799 - loss: 0.1079 - val_accuracy: 0.9896 - val_loss: 0.0719 - learning_rate: 5.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1300/1300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.1108 - val_accuracy: 0.9887 - val_loss: 0.0721 - learning_rate: 5.0000e-04\n",
      "\u001b[1m407/407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9902\n",
      "Tiempo de entrenamiento: 129.61 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cero       0.99      1.00      1.00      1000\n",
      "        Chau       0.99      0.99      0.99      1000\n",
      "       Cinco       1.00      0.98      0.99      1000\n",
      "      Cuatro       0.96      0.99      0.98      1000\n",
      "     De Nada       0.99      0.99      0.99      1000\n",
      "         Dos       1.00      1.00      1.00      1000\n",
      "     Gracias       0.99      0.99      0.99      1000\n",
      "        Hola       0.99      0.99      0.99      1000\n",
      "        Seis       0.99      0.98      0.99      1000\n",
      "   Te Quiero       0.99      1.00      0.99      1000\n",
      "        Tres       1.00      0.98      0.99      1000\n",
      "         Uno       1.00      1.00      1.00      1000\n",
      "          Yo       0.97      0.99      0.98      1000\n",
      "\n",
      "    accuracy                           0.99     13000\n",
      "   macro avg       0.99      0.99      0.99     13000\n",
      "weighted avg       0.99      0.99      0.99     13000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpr79tl2vj\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpr79tl2vj\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpr79tl2vj'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer_5')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 13), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2482999250608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483001839808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483001850896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483001857776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483001846144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483001848608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483001865344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483001863408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483002394928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483002393168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483002392992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483002394224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483002405664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483002403728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Seis\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Seis\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Tres\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Chau\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Seis\n",
      "Audio enviado: Yo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Chau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Gracias\n",
      "Audio enviado: Chau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6939 - loss: 1.1621 - val_accuracy: 0.9711 - val_loss: 0.2261 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9295 - loss: 0.3534 - val_accuracy: 0.9741 - val_loss: 0.1910 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9493 - loss: 0.2797 - val_accuracy: 0.9586 - val_loss: 0.2063 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.2487 - val_accuracy: 0.9836 - val_loss: 0.1444 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9552 - loss: 0.2348 - val_accuracy: 0.9817 - val_loss: 0.1465 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9578 - loss: 0.2247 - val_accuracy: 0.9862 - val_loss: 0.1266 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9618 - loss: 0.2066 - val_accuracy: 0.9822 - val_loss: 0.1350 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1981 - val_accuracy: 0.9847 - val_loss: 0.1289 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1965 - val_accuracy: 0.9846 - val_loss: 0.1219 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9643 - loss: 0.1882 - val_accuracy: 0.9796 - val_loss: 0.1292 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9656 - loss: 0.1837 - val_accuracy: 0.9800 - val_loss: 0.1238 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9658 - loss: 0.1788 - val_accuracy: 0.9837 - val_loss: 0.1159 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9671 - loss: 0.1773 - val_accuracy: 0.9844 - val_loss: 0.1162 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.1744 - val_accuracy: 0.9872 - val_loss: 0.1076 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9634 - loss: 0.1897 - val_accuracy: 0.9855 - val_loss: 0.1095 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9671 - loss: 0.1769 - val_accuracy: 0.9848 - val_loss: 0.1140 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9655 - loss: 0.1788 - val_accuracy: 0.9846 - val_loss: 0.1131 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.1716 - val_accuracy: 0.9871 - val_loss: 0.1064 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9696 - loss: 0.1689 - val_accuracy: 0.9860 - val_loss: 0.1039 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9661 - loss: 0.1748 - val_accuracy: 0.9806 - val_loss: 0.1172 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1693 - val_accuracy: 0.9851 - val_loss: 0.1034 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9655 - loss: 0.1729 - val_accuracy: 0.9846 - val_loss: 0.1058 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9691 - loss: 0.1662 - val_accuracy: 0.9849 - val_loss: 0.1087 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1746 - val_accuracy: 0.9850 - val_loss: 0.1078 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.1656 - val_accuracy: 0.9854 - val_loss: 0.1086 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9659 - loss: 0.1771 - val_accuracy: 0.9854 - val_loss: 0.1044 - learning_rate: 0.0010\n",
      "Epoch 27/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9748 - loss: 0.1453 - val_accuracy: 0.9885 - val_loss: 0.0884 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9765 - loss: 0.1343 - val_accuracy: 0.9882 - val_loss: 0.0876 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.9769 - loss: 0.1313 - val_accuracy: 0.9877 - val_loss: 0.0872 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.9762 - loss: 0.1311 - val_accuracy: 0.9884 - val_loss: 0.0845 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9766 - loss: 0.1278 - val_accuracy: 0.9897 - val_loss: 0.0776 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.1229 - val_accuracy: 0.9880 - val_loss: 0.0803 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9760 - loss: 0.1282 - val_accuracy: 0.9888 - val_loss: 0.0789 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.1252 - val_accuracy: 0.9879 - val_loss: 0.0786 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.1258 - val_accuracy: 0.9896 - val_loss: 0.0778 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.1259 - val_accuracy: 0.9898 - val_loss: 0.0773 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1222 - val_accuracy: 0.9893 - val_loss: 0.0767 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9764 - loss: 0.1251 - val_accuracy: 0.9894 - val_loss: 0.0746 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9756 - loss: 0.1263 - val_accuracy: 0.9913 - val_loss: 0.0693 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.1158 - val_accuracy: 0.9886 - val_loss: 0.0759 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1183 - val_accuracy: 0.9891 - val_loss: 0.0736 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.1146 - val_accuracy: 0.9905 - val_loss: 0.0676 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.1244 - val_accuracy: 0.9887 - val_loss: 0.0738 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 0.1223 - val_accuracy: 0.9896 - val_loss: 0.0691 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9773 - loss: 0.1163 - val_accuracy: 0.9893 - val_loss: 0.0696 - learning_rate: 5.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9772 - loss: 0.1153 - val_accuracy: 0.9887 - val_loss: 0.0715 - learning_rate: 5.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9764 - loss: 0.1168 - val_accuracy: 0.9907 - val_loss: 0.0703 - learning_rate: 5.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.1039 - val_accuracy: 0.9914 - val_loss: 0.0645 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.1008 - val_accuracy: 0.9914 - val_loss: 0.0642 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1400/1400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0935 - val_accuracy: 0.9926 - val_loss: 0.0600 - learning_rate: 2.5000e-04\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9925\n",
      "Tiempo de entrenamiento: 179.37 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cero       1.00      1.00      1.00      1000\n",
      "        Chau       1.00      0.99      0.99      1000\n",
      "       Cinco       0.99      0.98      0.99      1000\n",
      "      Cuatro       0.98      0.99      0.98      1000\n",
      "     De Nada       0.99      0.99      0.99      1000\n",
      "         Dos       1.00      1.00      1.00      1000\n",
      "     Gracias       0.99      0.99      0.99      1000\n",
      "        Hola       0.99      0.99      0.99      1000\n",
      "        Seis       1.00      0.99      0.99      1000\n",
      "       Siete       1.00      0.99      1.00      1000\n",
      "   Te Quiero       0.99      1.00      0.99      1000\n",
      "        Tres       1.00      0.99      0.99      1000\n",
      "         Uno       1.00      1.00      1.00      1000\n",
      "          Yo       0.96      1.00      0.98      1000\n",
      "\n",
      "    accuracy                           0.99     14000\n",
      "   macro avg       0.99      0.99      0.99     14000\n",
      "weighted avg       0.99      0.99      0.99     14000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpn2ojgdwi\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpn2ojgdwi\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpn2ojgdwi'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer_6')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 14), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2483049988160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483049993264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050004000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050003472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483049999600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050001888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050130848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050128912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050122224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050134016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050134192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050266688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050278128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483050276192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Siete\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Tres\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Seis\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Seis\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Chau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7204 - loss: 1.1166 - val_accuracy: 0.9712 - val_loss: 0.2169 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9361 - loss: 0.3354 - val_accuracy: 0.9756 - val_loss: 0.1822 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9484 - loss: 0.2699 - val_accuracy: 0.9817 - val_loss: 0.1523 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9572 - loss: 0.2380 - val_accuracy: 0.9824 - val_loss: 0.1448 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9591 - loss: 0.2201 - val_accuracy: 0.9866 - val_loss: 0.1281 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.2121 - val_accuracy: 0.9837 - val_loss: 0.1294 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9616 - loss: 0.2038 - val_accuracy: 0.9826 - val_loss: 0.1322 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9641 - loss: 0.1912 - val_accuracy: 0.9834 - val_loss: 0.1214 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9651 - loss: 0.1904 - val_accuracy: 0.9870 - val_loss: 0.1115 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9655 - loss: 0.1845 - val_accuracy: 0.9868 - val_loss: 0.1115 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1846 - val_accuracy: 0.9788 - val_loss: 0.1325 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1771 - val_accuracy: 0.9850 - val_loss: 0.1135 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9650 - loss: 0.1782 - val_accuracy: 0.9872 - val_loss: 0.1055 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9650 - loss: 0.1813 - val_accuracy: 0.9871 - val_loss: 0.1070 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9681 - loss: 0.1662 - val_accuracy: 0.9879 - val_loss: 0.1075 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9698 - loss: 0.1644 - val_accuracy: 0.9804 - val_loss: 0.1321 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.1690 - val_accuracy: 0.9862 - val_loss: 0.1108 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.1703 - val_accuracy: 0.9835 - val_loss: 0.1118 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.1411 - val_accuracy: 0.9900 - val_loss: 0.0895 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.1366 - val_accuracy: 0.9901 - val_loss: 0.0854 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.1302 - val_accuracy: 0.9895 - val_loss: 0.0860 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9757 - loss: 0.1348 - val_accuracy: 0.9884 - val_loss: 0.0861 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.1309 - val_accuracy: 0.9896 - val_loss: 0.0842 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1224 - val_accuracy: 0.9900 - val_loss: 0.0822 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1226 - val_accuracy: 0.9898 - val_loss: 0.0795 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1201 - val_accuracy: 0.9871 - val_loss: 0.0869 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.1198 - val_accuracy: 0.9891 - val_loss: 0.0776 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.1210 - val_accuracy: 0.9893 - val_loss: 0.0760 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.1193 - val_accuracy: 0.9906 - val_loss: 0.0757 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.1173 - val_accuracy: 0.9900 - val_loss: 0.0795 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.1181 - val_accuracy: 0.9903 - val_loss: 0.0740 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.1129 - val_accuracy: 0.9888 - val_loss: 0.0772 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.1166 - val_accuracy: 0.9902 - val_loss: 0.0730 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.1226 - val_accuracy: 0.9892 - val_loss: 0.0770 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.1149 - val_accuracy: 0.9912 - val_loss: 0.0727 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.1192 - val_accuracy: 0.9885 - val_loss: 0.0789 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.1188 - val_accuracy: 0.9893 - val_loss: 0.0752 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.1160 - val_accuracy: 0.9895 - val_loss: 0.0736 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9765 - loss: 0.1164 - val_accuracy: 0.9892 - val_loss: 0.0755 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9784 - loss: 0.1143 - val_accuracy: 0.9908 - val_loss: 0.0736 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9815 - loss: 0.1033 - val_accuracy: 0.9911 - val_loss: 0.0664 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0931 - val_accuracy: 0.9912 - val_loss: 0.0650 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9838 - loss: 0.0911 - val_accuracy: 0.9923 - val_loss: 0.0638 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0934 - val_accuracy: 0.9919 - val_loss: 0.0624 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9837 - loss: 0.0903 - val_accuracy: 0.9924 - val_loss: 0.0610 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0894 - val_accuracy: 0.9912 - val_loss: 0.0628 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0935 - val_accuracy: 0.9914 - val_loss: 0.0631 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0910 - val_accuracy: 0.9922 - val_loss: 0.0595 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9827 - loss: 0.0906 - val_accuracy: 0.9912 - val_loss: 0.0606 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9834 - loss: 0.0862 - val_accuracy: 0.9912 - val_loss: 0.0591 - learning_rate: 2.5000e-04\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 936us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9902\n",
      "Tiempo de entrenamiento: 152.24 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cero       1.00      0.99      0.99      1000\n",
      "        Chau       1.00      0.99      0.99      1000\n",
      "       Cinco       0.99      0.98      0.99      1000\n",
      "      Cuatro       0.96      0.99      0.97      1000\n",
      "     De Nada       0.99      0.98      0.99      1000\n",
      "         Dos       1.00      1.00      1.00      1000\n",
      "     Gracias       1.00      0.99      0.99      1000\n",
      "        Hola       0.99      0.99      0.99      1000\n",
      "        Ocho       1.00      1.00      1.00      1000\n",
      "        Seis       0.99      0.98      0.99      1000\n",
      "       Siete       1.00      0.99      0.99      1000\n",
      "   Te Quiero       0.99      0.99      0.99      1000\n",
      "        Tres       1.00      0.98      0.99      1000\n",
      "         Uno       1.00      1.00      1.00      1000\n",
      "          Yo       0.96      0.99      0.98      1000\n",
      "\n",
      "    accuracy                           0.99     15000\n",
      "   macro avg       0.99      0.99      0.99     15000\n",
      "weighted avg       0.99      0.99      0.99     15000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmp5sv5v6u4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmp5sv5v6u4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmp5sv5v6u4'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer_7')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 15), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2482543482608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482542772992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482493796448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482999116368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2484461820336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2484461822448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482999117248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482898374672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482898381888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482898373440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482898376432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482898376080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482899542512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482830634224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Ocho\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Yo\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Seis\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Ocho\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Ocho\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Ocho\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Ocho\n",
      "Audio enviado: Chau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Ocho\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Ocho\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Cuatro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Cámara UDP iniciada para recolección.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7098 - loss: 1.1547 - val_accuracy: 0.9719 - val_loss: 0.2166 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9420 - loss: 0.3191 - val_accuracy: 0.9808 - val_loss: 0.1659 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9506 - loss: 0.2666 - val_accuracy: 0.9833 - val_loss: 0.1521 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9550 - loss: 0.2466 - val_accuracy: 0.9856 - val_loss: 0.1343 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9606 - loss: 0.2163 - val_accuracy: 0.9855 - val_loss: 0.1289 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.1985 - val_accuracy: 0.9819 - val_loss: 0.1340 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9596 - loss: 0.2096 - val_accuracy: 0.9875 - val_loss: 0.1152 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9636 - loss: 0.1927 - val_accuracy: 0.9841 - val_loss: 0.1200 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.1869 - val_accuracy: 0.9871 - val_loss: 0.1115 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9659 - loss: 0.1847 - val_accuracy: 0.9834 - val_loss: 0.1167 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.1746 - val_accuracy: 0.9852 - val_loss: 0.1141 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1786 - val_accuracy: 0.9860 - val_loss: 0.1146 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9645 - loss: 0.1844 - val_accuracy: 0.9843 - val_loss: 0.1128 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1763 - val_accuracy: 0.9871 - val_loss: 0.1131 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.1535 - val_accuracy: 0.9898 - val_loss: 0.0913 - learning_rate: 5.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9754 - loss: 0.1397 - val_accuracy: 0.9912 - val_loss: 0.0853 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.1354 - val_accuracy: 0.9898 - val_loss: 0.0829 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9751 - loss: 0.1362 - val_accuracy: 0.9907 - val_loss: 0.0813 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9761 - loss: 0.1310 - val_accuracy: 0.9905 - val_loss: 0.0783 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9766 - loss: 0.1275 - val_accuracy: 0.9901 - val_loss: 0.0761 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.1256 - val_accuracy: 0.9902 - val_loss: 0.0768 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9756 - loss: 0.1266 - val_accuracy: 0.9912 - val_loss: 0.0765 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.1222 - val_accuracy: 0.9909 - val_loss: 0.0723 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1227 - val_accuracy: 0.9896 - val_loss: 0.0752 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9753 - loss: 0.1272 - val_accuracy: 0.9902 - val_loss: 0.0747 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1197 - val_accuracy: 0.9905 - val_loss: 0.0707 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9764 - loss: 0.1200 - val_accuracy: 0.9900 - val_loss: 0.0741 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.1163 - val_accuracy: 0.9903 - val_loss: 0.0724 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.1201 - val_accuracy: 0.9907 - val_loss: 0.0710 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.1141 - val_accuracy: 0.9864 - val_loss: 0.0865 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.1171 - val_accuracy: 0.9910 - val_loss: 0.0704 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.1199 - val_accuracy: 0.9903 - val_loss: 0.0685 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9772 - loss: 0.1174 - val_accuracy: 0.9907 - val_loss: 0.0716 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9762 - loss: 0.1203 - val_accuracy: 0.9912 - val_loss: 0.0698 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9759 - loss: 0.1189 - val_accuracy: 0.9909 - val_loss: 0.0713 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1173 - val_accuracy: 0.9899 - val_loss: 0.0667 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.1201 - val_accuracy: 0.9915 - val_loss: 0.0678 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9764 - loss: 0.1233 - val_accuracy: 0.9919 - val_loss: 0.0689 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9773 - loss: 0.1154 - val_accuracy: 0.9913 - val_loss: 0.0680 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.1154 - val_accuracy: 0.9901 - val_loss: 0.0693 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9774 - loss: 0.1178 - val_accuracy: 0.9905 - val_loss: 0.0688 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0987 - val_accuracy: 0.9909 - val_loss: 0.0661 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0984 - val_accuracy: 0.9912 - val_loss: 0.0627 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0938 - val_accuracy: 0.9930 - val_loss: 0.0582 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0964 - val_accuracy: 0.9921 - val_loss: 0.0613 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.0916 - val_accuracy: 0.9912 - val_loss: 0.0605 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9822 - loss: 0.0926 - val_accuracy: 0.9924 - val_loss: 0.0588 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9827 - loss: 0.0942 - val_accuracy: 0.9923 - val_loss: 0.0564 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9815 - loss: 0.0951 - val_accuracy: 0.9934 - val_loss: 0.0582 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m1600/1600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9834 - loss: 0.0897 - val_accuracy: 0.9931 - val_loss: 0.0550 - learning_rate: 2.5000e-04\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 888us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informe del Modelo ---\n",
      "Precisión en datos de prueba: 0.9918\n",
      "Tiempo de entrenamiento: 161.91 segundos\n",
      "\n",
      "Clasificación detallada:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cero       1.00      1.00      1.00      1000\n",
      "        Chau       1.00      0.99      0.99      1000\n",
      "       Cinco       1.00      0.98      0.99      1000\n",
      "      Cuatro       0.96      0.99      0.97      1000\n",
      "     De Nada       1.00      0.98      0.99      1000\n",
      "         Dos       1.00      0.99      1.00      1000\n",
      "     Gracias       0.98      0.99      0.99      1000\n",
      "        Hola       0.99      1.00      0.99      1000\n",
      "       Nueve       1.00      1.00      1.00      1000\n",
      "        Ocho       1.00      1.00      1.00      1000\n",
      "        Seis       0.99      0.99      0.99      1000\n",
      "       Siete       1.00      0.99      0.99      1000\n",
      "   Te Quiero       0.99      1.00      0.99      1000\n",
      "        Tres       1.00      0.98      0.99      1000\n",
      "         Uno       1.00      1.00      1.00      1000\n",
      "          Yo       0.98      0.99      0.98      1000\n",
      "\n",
      "    accuracy                           0.99     16000\n",
      "   macro avg       0.99      0.99      0.99     16000\n",
      "weighted avg       0.99      0.99      0.99     16000\n",
      "\n",
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpq0bvv2z3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpq0bvv2z3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpq0bvv2z3'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer_8')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 16), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2483140261168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140262928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140404928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140407040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140400352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140402640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140484736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140482800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140490016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140490544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140488080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2483140489312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482999909840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2482999907904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "convertido a Tflite para evaluacion en tiemop real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Ocho\n",
      "Audio enviado: Nueve\n",
      "Audio enviado: Ocho\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Seis\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Yo\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Gracias\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cero\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Dos\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Cuatro\n",
      "Audio enviado: Cinco\n",
      "Audio enviado: Tres\n",
      "Audio enviado: Seis\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Ocho\n",
      "Audio enviado: Nueve\n",
      "Audio enviado: Chau\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Chau\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000024275A00430>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_6060\\63139822.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Audio enviado: Cuatro\n",
      "Saliendo del programa...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #run()\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
