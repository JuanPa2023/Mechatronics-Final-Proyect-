{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd78588",
   "metadata": {},
   "source": [
    "# ------------- PROYECTO FINAL G & S--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067e16c",
   "metadata": {},
   "source": [
    "## LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a1bf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Comunicación y cámara\n",
    "import socket\n",
    "import queue\n",
    "\n",
    "# Voz\n",
    "import pyttsx3\n",
    "import threading\n",
    "\n",
    "# Módulos para reconocimiento de voz\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "import io\n",
    "import wave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd329ca",
   "metadata": {},
   "source": [
    "## UDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0df331d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para el reconocimiento de voz\n",
    "SAMPLE_RATE_IN = 48000  # Tasa del micrófono INMP441\n",
    "SAMPLE_RATE_OUT = 16000  # Tasa requerida por la API de reconocimiento\n",
    "BUFFER_DURATION = 5  # segundos\n",
    "\n",
    "\n",
    "UDP_IP_PI = \"192.168.7.2\"  # IP de la Raspberry Pi\n",
    "UDP_OPEN = '0.0.0.0'\n",
    "\n",
    "# Puertos para diferentes servicios\n",
    "UDP_PORT_MICROFONO = 5006\n",
    "UDP_PORT_TEXT = 5005\n",
    "UDP_PORT_SERVO = 5001  # Puerto para enviar comandos\n",
    "UDP_PORT_PARLANTE = 5003\n",
    "UDP_PORT_CAM = 5002  # Puerto para recibir video\n",
    "MAX_PACKET_SIZE = 1400  # Tamaño máximo del paquete UDP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f43af6",
   "metadata": {},
   "source": [
    "## VOZ A PANTALLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c1645",
   "metadata": {},
   "source": [
    "### RECONOCEDOR DE VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a852357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el reconocedor de voz\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Configuración UDP para voz\n",
    "sock_voice = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "microfono_queue = queue.Queue()\n",
    "\n",
    "# Variable para controlar el servicio de reconocimiento de voz\n",
    "speech_recognition_running = True #VA EN EL WHILE.\n",
    "\n",
    "# Variable para almacenar la última transcripción\n",
    "last_transcription = \"\" #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "\n",
    "def recibir_audio():\n",
    "    sock_audio = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    try:\n",
    "        sock_audio.bind((\"0.0.0.0\", UDP_PORT_MICROFONO))\n",
    "        \n",
    "        buffer = bytearray()\n",
    "        bytes_needed = SAMPLE_RATE_IN * 4 * BUFFER_DURATION  # 4 bytes por muestra (32-bit)\n",
    "        \n",
    "        while speech_recognition_running:\n",
    "            data, _ = sock_audio.recvfrom(4096)\n",
    "            buffer.extend(data)\n",
    "            \n",
    "            while len(buffer) >= bytes_needed:\n",
    "                # Extraer 5 segundos de audio\n",
    "                chunk = bytes(buffer[:bytes_needed])\n",
    "                del buffer[:bytes_needed]\n",
    "                \n",
    "                # Convertir a formato numpy\n",
    "                audio_int32 = np.frombuffer(chunk, dtype=np.int32)\n",
    "                audio_float32 = audio_int32.astype(np.float32) / 2**31\n",
    "                \n",
    "                # Remuestrear a 16kHz\n",
    "                audio_16k = librosa.resample(\n",
    "                    audio_float32,\n",
    "                    orig_sr=SAMPLE_RATE_IN,\n",
    "                    target_sr=SAMPLE_RATE_OUT\n",
    "                )\n",
    "                \n",
    "                # Convertir a int16 para la API de reconocimiento\n",
    "                audio_int16 = (audio_16k * 32767).astype(np.int16)\n",
    "                \n",
    "                # Crear un archivo WAV en memoria\n",
    "                wav_buffer = io.BytesIO()\n",
    "                with wave.open(wav_buffer, 'wb') as wav_file:\n",
    "                    wav_file.setnchannels(1)  # Mono\n",
    "                    wav_file.setsampwidth(2)  # 2 bytes por muestra (16 bits)\n",
    "                    wav_file.setframerate(SAMPLE_RATE_OUT)\n",
    "                    wav_file.writeframes(audio_int16.tobytes())\n",
    "                \n",
    "                wav_buffer.seek(0)  # Rebobinar el buffer\n",
    "                microfono_queue.put(wav_buffer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error en recibir_audio: {e}\")\n",
    "    #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY  ver de comentarlo\n",
    "    finally: \n",
    "        sock_audio.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5e666",
   "metadata": {},
   "source": [
    "### PROCESAR AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0f2738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_audio():\n",
    "    global last_transcription #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "    while speech_recognition_running:\n",
    "        try:\n",
    "            wav_buffer = microfono_queue.get(timeout=1)\n",
    "            \n",
    "            # Crear un objeto AudioData desde el buffer WAV\n",
    "            with sr.AudioFile(wav_buffer) as source:\n",
    "                audio_data = recognizer.record(source)\n",
    "            \n",
    "            # Realizar la transcripción usando la API gratuita de Google\n",
    "            transcription = recognizer.recognize_google(audio_data, language=\"es-ES\")\n",
    "            \n",
    "            print(f\"Transcripción: {transcription}\")\n",
    "            last_transcription = transcription #NO ESTAN EN SPEECH-TO-TEXT-FREE-UDP-V1.PY\n",
    "            \n",
    "            # Enviar transcripción por UDP si es necesario\n",
    "            sock_voice.sendto(transcription.encode(), (UDP_IP_PI, UDP_PORT_TEXT))\n",
    "            \n",
    "        except queue.Empty:\n",
    "            continue\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"No se detectó voz en el audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Error en la solicitud a la API de Google: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error en la transcripción: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2687dfd",
   "metadata": {},
   "source": [
    "### ACTIVACION DE RECONOCIMIENTO DE VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcdb250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para iniciar el servicio de reconocimiento de voz\n",
    "def start_speech_recognition():\n",
    "    global speech_recognition_running\n",
    "    speech_recognition_running = True\n",
    "    \n",
    "    # Iniciar hilos para el reconocimiento de voz\n",
    "    audio_thread = threading.Thread(target=recibir_audio, daemon=True)\n",
    "    process_thread = threading.Thread(target=procesar_audio, daemon=True)\n",
    "    \n",
    "    audio_thread.start()\n",
    "    process_thread.start()\n",
    "    \n",
    "    print(\"Servicio de reconocimiento de voz iniciado...\")\n",
    "    return audio_thread, process_thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67993b4d",
   "metadata": {},
   "source": [
    "### DETENCION DE RECONOCIMIENTO DE VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cfb07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para detener el servicio de reconocimiento de voz\n",
    "def stop_speech_recognition():\n",
    "    global speech_recognition_running\n",
    "    speech_recognition_running = False\n",
    "    sock_voice.close()\n",
    "    print(\"Servicio de reconocimiento de voz detenido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838398c2",
   "metadata": {},
   "source": [
    "## CAMARA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ceab1",
   "metadata": {},
   "source": [
    "### MOTOR TEXTO-VOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0814348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa el socket UDP (compartido para todos los hilos)\n",
    "udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "\n",
    "# Inicializa el motor TTS\n",
    "tts_engine = pyttsx3.init()\n",
    "tts_engine.setProperty('rate', 150)\n",
    "\n",
    "# Objeto de bloqueo para sincronización\n",
    "tts_lock = threading.Lock()\n",
    "\n",
    "# Variable global para última reproducción\n",
    "last_spoken_gesture = None\n",
    "\n",
    "def speak_text(text):\n",
    "    global last_spoken_gesture, udp_socket\n",
    "\n",
    "    if not hasattr(speak_text, 'queue'):\n",
    "        speak_text.queue = []\n",
    "        speak_text.processing = False\n",
    "        speak_text.thread = None\n",
    "        speak_text.event = threading.Event()  # Nuevo evento para sincronización\n",
    "        speak_text.current_audio = None  # Bandera de audio en transmisión\n",
    "\n",
    "    # Evitar duplicados y agregar a cola\n",
    "    if text != last_spoken_gesture and text not in speak_text.queue:\n",
    "        speak_text.queue.append(text)\n",
    "        last_spoken_gesture = text\n",
    "\n",
    "    def _process_queue():\n",
    "        while speak_text.queue or speak_text.current_audio:\n",
    "            # Esperar si hay audio en curso\n",
    "            if speak_text.current_audio:\n",
    "                time.sleep(0.1)\n",
    "                continue\n",
    "\n",
    "            # Bloquear mientras se procesa\n",
    "            with tts_lock:\n",
    "                if speak_text.queue:\n",
    "                    speak_text.current_audio = speak_text.queue.pop(0)\n",
    "                    \n",
    "                    try:\n",
    "                        # Generar audio\n",
    "                        temp_file = \"temp_audio.wav\"\n",
    "                        tts_engine.save_to_file(speak_text.current_audio, temp_file)\n",
    "                        tts_engine.runAndWait()\n",
    "\n",
    "                        # Calcular tiempo de audio aproximado\n",
    "                        audio_duration = len(open(temp_file, 'rb').read()) / (16000 * 2)  # 16KHz, 16bits\n",
    "                        \n",
    "                        # Enviar por UDP\n",
    "                        with open(temp_file, 'rb') as f:\n",
    "                            audio_data = f.read()\n",
    "                            total_chunks = (len(audio_data) + MAX_PACKET_SIZE - 1) // MAX_PACKET_SIZE\n",
    "                            for i in range(total_chunks):\n",
    "                                chunk = audio_data[i*MAX_PACKET_SIZE:(i+1)*MAX_PACKET_SIZE]\n",
    "                                udp_socket.sendto(chunk, (UDP_IP_PI, UDP_PORT_PARLANTE))\n",
    "                                time.sleep(0.001)\n",
    "\n",
    "                        print(f\"Audio enviado: {speak_text.current_audio}\")\n",
    "                        \n",
    "                        # Esperar tiempo estimado de reproducción\n",
    "                        time.sleep(audio_duration * 0.8)  # Margen de seguridad\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error: {str(e)}\")\n",
    "                    finally:\n",
    "                        speak_text.current_audio = None\n",
    "                        if os.path.exists(temp_file):\n",
    "                            os.remove(temp_file)\n",
    "\n",
    "        speak_text.processing = False\n",
    "\n",
    "    if not speak_text.processing:\n",
    "        speak_text.processing = True\n",
    "        speak_text.thread = threading.Thread(target=_process_queue, daemon=True)\n",
    "        speak_text.thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb60538",
   "metadata": {},
   "source": [
    "### MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afe0a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5, #probar con 0.4\n",
    "    min_tracking_confidence=0.5 #probar con 0.4\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c14f25",
   "metadata": {},
   "source": [
    "### COMUNICACION CAMARA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eaba08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDPCamera:\n",
    "    def __init__(self):\n",
    "        self.host = UDP_OPEN\n",
    "        self.port = UDP_PORT_CAM\n",
    "        self.buffer_size = 65536\n",
    "        self.mtu = 1400\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.settimeout(2)\n",
    "        self.frame = None\n",
    "        self.fragments = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Configuración INTRÍNSECA de MediaPipe para el seguimiento\n",
    "        self.hands_tracker = mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=1,\n",
    "            min_detection_confidence=0.6,\n",
    "            min_tracking_confidence=0.6\n",
    "        )\n",
    "        \n",
    "        # Socket para enviar datos del servo\n",
    "        self.send_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        \n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.sock.bind((self.host, self.port))\n",
    "            self.thread = threading.Thread(target=self._receive_frames, daemon=True)\n",
    "            self.thread.start()\n",
    "\n",
    "    def _process_hand(self, frame):\n",
    "        # Procesamiento específico de la muñeca\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = self.hands_tracker.process(frame_rgb)\n",
    "        \n",
    "        if results.multi_hand_landmarks and results.multi_handedness:\n",
    "            for hand_handedness in results.multi_handedness:\n",
    "                if hand_handedness.classification[0].label == 'Left':\n",
    "                    wrist = results.multi_hand_landmarks[0].landmark[mp_hands.HandLandmark.WRIST]\n",
    "                    # Mapear coordenadas de la palma a un rango de -7.5 a 7.5\n",
    "                    x_normalized = int((wrist.x - 0.5) * 15) \n",
    "                    \n",
    "                    # Envío UDP automático\n",
    "                    self.send_sock.sendto(\n",
    "                        str(x_normalized).encode(), \n",
    "                        (UDP_IP_PI, UDP_PORT_SERVO)\n",
    "                    )\n",
    "                    \n",
    "                    # Dibujar punto (opcional)\n",
    "                    wrist_pixel = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                        wrist.x, wrist.y, frame.shape[1], frame.shape[0]\n",
    "                    )\n",
    "                    if wrist_pixel:\n",
    "                        cv2.circle(frame, wrist_pixel, 10, (0, 255, 0), -1)\n",
    "                    \n",
    "                    return x_normalized\n",
    "        return None\n",
    "\n",
    "    def _receive_frames(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                fragment, _ = self.sock.recvfrom(self.buffer_size)\n",
    "                with self.lock:\n",
    "                    self.fragments.append(fragment)\n",
    "                    if len(fragment) < self.mtu:\n",
    "                        frame_bytes = b''.join(self.fragments)\n",
    "                        self.fragments = []\n",
    "                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)\n",
    "                        frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)\n",
    "                        \n",
    "                        # Procesamiento AUTOMÁTICO de la mano\n",
    "                        if frame is not None:\n",
    "                            self._process_hand(frame)\n",
    "                            self.frame = frame  # Almacenar frame procesado\n",
    "                            \n",
    "            except socket.timeout:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def read(self):\n",
    "        with self.lock:\n",
    "            if self.frame is not None:\n",
    "                return True, self.frame.copy()\n",
    "            return False, None\n",
    "\n",
    "    def release(self):\n",
    "        self.running = False\n",
    "        self.hands_tracker.close()\n",
    "        with self.lock:\n",
    "            self.fragments = []\n",
    "            self.frame = None\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        self.sock.close()\n",
    "        self.send_sock.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cd388",
   "metadata": {},
   "source": [
    "### MODELO TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc0a7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel:\n",
    "    def __init__(self, model_path):\n",
    "        # Cargar el modelo TFLite\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        \n",
    "        # Obtener detalles de entrada y salida\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        # Asegurar el tipo de dato correcto y agregar dimensión batch si es necesario\n",
    "        input_data = np.array(input_data, dtype=self.input_details[0]['dtype'])\n",
    "        if len(input_data.shape) == len(self.input_details[0]['shape']) - 1:\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        # Establecer la entrada y ejecutar la inferencia\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.interpreter.invoke()\n",
    "        \n",
    "        # Obtener la salida\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "253769a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de TensorFlow para rendimiento\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Configuración de TensorFlow para rendimiento en CPU\n",
    "    try:\n",
    "        # Verificar si hay GPU disponible (para futuras expansiones)\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        if physical_devices:\n",
    "            # Configuración para GPU (no se ejecutará en tu caso)\n",
    "            for device in physical_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(\"GPU disponible para aceleración\")\n",
    "        else:\n",
    "            # Optimización para CPU\n",
    "            tf.config.threading.set_intra_op_parallelism_threads(4)  # Aprovecha núcleos físicos\n",
    "            tf.config.threading.set_inter_op_parallelism_threads(2)  # Paralelismo entre operaciones\n",
    "            print(\"Modo CPU activado: Configuración optimizada para Intel Core i7-7500U\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error de configuración: {str(e)}\")\n",
    "        print(\"Usando configuración por defecto de CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f84a7",
   "metadata": {},
   "source": [
    "### ARCHIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb605c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "data_dir = \"hand_gestures_data_v15\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Modelo y datos de entrenamiento\n",
    "model = None\n",
    "# Inicializar scaler y label encoder\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "model_file = \"hand_gesture_nn_model_v15.h5\"\n",
    "scaler_file = \"hand_gesture_scaler_v15.pkl\"\n",
    "encoder_file = \"hand_gesture_encoder_v15.pkl\"\n",
    "gesture_data = \"gesture_data_v15.pkl\" \n",
    "model_tflite = \"modelo_optimizadotl_v15.tflite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a4ec0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para estado\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Estado del sistema\n",
    "is_trained = False\n",
    "is_collecting = False\n",
    "current_gesture = \"\"\n",
    "samples_collected = 0\n",
    "max_samples = 5000\n",
    "\n",
    "# Control de tiempo para la recolección continua\n",
    "last_sample_time = 0\n",
    "sample_delay = 0.05  # 50ms entre muestras\n",
    "\n",
    "# Temporizador para mostrar mensajes\n",
    "message = \"\"\n",
    "message_until = 0\n",
    "\n",
    "# Para evaluación del modelo\n",
    "metrics = {\n",
    "    'accuracy': 0,\n",
    "    'val_accuracy': 0,\n",
    "    'training_time': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ce453",
   "metadata": {},
   "source": [
    "### EXTRACCION DE LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4dd6866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hand_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extrae los landmarks de las manos desde un frame de video.\n",
    "\n",
    "    Args:\n",
    "        frame: Imagen capturada por la cámara (en formato BGR).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Lista de landmarks normalizados (126 elementos) y booleano indicando si se detectaron manos.\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    landmarks_data = []\n",
    "    hands_detected = False\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hands_detected = True\n",
    "        # Extraer landmarks de hasta dos manos\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Dibujar landmarks en el frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = []\n",
    "\n",
    "            # Extraer coordenadas (x,y,z) de los 21 landmarks\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            landmarks_data.extend(landmarks)\n",
    "    \n",
    "    # Normalizar para 2 manos (si solo hay una o ninguna, rellenar con ceros)\n",
    "    while len(landmarks_data) < 21 * 3 * 2:  # 21 landmarks * 3 coordenadas * 2 manos\n",
    "        landmarks_data.append(0.0)\n",
    "    \n",
    "    # Limitar a exactamente 126 valores (21 landmarks * 3 coordenadas * 2 manos)\n",
    "    landmarks_data = landmarks_data[:21 * 3 * 2]\n",
    "    \n",
    "    return landmarks_data, hands_detected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62350ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_message(message_text, duration=2):\n",
    "    global message, message_until\n",
    "    message = message_text\n",
    "    message_until = time.time() + duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2fc5e",
   "metadata": {},
   "source": [
    "### RECOLECCION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e3b91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_collection(gesture_name):\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = True\n",
    "    current_gesture = gesture_name\n",
    "    samples_collected = 0\n",
    "    set_message(f\"Mantenga la seña frente a la cámara. Recolectando '{gesture_name}'...\", 3)\n",
    "\n",
    "def stop_collection():\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = False\n",
    "    current_gesture = \"\"\n",
    "    samples_collected = 0\n",
    "    set_message(\"Recolección finalizada\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7d27a",
   "metadata": {},
   "source": [
    "### GUARDADO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e60408d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "    global data, labels\n",
    "    data_to_save = {\"features\": data, \"labels\": labels}\n",
    "    with open(f\"{data_dir}/{gesture_data}\", \"wb\") as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "    set_message(f\"Datos guardados: {len(data)} muestras\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1258d",
   "metadata": {},
   "source": [
    "### RECOLECCION DE MUESTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b898ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sample(landmarks):\n",
    "    global is_collecting, samples_collected, last_sample_time, data, labels\n",
    "    \n",
    "    if not is_collecting:\n",
    "        return False\n",
    "    \n",
    "    current_time = time.time()\n",
    "    if current_time - last_sample_time >= sample_delay:\n",
    "        data.append(landmarks)\n",
    "        labels.append(current_gesture)\n",
    "        samples_collected += 1\n",
    "        last_sample_time = current_time\n",
    "        \n",
    "        if samples_collected % 10 == 0:\n",
    "            save_data()\n",
    "        \n",
    "        if samples_collected >= max_samples:\n",
    "            stop_collection()\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3435cd",
   "metadata": {},
   "source": [
    "### CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48579101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global data, labels\n",
    "    try:\n",
    "        with open(f\"{data_dir}/{gesture_data}\", \"rb\") as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "            data = loaded_data[\"features\"]\n",
    "            labels = loaded_data[\"labels\"]\n",
    "        set_message(f\"Datos cargados: {len(data)} muestras\", 2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos: {e}\")\n",
    "        set_message(\"No se encontraron datos previos\", 2)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a440d50",
   "metadata": {},
   "source": [
    "### RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc72c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_exists():\n",
    "    return os.path.exists(model_file) and os.path.exists(scaler_file) and os.path.exists(encoder_file)\n",
    "\n",
    "def create_neural_network(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79c92d",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d42f791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global model, scaler, label_encoder, metrics, is_trained\n",
    "    \n",
    "    if len(data) < 10:\n",
    "        set_message(\"Se necesitan más datos para entrenar\", 2)\n",
    "        return False\n",
    "    \n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Codificar etiquetas\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Dividir datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Normalizar datos\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Crear y entrenar modelo\n",
    "    num_classes = len(set(y_encoded))\n",
    "    set_message(f\"Entrenando modelo con {num_classes} clases...\", 2)\n",
    "    \n",
    "    model = create_neural_network(X_train.shape[1], num_classes)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Guardar métricas\n",
    "    metrics['accuracy'] = accuracy\n",
    "    metrics['val_accuracy'] = max(history.history['val_accuracy'])\n",
    "    metrics['training_time'] = training_time\n",
    "    \n",
    "    # Guardar modelo y preprocesadores\n",
    "    model.save(model_file)\n",
    "    with open(scaler_file, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(encoder_file, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    set_message(f\"Modelo entrenado con precisión: {accuracy:.2%}\", 3)\n",
    "    is_trained = True\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859d1ad",
   "metadata": {},
   "source": [
    "### CARGA DEL MODELO ENTRENADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "454258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model():\n",
    "    global scaler, label_encoder\n",
    "    try:\n",
    "        model = load_model(model_file)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(encoder_file, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        set_message(\"Modelo cargado correctamente\", 2)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo: {e}\")\n",
    "        set_message(\"Error al cargar el modelo\", 2)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51651eed",
   "metadata": {},
   "source": [
    "### MODELO DE TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e5be08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.5):\n",
    "    try:\n",
    "        # Preprocesar los landmarks\n",
    "        landmarks_array = np.array(landmarks).reshape(1, -1)\n",
    "        landmarks_scaled = scaler.transform(landmarks_array)\n",
    "        \n",
    "        # Realizar predicción\n",
    "        predictions = tflite_model.predict(landmarks_scaled)[0]\n",
    "        \n",
    "        # Obtener la clase con mayor probabilidad\n",
    "        max_prob_idx = np.argmax(predictions)\n",
    "        confidence = predictions[max_prob_idx]\n",
    "        \n",
    "        if confidence >= threshold:\n",
    "            # Decodificar la etiqueta\n",
    "            predicted_label = label_encoder.inverse_transform([max_prob_idx])[0]\n",
    "            return predicted_label, confidence\n",
    "        else:\n",
    "            return \"Desconocido\", confidence\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la predicción: {e}\")\n",
    "        return \"Error\", 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b02471",
   "metadata": {},
   "source": [
    "### CONVERSION A TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32082a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model_file, model_tflite):\n",
    "    try:\n",
    "        if not os.path.exists(model_file):\n",
    "            raise FileNotFoundError(f\"El archivo {model_file} no existe.\")\n",
    "        \n",
    "        # Cargar el modelo entrenado\n",
    "        modelo = load_model(model_file)\n",
    "        \n",
    "        # Convertir a TensorFlow Lite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo convertido\n",
    "        with open(model_tflite, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(\"Modelo convertido a TensorFlow Lite.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al convertir el modelo a TFLite:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd15d98",
   "metadata": {},
   "source": [
    "### MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89e8856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_menu():\n",
    "    print(\"\\n=== MENU PRINCIPAL ===\")\n",
    "    print(\"1. Recolectar nueva seña\")\n",
    "    print(\"2. Entrenar modelo\")\n",
    "    print(\"3. Listar señas cargadas\")\n",
    "    print(\"4. Evaluar en tiempo real\")\n",
    "    print(\"5. Salir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c02105",
   "metadata": {},
   "source": [
    "### LISTADO DE GESTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81a424fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gestures():\n",
    "    # Asumiendo que 'labels' es la lista donde se guardan las señas\n",
    "    if not labels:\n",
    "        print(\"No hay señas guardadas.\")\n",
    "    else:\n",
    "        unique_gestures = list(set(labels))\n",
    "        print(\"\\n--- Señas Guardadas ---\")\n",
    "        for i, gesture in enumerate(unique_gestures, 1):\n",
    "            print(f\"{i}. {gesture}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7bd27",
   "metadata": {},
   "source": [
    "### RECOLECCION DE SEÑAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f9c0ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_collection_mode():\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP iniciada para recolección.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    while is_collecting:  # Asumiendo que 'is_collecting' se activa en start_collection()\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame)\n",
    "\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        # Mostrar información en pantalla durante la recolección\n",
    "        progress = int((samples_collected / max_samples) * frame_w)\n",
    "        cv2.rectangle(frame, (0, 0), (progress, 20), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, f\"Recolectando: {current_gesture} ({samples_collected}/{max_samples})\", \n",
    "                    (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        if hands_detected:\n",
    "            collect_sample(landmarks)\n",
    "        else:\n",
    "            cv2.putText(frame, \"¡Muestra las manos!\", (frame_w//2 - 100, frame_h//2), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "        if not is_collecting:  # Cuando termina la recolección\n",
    "            menu_active = True\n",
    "            save_data()\n",
    "        \n",
    "        cv2.imshow(\"Recolectar Señas\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        # Puedes agregar una tecla para finalizar la recolección, por ejemplo 'm' para volver al menú.\n",
    "        if key == ord('m'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43829529",
   "metadata": {},
   "source": [
    "### EVALUACION EN TIEMPO REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "177c5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_mode():\n",
    "    global model_tflite\n",
    "    # Inicializa el modelo TFLite si aún no se ha cargado\n",
    "    if os.path.exists(model_tflite):\n",
    "        tflite_model = TFLiteModel(model_tflite)\n",
    "    else:\n",
    "        print(\"El modelo TFLite no existe. Conviértelo primero.\")\n",
    "        return\n",
    "\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP iniciada para evaluación en tiempo real.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    #NUEVO BARRITA DE PROGRESO\n",
    "    # Variables para el sistema de confirmación de señas\n",
    "    consecutive_frames = 0\n",
    "    last_prediction = \"\"\n",
    "    confirmation_threshold = 5  # Número de frames consecutivos necesarios\n",
    "    confirmed_gesture = \"\"\n",
    "    \n",
    "    # Para el enfoque alternativo de ventana deslizante\n",
    "    window_size = 5\n",
    "    prediction_window = []\n",
    "    \n",
    "    # Umbral de confianza para considerar una detección válida\n",
    "    confidence_threshold = 0.9\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame)\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        #NUEVO BARRITA DE PROGRESO\n",
    "        if hands_detected:\n",
    "            prediction, confidence = predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=confidence_threshold)\n",
    "            \n",
    "            # Extraer valor escalar en caso de que 'confidence' sea un array\n",
    "            confidence_value = np.max(confidence) if isinstance(confidence, np.ndarray) else confidence\n",
    "            \n",
    "            # Color basado en la confianza\n",
    "            color = (0, 255, 0) if confidence_value > confidence_threshold else (0, 165, 255)\n",
    "            \n",
    "            # Mostrar la predicción actual\n",
    "            cv2.putText(frame, f\"Seña detectada: {prediction}\", (10, 50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "            cv2.putText(frame, f\"Confianza: {confidence_value:.2%}\", (10, 90), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Sistema de confirmación por frames consecutivos\n",
    "            if confidence_value > confidence_threshold and prediction != \"Desconocido\":\n",
    "                # Enfoque 1: Frames consecutivos\n",
    "                if prediction == last_prediction:\n",
    "                    consecutive_frames += 1\n",
    "                else:\n",
    "                    consecutive_frames = 1  # Reiniciar contador si cambia la predicción\n",
    "                \n",
    "                last_prediction = prediction\n",
    "                \n",
    "                # Actualizar la ventana deslizante para el enfoque alternativo\n",
    "                prediction_window.append(prediction)\n",
    "                if len(prediction_window) > window_size:\n",
    "                    prediction_window.pop(0)  # Mantener solo los últimos 'window_size' elementos\n",
    "                \n",
    "                # Mostrar barra de progreso para la confirmación\n",
    "                progress_width = int((consecutive_frames / confirmation_threshold) * 200)  # Ancho máximo de 200 píxeles\n",
    "                progress_width = min(progress_width, 200)  # Limitar al máximo\n",
    "                \n",
    "                # Dibujar barra de progreso\n",
    "                cv2.rectangle(frame, (10, 120), (210, 140), (100, 100, 100), -1)  # Fondo gris\n",
    "                cv2.rectangle(frame, (10, 120), (10 + progress_width, 140), (0, 255, 0), -1)  # Barra verde\n",
    "                cv2.putText(frame, f\"Confirmando: {consecutive_frames}/{confirmation_threshold}\", (10, 160), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "                \n",
    "                # Verificar si se ha alcanzado el umbral de confirmación\n",
    "                if consecutive_frames >= confirmation_threshold:\n",
    "                    if confirmed_gesture != prediction:  # Evitar repeticiones\n",
    "                        confirmed_gesture = prediction\n",
    "                        threading.Thread(target=speak_text, args=(prediction,), daemon=True).start()\n",
    "                        cv2.putText(frame, f\"¡CONFIRMADO!: {prediction}\", (frame_w//4, frame_h//2), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
    "                \n",
    "                # Enfoque alternativo: Seña más frecuente en la ventana\n",
    "                if len(prediction_window) == window_size:\n",
    "                    from collections import Counter\n",
    "                    most_common = Counter(prediction_window).most_common(1)[0]  # (elemento, frecuencia)\n",
    "                    most_common_gesture, frequency = most_common\n",
    "                    \n",
    "                    # Mostrar información sobre el enfoque alternativo\n",
    "                    cv2.putText(frame, f\"Más frecuente: {most_common_gesture} ({frequency}/{window_size})\", \n",
    "                                (10, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 0), 1)\n",
    "                    \n",
    "                    # Este enfoque se puede habilitar si se prefiere sobre el de frames consecutivos\n",
    "                    # Si se quiere usar este enfoque en lugar del de frames consecutivos, descomentar:\n",
    "                    # if frequency >= 3 and confirmed_gesture != most_common_gesture:  # Mayoría en ventana de 5\n",
    "                    #     confirmed_gesture = most_common_gesture\n",
    "                    #     threading.Thread(target=speak_text, args=(most_common_gesture,), daemon=True).start()\n",
    "            else:\n",
    "                # Reiniciar contador si la confianza es baja\n",
    "                consecutive_frames = 0\n",
    "                cv2.putText(frame, \"Confianza insuficiente\", (10, 160), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)\n",
    "        #HASTA ACA LLEGA LA MODIFICACION DE LA BARRITA DE PROGRESO        \n",
    "\n",
    "        else:\n",
    "            # Reiniciar contador si no se detectan manos\n",
    "            consecutive_frames = 0\n",
    "            #HASTA ACA LLEGA LA MODIFICACION DE LA BARRITA DE PROGRESO \n",
    "\n",
    "            cv2.putText(frame, \"Acerca las manos a la cámara\", (frame_w//4, frame_h//2), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "        cv2.putText(frame, \"Presiona M para volver al menú\", (10, frame_h - 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.imshow(\"Evaluación en Tiempo Real\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:  # ESC\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b50ac",
   "metadata": {},
   "source": [
    "### FUNCION PRINCIPAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ff93c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global model, is_trained, data, labels\n",
    "\n",
    "    # Iniciar el servicio de reconocimiento de voz al iniciar el programa\n",
    "    print(\"Iniciando servicio de reconocimiento de voz...\")\n",
    "    speech_threads = start_speech_recognition()\n",
    "    print(\"Servicio de reconocimiento de voz iniciado correctamente.\")\n",
    "    \n",
    "    # Inicialización del sistema\n",
    "    is_trained = False\n",
    "    model = None\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Cargar datos existentes\n",
    "    load_data()\n",
    "    \n",
    "    # Intentar cargar modelo si existe\n",
    "    if check_model_exists():\n",
    "        model = load_saved_model()\n",
    "        is_trained = True\n",
    "    else:\n",
    "        is_trained = False\n",
    "\n",
    "    # Mostrar el menú en la consola\n",
    "    print_menu()\n",
    "\n",
    "    # Bucle principal de selección en consola\n",
    "    while True:\n",
    "        opcion = input(\"\\nSelecciona una opción (Recolectar: 1, Entrenar: 2, Señas: 3, Evaluar: 4, Salir: 5): \").strip()\n",
    "        \n",
    "        if opcion == '1':\n",
    "            # Recolección de señas\n",
    "            gesture_name = input(\"Ingrese nombre de la seña (ej. 'Hola'): \")\n",
    "            if gesture_name:\n",
    "                start_collection(gesture_name)\n",
    "                # Iniciar la cámara para mostrar video durante la recolección\n",
    "                run_collection_mode()\n",
    "                \n",
    "        elif opcion == '2':\n",
    "            if len(data) > 10:\n",
    "                train_model()\n",
    "                model = load_saved_model() if check_model_exists() else None\n",
    "                is_trained = True\n",
    "                print(\"Entrenamiento completado. Modelo entrenado.\")\n",
    "                convert_to_tflite(model_file, model_tflite)\n",
    "                print(\"Convertido a TFLite para evaluación en tiempo real\")\n",
    "            else:\n",
    "                print(\"¡Necesitas al menos 10 muestras para entrenar!\")\n",
    "                \n",
    "        elif opcion == '3':\n",
    "            list_gestures()  # Lista las señas cargadas\n",
    "\n",
    "        elif opcion == '4':\n",
    "            if is_trained:\n",
    "                # Inicializar modo evaluación en tiempo real\n",
    "                print(\"Modo de evaluación activado.\")\n",
    "                run_evaluation_mode()\n",
    "            else:\n",
    "                print(\"¡Entrena el modelo primero (Opción 2)!\")\n",
    "                \n",
    "        elif opcion == '5':\n",
    "            print(\"Deteniendo servicio de reconocimiento de voz...\")\n",
    "            stop_speech_recognition()\n",
    "            print(\"Saliendo del programa...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Opción inválida, intenta nuevamente.\")\n",
    "        \n",
    "        # Mostrar nuevamente el menú luego de finalizar la opción seleccionada.\n",
    "        print_menu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbcb2f",
   "metadata": {},
   "source": [
    "# EJECUTAR PROGRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40955799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando servicio de reconocimiento de voz...\n",
      "Servicio de reconocimiento de voz iniciado...\n",
      "Servicio de reconocimiento de voz iniciado correctamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Transcripción: Hola hola transcribiendo que me\n",
      "No se detectó voz en el audio\n",
      "Transcripción: qué tal no estoy viendo acá toda la transcripción\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Hola\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Ocho\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Chau\n",
      "Transcripción: chao\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Siete\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Seis\n",
      "Transcripción: siete\n",
      "Transcripción: 6\n",
      "Audio enviado: Te Quiero\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Siete\n",
      "Audio enviado: Chau\n",
      "Transcripción: te quiero 7\n",
      "Audio enviado: Hola\n",
      "Audio enviado: Ocho\n",
      "Transcripción: chao Hola\n",
      "Audio enviado: Nueve\n",
      "Transcripción: 9\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Chau\n",
      "Transcripción: chao\n",
      "No se detectó voz en el audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000017E3F8D6F80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_15688\\250689476.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_15688\\250689476.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Cámara UDP iniciada para recolección.\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Transcripción: Hola hola\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Transcripción: grabar mensaje tengo que mandarlo a droga presidente\n",
      "No se detectó voz en el audio\n",
      "Transcripción: programa de transcripción que se va tildando\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000017E3F8D6F80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_15688\\250689476.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_15688\\250689476.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1130/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6596 - loss: 1.3402No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7176 - loss: 1.1402 - val_accuracy: 0.9693 - val_loss: 0.2144 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1682/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.3328No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.3325 - val_accuracy: 0.9837 - val_loss: 0.1583 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1685/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9546 - loss: 0.2606No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9546 - loss: 0.2606 - val_accuracy: 0.9817 - val_loss: 0.1593 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9586 - loss: 0.2349 - val_accuracy: 0.9870 - val_loss: 0.1295 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m 832/1700\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9608 - loss: 0.2196No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9610 - loss: 0.2171 - val_accuracy: 0.9868 - val_loss: 0.1224 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.2084No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.2084 - val_accuracy: 0.9867 - val_loss: 0.1175 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9616 - loss: 0.2063 - val_accuracy: 0.9864 - val_loss: 0.1178 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m 707/1700\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1980No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9631 - loss: 0.1968 - val_accuracy: 0.9869 - val_loss: 0.1130 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1689/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9656 - loss: 0.1817No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9656 - loss: 0.1818 - val_accuracy: 0.9885 - val_loss: 0.1075 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9654 - loss: 0.1904 - val_accuracy: 0.9875 - val_loss: 0.1061 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m 556/1700\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1824No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9660 - loss: 0.1840 - val_accuracy: 0.9875 - val_loss: 0.1082 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m1557/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1774No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1774 - val_accuracy: 0.9889 - val_loss: 0.1047 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.1742 - val_accuracy: 0.9874 - val_loss: 0.1020 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m 421/1700\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9678 - loss: 0.1745No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1785 - val_accuracy: 0.9867 - val_loss: 0.1074 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m1522/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9688 - loss: 0.1706No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9687 - loss: 0.1707 - val_accuracy: 0.9882 - val_loss: 0.1006 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1728 - val_accuracy: 0.9872 - val_loss: 0.1053 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m 510/1700\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.1785No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9682 - loss: 0.1735 - val_accuracy: 0.9860 - val_loss: 0.1045 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m1373/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1682No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9677 - loss: 0.1676 - val_accuracy: 0.9885 - val_loss: 0.1026 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9656 - loss: 0.1722 - val_accuracy: 0.9870 - val_loss: 0.1050 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m 548/1700\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.1713No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.1685 - val_accuracy: 0.9903 - val_loss: 0.0925 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m1655/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1738No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1738 - val_accuracy: 0.9874 - val_loss: 0.1048 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9685 - loss: 0.1667 - val_accuracy: 0.9877 - val_loss: 0.0983 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m 876/1700\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9707 - loss: 0.1571No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9695 - loss: 0.1628 - val_accuracy: 0.9870 - val_loss: 0.1069 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m1455/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9694 - loss: 0.1670No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9694 - loss: 0.1672 - val_accuracy: 0.9871 - val_loss: 0.1020 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9687 - loss: 0.1660 - val_accuracy: 0.9893 - val_loss: 0.1015 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m 311/1700\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9739 - loss: 0.1510No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9758 - loss: 0.1411 - val_accuracy: 0.9913 - val_loss: 0.0858 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m1342/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.1239No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.1243 - val_accuracy: 0.9911 - val_loss: 0.0804 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.1301 - val_accuracy: 0.9921 - val_loss: 0.0771 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m 342/1700\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.1296No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.1271 - val_accuracy: 0.9909 - val_loss: 0.0778 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m1400/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.1207No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.1210 - val_accuracy: 0.9921 - val_loss: 0.0730 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9773 - loss: 0.1253 - val_accuracy: 0.9910 - val_loss: 0.0740 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m 281/1700\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.1212No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9770 - loss: 0.1224 - val_accuracy: 0.9922 - val_loss: 0.0702 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m1212/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.1190No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9778 - loss: 0.1195 - val_accuracy: 0.9908 - val_loss: 0.0743 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.1184 - val_accuracy: 0.9924 - val_loss: 0.0701 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m 123/1700\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9739 - loss: 0.1465No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.1237 - val_accuracy: 0.9917 - val_loss: 0.0711 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m1110/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9791 - loss: 0.1157No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.1171 - val_accuracy: 0.9926 - val_loss: 0.0676 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.1160 - val_accuracy: 0.9907 - val_loss: 0.0721 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m 182/1700\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9805 - loss: 0.1193No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.1192 - val_accuracy: 0.9916 - val_loss: 0.0670 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m1289/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.1175No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.1173 - val_accuracy: 0.9918 - val_loss: 0.0657 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.1161 - val_accuracy: 0.9896 - val_loss: 0.0729 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m 348/1700\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1217No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.1173 - val_accuracy: 0.9919 - val_loss: 0.0679 - learning_rate: 5.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m1463/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.1197No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.1191 - val_accuracy: 0.9914 - val_loss: 0.0700 - learning_rate: 5.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9780 - loss: 0.1153 - val_accuracy: 0.9920 - val_loss: 0.0657 - learning_rate: 5.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m 509/1700\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.1084No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9772 - loss: 0.1159 - val_accuracy: 0.9917 - val_loss: 0.0658 - learning_rate: 5.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m1515/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.1009No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.1008 - val_accuracy: 0.9924 - val_loss: 0.0636 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0923 - val_accuracy: 0.9927 - val_loss: 0.0618 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m 508/1700\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9811 - loss: 0.0991No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9822 - loss: 0.0962 - val_accuracy: 0.9937 - val_loss: 0.0562 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m1528/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0879No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0883 - val_accuracy: 0.9936 - val_loss: 0.0561 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9826 - loss: 0.0934 - val_accuracy: 0.9937 - val_loss: 0.0551 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m 563/1700\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0994No se detectó voz en el audio\n",
      "\u001b[1m1700/1700\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9824 - loss: 0.0961 - val_accuracy: 0.9941 - val_loss: 0.0546 - learning_rate: 2.5000e-04\n",
      "\u001b[1m532/532\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmp43ndludc\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmp43ndludc\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmp43ndludc'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 17), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1642477002192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642466149648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642460921200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642460913104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642460910640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642460918912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642461633648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642461627840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642461641040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642461640160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642461640336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642461641568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642476999200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1642477006944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "Convertido a TFLite para evaluación en tiempo real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Transcripción: concha de tu madre\n",
      "No se detectó voz en el audio\n",
      "Transcripción: concha de tu madre\n",
      "Transcripción: Brasil\n",
      "Transcripción: para grabar pantalla\n",
      "Transcripción: concha de tu puta madre\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Transcripción: concha de tu puta madre\n",
      "No se detectó voz en el audio\n",
      "\n",
      "--- Señas Guardadas ---\n",
      "1. Hola\n",
      "2. Uno\n",
      "3. Cuatro\n",
      "4. Chau\n",
      "5. Ocho\n",
      "6. Siete\n",
      "7. Yo\n",
      "8. Gracias\n",
      "9. Seis\n",
      "10. Tres\n",
      "11. Diez\n",
      "12. Nueve\n",
      "13. Te Quiero\n",
      "14. Dos\n",
      "15. Cinco\n",
      "16. Cero\n",
      "17. De Nada\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "No se detectó voz en el audio\n",
      "\n",
      "--- Señas Guardadas ---\n",
      "1. Hola\n",
      "2. Uno\n",
      "3. Cuatro\n",
      "4. Chau\n",
      "5. Ocho\n",
      "6. Siete\n",
      "7. Yo\n",
      "8. Gracias\n",
      "9. Seis\n",
      "10. Tres\n",
      "11. Diez\n",
      "12. Nueve\n",
      "13. Te Quiero\n",
      "14. Dos\n",
      "15. Cinco\n",
      "16. Cero\n",
      "17. De Nada\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Modo de evaluación activado.\n",
      "Cámara UDP iniciada para evaluación en tiempo real.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio enviado: Hola\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Diez\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Nueve\n",
      "Audio enviado: Ocho\n",
      "Transcripción: 9\n",
      "Audio enviado: Nueve\n",
      "Audio enviado: Siete\n",
      "Transcripción: 9\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Tres\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Transcripción: qué es eso\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Dos\n",
      "Transcripción: de nada\n",
      "Audio enviado: Tres\n",
      "Transcripción: dos tres\n",
      "Audio enviado: Uno\n",
      "Audio enviado: Seis\n",
      "Transcripción: uno\n",
      "Audio enviado: Cinco\n",
      "Transcripción: 6\n",
      "Audio enviado: Cuatro\n",
      "Transcripción: cinco\n",
      "Audio enviado: Yo\n",
      "Audio enviado: Te Quiero\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Cuatro\n",
      "Transcripción: te quiero\n",
      "Audio enviado: Gracias\n",
      "Transcripción: gracias\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Dos\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Hola\n",
      "Transcripción: de nada\n",
      "Transcripción: Hola\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Chau\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Seis\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Chau\n",
      "Transcripción: chao\n",
      "Audio enviado: Nueve\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Diez\n",
      "Transcripción: 10\n",
      "Audio enviado: Ocho\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Siete\n",
      "No se detectó voz en el audio\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Chau\n",
      "Transcripción: de nada\n",
      "Audio enviado: Hola\n",
      "Transcripción: Hola\n",
      "Audio enviado: De Nada\n",
      "Audio enviado: Cinco\n",
      "Transcripción: de nada\n",
      "Transcripción: cinco\n",
      "Audio enviado: De Nada\n",
      "Transcripción: de nada\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Siete\n",
      "Audio enviado: De Nada\n",
      "No se detectó voz en el audio\n",
      "Audio enviado: Hola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UDPCamera.__del__ at 0x0000017E3F8D6F80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_15688\\250689476.py\", line 104, in __del__\n",
      "  File \"C:\\Users\\Nacho\\AppData\\Local\\Temp\\ipykernel_15688\\250689476.py\", line 94, in release\n",
      "  File \"c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\mediapipe\\python\\solution_base.py\", line 361, in close\n",
      "    raise ValueError('Closing SolutionBase._graph which is already None')\n",
      "ValueError: Closing SolutionBase._graph which is already None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Evaluar en tiempo real\n",
      "5. Salir\n",
      "Transcripción: De nada Hola\n",
      "Deteniendo servicio de reconocimiento de voz...\n",
      "Servicio de reconocimiento de voz detenido.\n",
      "Saliendo del programa...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    save_data()  # Guarda los datos recolectadosk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
