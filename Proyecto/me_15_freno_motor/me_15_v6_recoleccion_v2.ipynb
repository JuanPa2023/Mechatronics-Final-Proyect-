{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd78588",
   "metadata": {},
   "source": [
    "# ------------- PROYECTO FINAL G & S--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067e16c",
   "metadata": {},
   "source": [
    "## LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1bf53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Comunicación y cámara\n",
    "import socket\n",
    "import queue\n",
    "\n",
    "# Voz\n",
    "import pyttsx3\n",
    "import threading\n",
    "\n",
    "# Módulos para reconocimiento de voz\n",
    "import speech_recognition as sr\n",
    "import librosa\n",
    "import io\n",
    "import wave\n",
    "\n",
    "# Generador de audios\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd329ca",
   "metadata": {},
   "source": [
    "## UDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df331d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para el reconocimiento de voz\n",
    "SAMPLE_RATE_IN = 48000  # Tasa del micrófono INMP441\n",
    "SAMPLE_RATE_OUT = 16000  # Tasa requerida por la API de reconocimiento\n",
    "BUFFER_DURATION = 5  # segundos\n",
    "\n",
    "\n",
    "UDP_IP_PI = \"192.168.7.2\"  # IP de la Raspberry Pi\n",
    "UDP_OPEN = '0.0.0.0'\n",
    "\n",
    "# Puertos para diferentes servicios\n",
    "UDP_PORT_MICROFONO = 5006\n",
    "UDP_PORT_TEXT = 5005\n",
    "UDP_PORT_SERVO = 5001  # Puerto para enviar comandos\n",
    "UDP_PORT_PARLANTE = 5003\n",
    "UDP_PORT_CAM = 5002  # Puerto para recibir video\n",
    "\n",
    "# MSS\n",
    "MAX_PACKET_SIZE = 1460  # Tamaño máximo del paquete UDP\n",
    "\n",
    "# Buffer UDP\n",
    "BUFFER_UDP = 65536 #16 bits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838398c2",
   "metadata": {},
   "source": [
    "## CAMARA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb60538",
   "metadata": {},
   "source": [
    "### MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe0a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5, #probar con 0.4\n",
    "    min_tracking_confidence=0.5 #probar con 0.4\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c14f25",
   "metadata": {},
   "source": [
    "### COMUNICACION CAMARA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ee9892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDPCamera:\n",
    "    def __init__(self):\n",
    "        self.host = UDP_OPEN\n",
    "        self.port = UDP_PORT_CAM\n",
    "        self.buffer_size = BUFFER_UDP\n",
    "        self.mtu = MAX_PACKET_SIZE\n",
    "        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "        self.sock.settimeout(2)\n",
    "        self.frame = None\n",
    "        self.fragments = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.lock = threading.Lock()\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.sock.bind((self.host, self.port))\n",
    "            self.thread = threading.Thread(target=self._receive_frames, daemon=True)\n",
    "            self.thread.start()\n",
    "\n",
    "    def _receive_frames(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                fragment, _ = self.sock.recvfrom(self.buffer_size)\n",
    "                with self.lock:\n",
    "                    self.fragments.append(fragment)\n",
    "                    if len(fragment) < self.mtu:  # Último fragmento\n",
    "                        frame_bytes = b''.join(self.fragments)\n",
    "                        self.fragments = []\n",
    "                        frame_array = np.frombuffer(frame_bytes, dtype=np.uint8)\n",
    "                        self.frame = cv2.imdecode(frame_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "            except socket.timeout:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error en recepción: {str(e)}\")\n",
    "                break\n",
    "\n",
    "    def read(self):\n",
    "        with self.lock:\n",
    "            if self.frame is not None:\n",
    "                return True, self.frame.copy()\n",
    "            return False, None\n",
    "\n",
    "    def release(self):\n",
    "        self.running = False\n",
    "        with self.lock:\n",
    "            self.fragments = []\n",
    "            self.frame = None\n",
    "        if self.thread and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "        self.sock.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cd388",
   "metadata": {},
   "source": [
    "### MODELO TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0a7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel:\n",
    "    def __init__(self, model_path):\n",
    "        # Cargar el modelo TFLite\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "        self.interpreter.allocate_tensors()\n",
    "        \n",
    "        # Obtener detalles de entrada y salida\n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        # Asegurar el tipo de dato correcto y agregar dimensión batch si es necesario\n",
    "        input_data = np.array(input_data, dtype=self.input_details[0]['dtype'])\n",
    "        if len(input_data.shape) == len(self.input_details[0]['shape']) - 1:\n",
    "            input_data = np.expand_dims(input_data, axis=0)\n",
    "        \n",
    "        # Establecer la entrada y ejecutar la inferencia\n",
    "        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)\n",
    "        self.interpreter.invoke()\n",
    "        \n",
    "        # Obtener la salida\n",
    "        output_data = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253769a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de TensorFlow para rendimiento\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Configuración de TensorFlow para rendimiento en CPU\n",
    "    try:\n",
    "        # Verificar si hay GPU disponible (para futuras expansiones)\n",
    "        physical_devices = tf.config.list_physical_devices('GPU')\n",
    "        \n",
    "        if physical_devices:\n",
    "            # Configuración para GPU (no se ejecutará en tu caso)\n",
    "            for device in physical_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "            print(\"GPU disponible para aceleración\")\n",
    "        else:\n",
    "            # Optimización para CPU\n",
    "            tf.config.threading.set_intra_op_parallelism_threads(4)  # Aprovecha núcleos físicos\n",
    "            tf.config.threading.set_inter_op_parallelism_threads(2)  # Paralelismo entre operaciones\n",
    "            print(\"Modo CPU activado: Configuración optimizada para Intel Core i7-7500U\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error de configuración: {str(e)}\")\n",
    "        print(\"Usando configuración por defecto de CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f84a7",
   "metadata": {},
   "source": [
    "### ARCHIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb605c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "data_dir = \"hand_gestures_data_v15\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Modelo y datos de entrenamiento\n",
    "model = None\n",
    "# Inicializar scaler y label encoder\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "model_file = \"hand_gesture_nn_model_v15.h5\"\n",
    "scaler_file = \"hand_gesture_scaler_v15.pkl\"\n",
    "encoder_file = \"hand_gesture_encoder_v15.pkl\"\n",
    "gesture_data = \"gesture_data_v15.pkl\" \n",
    "model_tflite = \"modelo_optimizadotl_v15.tflite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a4ec0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables globales para estado\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Estado del sistema\n",
    "is_trained = False\n",
    "is_collecting = False\n",
    "current_gesture = \"\"\n",
    "samples_collected = 0\n",
    "max_samples = 5000\n",
    "\n",
    "# Control de tiempo para la recolección continua\n",
    "last_sample_time = 0\n",
    "sample_delay = 0.05  # 50ms entre muestras\n",
    "\n",
    "# Temporizador para mostrar mensajes\n",
    "message = \"\"\n",
    "message_until = 0\n",
    "\n",
    "# Para evaluación del modelo\n",
    "metrics = {\n",
    "    'accuracy': 0,\n",
    "    'val_accuracy': 0,\n",
    "    'training_time': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ce453",
   "metadata": {},
   "source": [
    "### EXTRACCION DE LANDMARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b9b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "send_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # socket de los puntos de la muñeca\n",
    "\n",
    "def extract_hand_landmarks(frame, send_sock=None):\n",
    "    \"\"\"\n",
    "    Extrae landmarks de las manos y realiza seguimiento de la muñeca derecha.\n",
    "    \n",
    "    Args:\n",
    "        frame: Imagen en formato BGR\n",
    "        send_sock: Socket UDP opcional para enviar datos de seguimiento\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (landmarks_data, hands_detected)\n",
    "    \"\"\"\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    landmarks_data = []\n",
    "    hands_detected = False\n",
    "    x_normalized = None\n",
    "    right_wrist_pixel = None\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hands_detected = True\n",
    "        # Procesar ambas manos para landmarks\n",
    "        for hand_idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Dibujar landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Extraer coordenadas\n",
    "            landmarks = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "            landmarks_data.extend(landmarks)\n",
    "            \n",
    "            # Detectar mano derecha para seguimiento\n",
    "            if results.multi_handedness:\n",
    "                handedness = results.multi_handedness[hand_idx]\n",
    "                if handedness.classification[0].label == 'Left' and not x_normalized:\n",
    "                    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "                    \n",
    "                    # Calcular coordenadas normalizadas\n",
    "                    x_normalized = int((wrist.x - 0.5) * 15)  # Rango -7.5 a 7.5\n",
    "                    #print(x_normalized)\n",
    "                    \n",
    "                    # Obtener coordenadas para dibujo\n",
    "                    right_wrist_pixel = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                        wrist.x, wrist.y, frame.shape[1], frame.shape[0]\n",
    "                    )\n",
    "\n",
    "        # Enviar datos y dibujar si se detectó mano derecha\n",
    "        if x_normalized is not None:\n",
    "            if send_sock:  # Solo enviar si se provee socket\n",
    "                send_sock.sendto(\n",
    "                    str(x_normalized).encode(),\n",
    "                    (UDP_IP_PI, UDP_PORT_SERVO)  # Asegurar que estas constantes están definidas\n",
    "                )\n",
    "            if right_wrist_pixel:\n",
    "                cv2.circle(frame, right_wrist_pixel, 10, (0, 255, 0), -1)\n",
    "    \n",
    "    # Rellenar con ceros si no hay manos\n",
    "    while len(landmarks_data) < 21 * 3 * 2:\n",
    "        landmarks_data.append(0.0)\n",
    "    landmarks_data = landmarks_data[:21 * 3 * 2]\n",
    "    \n",
    "    return landmarks_data, hands_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62350ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_message(message_text, duration=2):\n",
    "    global message, message_until\n",
    "    message = message_text\n",
    "    message_until = time.time() + duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2fc5e",
   "metadata": {},
   "source": [
    "### RECOLECCION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3b91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_collection(gesture_name):\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = True\n",
    "    current_gesture = gesture_name\n",
    "    samples_collected = 0\n",
    "    set_message(f\"Mantenga la seña frente a la cámara. Recolectando '{gesture_name}'...\", 3)\n",
    "\n",
    "def stop_collection():\n",
    "    global is_collecting, current_gesture, samples_collected\n",
    "    is_collecting = False\n",
    "    current_gesture = \"\"\n",
    "    samples_collected = 0\n",
    "    set_message(\"Recolección finalizada\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7d27a",
   "metadata": {},
   "source": [
    "### GUARDADO DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e60408d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data():\n",
    "    global data, labels\n",
    "    data_to_save = {\"features\": data, \"labels\": labels}\n",
    "    with open(f\"{data_dir}/{gesture_data}\", \"wb\") as f:\n",
    "        pickle.dump(data_to_save, f)\n",
    "    set_message(f\"Datos guardados: {len(data)} muestras\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1258d",
   "metadata": {},
   "source": [
    "### RECOLECCION DE MUESTRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b898ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sample(landmarks):\n",
    "    global is_collecting, samples_collected, last_sample_time, data, labels\n",
    "    \n",
    "    if not is_collecting:\n",
    "        return False\n",
    "    \n",
    "    current_time = time.time()\n",
    "    if current_time - last_sample_time >= sample_delay:\n",
    "        data.append(landmarks)\n",
    "        labels.append(current_gesture)\n",
    "        samples_collected += 1\n",
    "        last_sample_time = current_time\n",
    "        \n",
    "        if samples_collected % max_samples == 0:\n",
    "            save_data()\n",
    "        \n",
    "        if samples_collected >= max_samples:\n",
    "            stop_collection()\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3435cd",
   "metadata": {},
   "source": [
    "### CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48579101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    global data, labels\n",
    "    try:\n",
    "        with open(f\"{data_dir}/{gesture_data}\", \"rb\") as f:\n",
    "            loaded_data = pickle.load(f)\n",
    "            data = loaded_data[\"features\"]\n",
    "            labels = loaded_data[\"labels\"]\n",
    "        set_message(f\"Datos cargados: {len(data)} muestras\", 2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar datos: {e}\")\n",
    "        set_message(\"No se encontraron datos previos\", 2)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a440d50",
   "metadata": {},
   "source": [
    "### RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc72c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_exists():\n",
    "    return os.path.exists(model_file) and os.path.exists(scaler_file) and os.path.exists(encoder_file)\n",
    "\n",
    "def create_neural_network(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,), kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb79c92d",
   "metadata": {},
   "source": [
    "### ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d42f791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global model, scaler, label_encoder, metrics, is_trained\n",
    "    \n",
    "    if len(data) < 10:\n",
    "        set_message(\"Se necesitan más datos para entrenar\", 2)\n",
    "        return False\n",
    "    \n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Codificar etiquetas\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Dividir datos\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    # Normalizar datos\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Crear y entrenar modelo\n",
    "    num_classes = len(set(y_encoded))\n",
    "    set_message(f\"Entrenando modelo con {num_classes} clases...\", 2)\n",
    "    \n",
    "    model = create_neural_network(X_train.shape[1], num_classes)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Guardar métricas\n",
    "    metrics['accuracy'] = accuracy\n",
    "    metrics['val_accuracy'] = max(history.history['val_accuracy'])\n",
    "    metrics['training_time'] = training_time\n",
    "    \n",
    "    # Guardar modelo y preprocesadores\n",
    "    model.save(model_file)\n",
    "    with open(scaler_file, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    with open(encoder_file, 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    set_message(f\"Modelo entrenado con precisión: {accuracy:.2%}\", 3)\n",
    "    is_trained = True\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859d1ad",
   "metadata": {},
   "source": [
    "### CARGA DEL MODELO ENTRENADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "454258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_model():\n",
    "    global scaler, label_encoder\n",
    "    try:\n",
    "        model = load_model(model_file)\n",
    "        with open(scaler_file, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        with open(encoder_file, 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "        set_message(\"Modelo cargado correctamente\", 2)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo: {e}\")\n",
    "        set_message(\"Error al cargar el modelo\", 2)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51651eed",
   "metadata": {},
   "source": [
    "### MODELO DE TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e5be08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tflite(landmarks, tflite_model, scaler, label_encoder, threshold=0.5):\n",
    "    try:\n",
    "        # Preprocesar los landmarks\n",
    "        landmarks_array = np.array(landmarks).reshape(1, -1)\n",
    "        landmarks_scaled = scaler.transform(landmarks_array)\n",
    "        \n",
    "        # Realizar predicción\n",
    "        predictions = tflite_model.predict(landmarks_scaled)[0]\n",
    "        \n",
    "        # Obtener la clase con mayor probabilidad\n",
    "        max_prob_idx = np.argmax(predictions)\n",
    "        confidence = predictions[max_prob_idx]\n",
    "        \n",
    "        if confidence >= threshold:\n",
    "            # Decodificar la etiqueta\n",
    "            predicted_label = label_encoder.inverse_transform([max_prob_idx])[0]\n",
    "            return predicted_label, confidence\n",
    "        else:\n",
    "            return \"Desconocido\", confidence\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la predicción: {e}\")\n",
    "        return \"Error\", 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b02471",
   "metadata": {},
   "source": [
    "### CONVERSION A TFLITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32082a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model_file, model_tflite):\n",
    "    try:\n",
    "        if not os.path.exists(model_file):\n",
    "            raise FileNotFoundError(f\"El archivo {model_file} no existe.\")\n",
    "        \n",
    "        # Cargar el modelo entrenado\n",
    "        modelo = load_model(model_file)\n",
    "        \n",
    "        # Convertir a TensorFlow Lite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n",
    "        tflite_model = converter.convert()\n",
    "        \n",
    "        # Guardar el modelo convertido\n",
    "        with open(model_tflite, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        print(\"Modelo convertido a TensorFlow Lite.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al convertir el modelo a TFLite:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f2d65",
   "metadata": {},
   "source": [
    "### ELIMINADOR DE SEÑAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71cb9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_gesture(target_label):\n",
    "    global data, labels\n",
    "    if target_label not in labels:\n",
    "        print(f\"Error: La etiqueta '{target_label}' no existe\")\n",
    "        return False\n",
    "    \n",
    "    # Filtrar elementos a mantener\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "    deleted_count = 0\n",
    "    \n",
    "    for feature, label in zip(data, labels):\n",
    "        if label == target_label:\n",
    "            deleted_count += 1\n",
    "        else:\n",
    "            new_data.append(feature)\n",
    "            new_labels.append(label)\n",
    "    \n",
    "    # Actualizar listas globales\n",
    "    data.clear()\n",
    "    labels.clear()\n",
    "    data.extend(new_data)\n",
    "    labels.extend(new_labels)\n",
    "    \n",
    "    print(f\"Se eliminaron {deleted_count} muestras de '{target_label}'\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9909a1bc",
   "metadata": {},
   "source": [
    "### GENERADOR DE AUDIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b3c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios y archivos\n",
    "audio_dir = \"pyttsx3_audios\"\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "# Configurar motor TTS\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)\n",
    "\n",
    "def trim_audio_silence(file_path):\n",
    "    \"\"\"Recorta silencios al inicio y final del audio\"\"\"\n",
    "    audio = AudioSegment.from_file(file_path, format=\"wav\")\n",
    "\n",
    "    # Parámetros ajustables\n",
    "    config = {\n",
    "        'min_silence_len': 200,     # 200 ms de silencio mínimo para considerar corte\n",
    "        'silence_thresh': -45,      # -45 dB de umbral de silencio\n",
    "        'end_buffer': 150           # 150 ms extra al final\n",
    "    }\n",
    "    \n",
    "    # Detectar segmentos no silenciosos\n",
    "    nonsilent_parts = detect_nonsilent(\n",
    "        audio,\n",
    "        min_silence_len=config['min_silence_len'], # Duración mínima de silencio a considerar (ms)\n",
    "        silence_thresh=config['silence_thresh'] # Umbral de volumen para considerar silencio (dB)\n",
    "    )\n",
    "    \n",
    "    if nonsilent_parts:\n",
    "        start = max(0, nonsilent_parts[0][0] - 50)  # 50 ms buffer inicial\n",
    "        end = nonsilent_parts[-1][1] + config['end_buffer']\n",
    "        trimmed_audio = audio[start:end]\n",
    "        trimmed_audio.export(file_path, format=\"wav\")\n",
    "\n",
    "def compilador_audios(label):\n",
    "    \"\"\"Genera y ajusta audio para eliminar silencios\"\"\"\n",
    "    nombre_archivo = label.replace(' ', '_').lower() + '.wav'\n",
    "    ruta_audio = os.path.join(audio_dir, nombre_archivo)\n",
    "    \n",
    "    if os.path.exists(ruta_audio):\n",
    "        return\n",
    "    \n",
    "    temp_path = os.path.join(audio_dir, \"temp.wav\")\n",
    "    try:\n",
    "        # Generar audio temporal\n",
    "        engine.save_to_file(label, temp_path)\n",
    "        engine.runAndWait()\n",
    "        \n",
    "        # Recortar y renombrar\n",
    "        trim_audio_silence(temp_path)\n",
    "        os.rename(temp_path, ruta_audio)\n",
    "        print(f\"Audio generado: {nombre_archivo}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generando {label}: {str(e)}\")\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "\n",
    "def generar_audios():\n",
    "    \"\"\"Genera audios para todas las etiquetas únicas\"\"\"\n",
    "    etiquetas_unicas = set(labels)\n",
    "    print(\"\\nGenerando audios para señas...\")\n",
    "    \n",
    "    for label in etiquetas_unicas:\n",
    "        compilador_audios(label)\n",
    "    \n",
    "    print(\"Proceso de generación de audios completado\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd15d98",
   "metadata": {},
   "source": [
    "### MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89e8856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_menu():\n",
    "    print(\"\\n=== MENU PRINCIPAL ===\")\n",
    "    print(\"1. Recolectar nueva seña\")\n",
    "    print(\"2. Entrenar modelo\")\n",
    "    print(\"3. Listar señas cargadas\")\n",
    "    print(\"4. Eliminar señas\")\n",
    "    print(\"5. Generar Audios\")\n",
    "    print(\"0. Salir\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c02105",
   "metadata": {},
   "source": [
    "### LISTADO DE GESTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81a424fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gestures():\n",
    "    # Asumiendo que 'labels' es la lista donde se guardan las señas\n",
    "    if not labels:\n",
    "        print(\"No hay señas guardadas.\")\n",
    "    else:\n",
    "        unique_gestures = list(set(labels))\n",
    "        print(\"\\n--- Señas Guardadas ---\")\n",
    "        for i, gesture in enumerate(unique_gestures, 1):\n",
    "            print(f\"{i}. {gesture}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7bd27",
   "metadata": {},
   "source": [
    "### RECOLECCION DE SEÑAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9c0ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_collection_mode():\n",
    "    # Inicia la cámara\n",
    "    try:\n",
    "        cap = UDPCamera()\n",
    "        print(\"Cámara UDP iniciada para recolección.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al iniciar la cámara: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    while is_collecting:  # Asumiendo que 'is_collecting' se activa en start_collection()\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "\n",
    "        landmarks, hands_detected = extract_hand_landmarks(frame)\n",
    "\n",
    "        frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "        # Mostrar información en pantalla durante la recolección\n",
    "        progress = int((samples_collected / max_samples) * frame_w)\n",
    "        cv2.rectangle(frame, (0, 0), (progress, 20), (0, 255, 0), -1)\n",
    "        cv2.putText(frame, f\"Recolectando: {current_gesture} ({samples_collected}/{max_samples})\", \n",
    "                    (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        if hands_detected:\n",
    "            collect_sample(landmarks)\n",
    "        else:\n",
    "            cv2.putText(frame, \"¡Muestra las manos!\", (frame_w//2 - 100, frame_h//2), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "        if not is_collecting:  # Cuando termina la recolección\n",
    "            menu_active = True\n",
    "            save_data()\n",
    "        \n",
    "        cv2.imshow(\"Recolectar Señas\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1)\n",
    "        # Pulsar esc para salir de la recolección.\n",
    "        if key == 27:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b50ac",
   "metadata": {},
   "source": [
    "### FUNCION PRINCIPAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ff93c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global model, is_trained, data, labels\n",
    "    \n",
    "    # Inicialización del sistema\n",
    "    is_trained = False\n",
    "    model = None\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Cargar datos existentes\n",
    "    load_data()\n",
    "    \n",
    "    # Intentar cargar modelo si existe\n",
    "    if check_model_exists():\n",
    "        model = load_saved_model()\n",
    "        is_trained = True\n",
    "    else:\n",
    "        is_trained = False\n",
    "\n",
    "    # Mostrar el menú en la consola\n",
    "    print_menu()\n",
    "\n",
    "    # Bucle principal de selección en consola\n",
    "    while True:\n",
    "        opcion = input(\"\\nSelecciona una opción (Recolectar: 1, Entrenar: 2, Listar: 3, Eliminar: 4, Gen.Audio: 5, Salir: 0): \").strip()\n",
    "        \n",
    "        if opcion == '1':\n",
    "            # Recolección de señas\n",
    "            gesture_name = input(\"Ingrese nombre de la seña (ej. 'Hola'): \")\n",
    "            if gesture_name:\n",
    "                start_collection(gesture_name)\n",
    "                # Iniciar la cámara para mostrar video durante la recolección\n",
    "                run_collection_mode()\n",
    "                \n",
    "        elif opcion == '2':\n",
    "            if len(data) > 10:\n",
    "                train_model()\n",
    "                model = load_saved_model() if check_model_exists() else None\n",
    "                is_trained = True\n",
    "                print(\"Entrenamiento completado. Modelo entrenado.\")\n",
    "                convert_to_tflite(model_file, model_tflite)\n",
    "                print(\"Convertido a TFLite para evaluación en tiempo real\")\n",
    "            else:\n",
    "                print(\"¡Necesitas al menos 10 muestras para entrenar!\")\n",
    "                \n",
    "        elif opcion == '3':\n",
    "            list_gestures()  # Lista las señas cargadas\n",
    "\n",
    "        elif opcion == '4':\n",
    "            if not labels:\n",
    "                print(\"No hay señas guardadas para eliminar\")\n",
    "                exit()\n",
    "            print(\"\\n--- Señas Registradas ---\")\n",
    "            for label in set(labels):\n",
    "                print(f\"- {label}\")\n",
    "            target_label = input(\"\\nIngrese el nombre exacto de la seña a eliminar: \").strip()\n",
    "            if delete_gesture(target_label):\n",
    "                save_data()\n",
    "            else:\n",
    "                print(\"No se realizaron cambios en los datos\")\n",
    "        \n",
    "        elif opcion == '5':\n",
    "            if not labels:\n",
    "                print(\"No hay señas guardadas\")\n",
    "                exit()\n",
    "            # Generar audios automáticamente\n",
    "            generar_audios()\n",
    "            \n",
    "\n",
    "                \n",
    "        elif opcion == '0':\n",
    "            print(\"Saliendo del programa...\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Opción inválida, intenta nuevamente.\")\n",
    "        \n",
    "        # Mostrar nuevamente el menú luego de finalizar la opción seleccionada.\n",
    "        print_menu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbcb2f",
   "metadata": {},
   "source": [
    "# EJECUTAR PROGRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40955799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Eliminar señas\n",
      "5. Generar Audios\n",
      "0. Salir\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\Desktop\\Mechatronics-Final-Proyect-\\venv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.6929 - loss: 1.2631 - val_accuracy: 0.9672 - val_loss: 0.2394 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9242 - loss: 0.3770 - val_accuracy: 0.9745 - val_loss: 0.1943 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9398 - loss: 0.3092 - val_accuracy: 0.9764 - val_loss: 0.1801 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9448 - loss: 0.2895 - val_accuracy: 0.9789 - val_loss: 0.1611 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9508 - loss: 0.2610 - val_accuracy: 0.9804 - val_loss: 0.1531 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9538 - loss: 0.2480 - val_accuracy: 0.9811 - val_loss: 0.1486 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9541 - loss: 0.2382 - val_accuracy: 0.9818 - val_loss: 0.1407 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.2397 - val_accuracy: 0.9812 - val_loss: 0.1380 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9563 - loss: 0.2256 - val_accuracy: 0.9826 - val_loss: 0.1395 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.2234 - val_accuracy: 0.9835 - val_loss: 0.1375 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.2187 - val_accuracy: 0.9828 - val_loss: 0.1306 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.2117 - val_accuracy: 0.9794 - val_loss: 0.1444 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9584 - loss: 0.2156 - val_accuracy: 0.9836 - val_loss: 0.1297 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9609 - loss: 0.2100 - val_accuracy: 0.9832 - val_loss: 0.1339 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9593 - loss: 0.2163 - val_accuracy: 0.9840 - val_loss: 0.1300 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9587 - loss: 0.2193 - val_accuracy: 0.9849 - val_loss: 0.1247 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 0.2105 - val_accuracy: 0.9828 - val_loss: 0.1283 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9596 - loss: 0.2200 - val_accuracy: 0.9804 - val_loss: 0.1404 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9615 - loss: 0.2050 - val_accuracy: 0.9820 - val_loss: 0.1316 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9604 - loss: 0.2054 - val_accuracy: 0.9826 - val_loss: 0.1322 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9607 - loss: 0.2048 - val_accuracy: 0.9818 - val_loss: 0.1322 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.1762 - val_accuracy: 0.9852 - val_loss: 0.1153 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.1653 - val_accuracy: 0.9840 - val_loss: 0.1145 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9707 - loss: 0.1627 - val_accuracy: 0.9871 - val_loss: 0.1051 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9717 - loss: 0.1555 - val_accuracy: 0.9859 - val_loss: 0.1053 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9721 - loss: 0.1504 - val_accuracy: 0.9870 - val_loss: 0.0950 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9702 - loss: 0.1569 - val_accuracy: 0.9857 - val_loss: 0.0985 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9715 - loss: 0.1487 - val_accuracy: 0.9861 - val_loss: 0.0966 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9703 - loss: 0.1526 - val_accuracy: 0.9855 - val_loss: 0.0955 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9720 - loss: 0.1506 - val_accuracy: 0.9862 - val_loss: 0.0931 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9709 - loss: 0.1486 - val_accuracy: 0.9877 - val_loss: 0.0927 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.1490 - val_accuracy: 0.9874 - val_loss: 0.0904 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9709 - loss: 0.1524 - val_accuracy: 0.9867 - val_loss: 0.0918 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.1454 - val_accuracy: 0.9875 - val_loss: 0.0907 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9732 - loss: 0.1436 - val_accuracy: 0.9866 - val_loss: 0.0929 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9720 - loss: 0.1455 - val_accuracy: 0.9862 - val_loss: 0.0967 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9709 - loss: 0.1480 - val_accuracy: 0.9863 - val_loss: 0.0935 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.1287 - val_accuracy: 0.9893 - val_loss: 0.0800 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.1268 - val_accuracy: 0.9885 - val_loss: 0.0825 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9773 - loss: 0.1207 - val_accuracy: 0.9892 - val_loss: 0.0789 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9771 - loss: 0.1203 - val_accuracy: 0.9882 - val_loss: 0.0799 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.1178 - val_accuracy: 0.9878 - val_loss: 0.0801 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.1168 - val_accuracy: 0.9888 - val_loss: 0.0756 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.1172 - val_accuracy: 0.9893 - val_loss: 0.0728 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.1153 - val_accuracy: 0.9901 - val_loss: 0.0712 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.1152 - val_accuracy: 0.9903 - val_loss: 0.0713 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9786 - loss: 0.1144 - val_accuracy: 0.9899 - val_loss: 0.0704 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9779 - loss: 0.1151 - val_accuracy: 0.9890 - val_loss: 0.0736 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9785 - loss: 0.1152 - val_accuracy: 0.9899 - val_loss: 0.0703 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m2100/2100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9777 - loss: 0.1152 - val_accuracy: 0.9887 - val_loss: 0.0741 - learning_rate: 2.5000e-04\n",
      "\u001b[1m657/657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento completado. Modelo entrenado.\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpjgj4h6zd\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpjgj4h6zd\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Nacho\\AppData\\Local\\Temp\\tmpjgj4h6zd'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 126), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 21), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2518075114768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2518075120576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2518075147008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2518075152464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2518075125856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2518075150704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2518075161440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2518075161792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2517860636576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2517860637104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2517860634640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2517860635872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2517860645376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2517860646080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Modelo convertido a TensorFlow Lite.\n",
      "Convertido a TFLite para evaluación en tiempo real\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Eliminar señas\n",
      "5. Generar Audios\n",
      "0. Salir\n",
      "\n",
      "--- Señas Guardadas ---\n",
      "1. Tres\n",
      "2. B\n",
      "3. Nueve\n",
      "4. Gracias\n",
      "5. Te Quiero\n",
      "6. Número\n",
      "7. De Nada\n",
      "8. Siete\n",
      "9. Ocho\n",
      "10. Hola\n",
      "11. Uno\n",
      "12. Yo\n",
      "13. A\n",
      "14. Cinco\n",
      "15. Chau\n",
      "16. Cuatro\n",
      "17. Diez\n",
      "18. Cero\n",
      "19. Abecedario\n",
      "20. Dos\n",
      "21. Seis\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Eliminar señas\n",
      "5. Generar Audios\n",
      "0. Salir\n",
      "Cámara UDP iniciada para recolección.\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Eliminar señas\n",
      "5. Generar Audios\n",
      "0. Salir\n",
      "\n",
      "--- Señas Registradas ---\n",
      "- Tres\n",
      "- B\n",
      "- Nueve\n",
      "- Gracias\n",
      "- Te Quiero\n",
      "- Número\n",
      "- De Nada\n",
      "- Siete\n",
      "- Ocho\n",
      "- Hola\n",
      "- Uno\n",
      "- Yo\n",
      "- A\n",
      "- Cinco\n",
      "- Chau\n",
      "- Cuatro\n",
      "- Diez\n",
      "- borrar\n",
      "- Cero\n",
      "- Abecedario\n",
      "- Dos\n",
      "- Seis\n",
      "Se eliminaron 103 muestras de 'borrar'\n",
      "\n",
      "=== MENU PRINCIPAL ===\n",
      "1. Recolectar nueva seña\n",
      "2. Entrenar modelo\n",
      "3. Listar señas cargadas\n",
      "4. Eliminar señas\n",
      "5. Generar Audios\n",
      "0. Salir\n",
      "Saliendo del programa...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    save_data()  # Guarda los datos recolectados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
