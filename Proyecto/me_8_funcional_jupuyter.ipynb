{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------PROYECTO FINAL-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTAR LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.saving.saving_api import load_model\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. INICIALIZAR MEDIAPIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial global\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    model_complexity=1\n",
    ")\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "dataset_dir = \"dataset_9\"\n",
    "model_path = \"gesture_model.h5\"\n",
    "sequence_length = 30\n",
    "total_landmarks = 126\n",
    "gestures = []\n",
    "X_mean = None\n",
    "X_std = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones principales\n",
    "def init_system():\n",
    "    global gestures\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    gestures = get_existing_gestures()\n",
    "    \n",
    "def get_existing_gestures():\n",
    "    return [d for d in os.listdir(dataset_dir) \n",
    "           if os.path.isdir(os.path.join(dataset_dir, d))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETECCION DE MANO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hands():\n",
    "    print(\"\\nIniciando detección de manos. Presiona 'ESC' para salir.\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow(\"Detección de Manos\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECOLLECION DE DATOS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    global gestures\n",
    "    gesture = input(\"\\nIngrese la palabra o letra para la cual desea recolectar datos: \").upper()\n",
    "    num_sequences = int(input(\"Ingrese el número de secuencias a capturar (recomendado: 50): \"))\n",
    "    \n",
    "    save_dir = os.path.join(dataset_dir, gesture)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\nRecolectando datos para el gesto '{gesture}'. Presiona 'ESC' para cancelar.\")\n",
    "    print(\"Mantenga la seña frente a la cámara...\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    sequence = []\n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            all_landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    all_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                all_landmarks += [0.0] * 63\n",
    "            \n",
    "            sequence.append(all_landmarks)\n",
    "            \n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        if len(sequence) == sequence_length:\n",
    "            np.save(os.path.join(save_dir, f\"secuencia_{counter}.npy\"), sequence)\n",
    "            counter += 1\n",
    "            sequence = []\n",
    "            print(f\"Secuencias capturadas: {counter}/{num_sequences}\")\n",
    "\n",
    "        cv2.imshow(\"Recolección de Datos\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27 or counter >= num_sequences:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    gestures = get_existing_gestures()\n",
    "    print(f\"\\nSe recolectaron {counter} secuencias para el gesto '{gesture}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CARGA DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for label_idx, gesture in enumerate(gestures):\n",
    "        gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "        sequences = [f for f in os.listdir(gesture_dir) if f.endswith('.npy')]\n",
    "        \n",
    "        for seq_file in sequences:\n",
    "            seq_path = os.path.join(gesture_dir, seq_file)\n",
    "            sequence = np.load(seq_path)\n",
    "            \n",
    "            if sequence.shape == (sequence_length, total_landmarks):\n",
    "                X.append(sequence)\n",
    "                y.append(label_idx)\n",
    "    \n",
    "    return np.array(X), np.array(y), gestures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    global X_mean, X_std, gestures\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay datos recolectados. Primero recolecte datos de gestos.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nCargando datos y preparando el entrenamiento...\")\n",
    "    X, y, gestures = load_data()\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "    X_mean = np.mean(X, axis=(0, 1))\n",
    "    X_std = np.std(X, axis=(0, 1))\n",
    "    X = (X - X_mean) / X_std\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(sequence_length, total_landmarks))\n",
    "    x = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling1D(2)(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', \n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
    "    outputs = tf.keras.layers.Dense(len(gestures), activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(\"\\nIniciando entrenamiento...\")\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],\n",
    "        verbose=1\n",
    "    )\n",
    "    model.save(model_path)\n",
    "    print(f\"\\nModelo guardado en {model_path}\")\n",
    "    \n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Precisión de validación final: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUACION DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"\\nPrimero debe entrenar el modelo.\")\n",
    "        return\n",
    "    \n",
    "    if X_mean is None or X_std is None:\n",
    "        print(\"\\nERROR: Debe entrenar el modelo primero para obtener los parámetros de normalización\")\n",
    "        return\n",
    "\n",
    "    model = load_model(model_path)\n",
    "    print(\"\\nCargando modelo y preparando evaluación...\")\n",
    "    \n",
    "    sequence = []\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    print(\"\\nMostrando predicciones en tiempo real. Presiona 'ESC' para salir.\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            all_landmarks = []\n",
    "            for hand in results.multi_hand_landmarks[:2]:\n",
    "                for lm in hand.landmark:\n",
    "                    all_landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            if len(results.multi_hand_landmarks) < 2:\n",
    "                all_landmarks += [0.0] * 63\n",
    "            \n",
    "            sequence.append(all_landmarks)\n",
    "            \n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        else:\n",
    "            sequence = []\n",
    "        \n",
    "        sequence = sequence[-sequence_length:]\n",
    "        \n",
    "        if len(sequence) == sequence_length:\n",
    "            try:\n",
    "                seq_array = np.array(sequence)\n",
    "                seq_array = (seq_array - X_mean) / X_std\n",
    "                input_data = seq_array.reshape(1, sequence_length, total_landmarks)\n",
    "                \n",
    "                prediction = model.predict(input_data, verbose=0)[0]\n",
    "                predicted_class = np.argmax(prediction)\n",
    "                confidence = np.max(prediction)\n",
    "                \n",
    "                if confidence > 0.8:\n",
    "                    gesture = gestures[predicted_class]\n",
    "                    cv2.putText(frame, f\"{gesture} ({confidence:.2%})\", (10, 50),\n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError en predicción: {str(e)}\")\n",
    "                break\n",
    "\n",
    "        cv2.imshow(\"Predicciones en Tiempo Real\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REENTRENAR GESTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_gesture():\n",
    "    global gestures\n",
    "    if not gestures:\n",
    "        print(\"\\nNo hay gestos para reentrenar. Primero recolecte datos.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nGestos disponibles para reentrenar:\")\n",
    "    for i, gesture in enumerate(gestures):\n",
    "        print(f\"{i+1}. {gesture}\")\n",
    "\n",
    "    try:\n",
    "        choice = int(input(\"\\nSeleccione el número del gesto a reentrenar: \")) - 1\n",
    "        if 0 <= choice < len(gestures):\n",
    "            gesture = gestures[choice]\n",
    "            gesture_dir = os.path.join(dataset_dir, gesture)\n",
    "            \n",
    "            for file in os.listdir(gesture_dir):\n",
    "                os.remove(os.path.join(gesture_dir, file))\n",
    "            \n",
    "            print(f\"\\nDatos anteriores de '{gesture}' eliminados.\")\n",
    "            collect_data()\n",
    "            train_model()\n",
    "        else:\n",
    "            print(\"\\nSelección inválida.\")\n",
    "    except ValueError:\n",
    "        print(\"\\nPor favor, ingrese un número válido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menú principal\n",
    "def main():\n",
    "    init_system()\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n=== Sistema de Reconocimiento de Lenguaje de Señas ===\")\n",
    "        print(\"1. Detectar Manos\")\n",
    "        print(\"2. Recolectar Datos\")\n",
    "        print(\"3. Entrenar Modelo\")\n",
    "        print(\"4. Evaluar\")\n",
    "        print(\"5. Reentrenar Gesto\")\n",
    "        print(\"6. Salir\")\n",
    "        \n",
    "        choice = input(\"\\nSeleccione una opción: \")\n",
    "        \n",
    "        if choice == '1':\n",
    "            detect_hands()\n",
    "        elif choice == '2':\n",
    "            collect_data()\n",
    "        elif choice == '3':\n",
    "            train_model()\n",
    "        elif choice == '4':\n",
    "            evaluate()\n",
    "        elif choice == '5':\n",
    "            retrain_gesture()\n",
    "        elif choice == '6':\n",
    "            print(\"\\n¡Hasta luego!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"\\nOpción inválida. Por favor, intente de nuevo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MENU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Salir\n",
      "\n",
      "Cargando datos y preparando el entrenamiento...\n",
      "WARNING:tensorflow:From c:\\Users\\juanp\\Desktop\\Python\\.conda\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\juanp\\Desktop\\Python\\.conda\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "\n",
      "Iniciando entrenamiento...\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\juanp\\Desktop\\Python\\.conda\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\juanp\\Desktop\\Python\\.conda\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 10s 384ms/step - loss: 2.2657 - accuracy: 0.1312 - val_loss: 1.9416 - val_accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 2.0453 - accuracy: 0.5875 - val_loss: 1.9611 - val_accuracy: 0.9000\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 1.8732 - accuracy: 0.9375 - val_loss: 1.9571 - val_accuracy: 0.7250\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 1.7389 - accuracy: 0.9688 - val_loss: 1.9479 - val_accuracy: 0.6000\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 1.5987 - accuracy: 0.9750 - val_loss: 1.9348 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 1.4977 - accuracy: 0.9750 - val_loss: 1.9021 - val_accuracy: 0.4750\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 1.4007 - accuracy: 0.9563 - val_loss: 1.8603 - val_accuracy: 0.4750\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 1.3244 - accuracy: 0.9500 - val_loss: 1.8181 - val_accuracy: 0.4750\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 1.2498 - accuracy: 0.9688 - val_loss: 1.7593 - val_accuracy: 0.6000\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 48ms/step - loss: 1.1842 - accuracy: 1.0000 - val_loss: 1.6714 - val_accuracy: 0.6000\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 1.1233 - accuracy: 0.9875 - val_loss: 1.5829 - val_accuracy: 0.7750\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 1.0832 - accuracy: 0.9875 - val_loss: 1.4657 - val_accuracy: 0.8000\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 1.0482 - accuracy: 0.9937 - val_loss: 1.3622 - val_accuracy: 0.9000\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 64ms/step - loss: 1.0098 - accuracy: 1.0000 - val_loss: 1.2568 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.9759 - accuracy: 0.9937 - val_loss: 1.1817 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.9498 - accuracy: 1.0000 - val_loss: 1.1115 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.9343 - accuracy: 1.0000 - val_loss: 1.0510 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.9087 - accuracy: 1.0000 - val_loss: 1.0056 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.8989 - accuracy: 0.9937 - val_loss: 0.9632 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.8819 - accuracy: 1.0000 - val_loss: 0.9340 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.8680 - accuracy: 1.0000 - val_loss: 0.9110 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.8602 - accuracy: 1.0000 - val_loss: 0.8926 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.8457 - accuracy: 1.0000 - val_loss: 0.8773 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 73ms/step - loss: 0.8385 - accuracy: 1.0000 - val_loss: 0.8645 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.8285 - accuracy: 1.0000 - val_loss: 0.8520 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.8222 - accuracy: 1.0000 - val_loss: 0.8406 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.8140 - accuracy: 1.0000 - val_loss: 0.8314 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 51ms/step - loss: 0.8067 - accuracy: 1.0000 - val_loss: 0.8220 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 0.7987 - accuracy: 1.0000 - val_loss: 0.8145 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 74ms/step - loss: 0.7940 - accuracy: 1.0000 - val_loss: 0.8063 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 59ms/step - loss: 0.7910 - accuracy: 1.0000 - val_loss: 0.7992 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.7829 - accuracy: 1.0000 - val_loss: 0.7917 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 65ms/step - loss: 0.7757 - accuracy: 1.0000 - val_loss: 0.7847 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.7697 - accuracy: 1.0000 - val_loss: 0.7782 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 50ms/step - loss: 0.7641 - accuracy: 1.0000 - val_loss: 0.7719 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.7601 - accuracy: 1.0000 - val_loss: 0.7657 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 55ms/step - loss: 0.7542 - accuracy: 1.0000 - val_loss: 0.7599 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.7496 - accuracy: 1.0000 - val_loss: 0.7540 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.7429 - accuracy: 1.0000 - val_loss: 0.7486 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 79ms/step - loss: 0.7397 - accuracy: 1.0000 - val_loss: 0.7431 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 72ms/step - loss: 0.7351 - accuracy: 1.0000 - val_loss: 0.7380 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 58ms/step - loss: 0.7297 - accuracy: 1.0000 - val_loss: 0.7331 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 57ms/step - loss: 0.7239 - accuracy: 1.0000 - val_loss: 0.7281 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 63ms/step - loss: 0.7190 - accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 56ms/step - loss: 0.7157 - accuracy: 1.0000 - val_loss: 0.7180 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.7100 - accuracy: 1.0000 - val_loss: 0.7131 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 49ms/step - loss: 0.7049 - accuracy: 1.0000 - val_loss: 0.7083 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 53ms/step - loss: 0.6998 - accuracy: 1.0000 - val_loss: 0.7036 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 54ms/step - loss: 0.6957 - accuracy: 1.0000 - val_loss: 0.6988 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 52ms/step - loss: 0.6918 - accuracy: 1.0000 - val_loss: 0.6939 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juanp\\Desktop\\Python\\.conda\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo guardado en gesture_model.h5\n",
      "Precisión de validación final: 100.00%\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Salir\n",
      "\n",
      "Cargando modelo y preparando evaluación...\n",
      "\n",
      "Mostrando predicciones en tiempo real. Presiona 'ESC' para salir.\n",
      "\n",
      "=== Sistema de Reconocimiento de Lenguaje de Señas ===\n",
      "1. Detectar Manos\n",
      "2. Recolectar Datos\n",
      "3. Entrenar Modelo\n",
      "4. Evaluar\n",
      "5. Reentrenar Gesto\n",
      "6. Salir\n",
      "\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
